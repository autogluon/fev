{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers the following topics:\n",
    "1. Adding a wrapper for your model to [fev/examples](https://github.com/autogluon/fev/tree/main/examples).\n",
    "2. Submitting the results for your model to the [fev-leaderboard](https://huggingface.co/spaces/autogluon/fev-leaderboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a wrapper for your model to [fev/examples](https://github.com/autogluon/fev/tree/main/examples).\n",
    "To add a wrapper for your library to `fev/examples`, you need to create a folder under `fev/examples/{YOUR_MODEL_NAME}` that contains:\n",
    "- A Python file `evaluate_model.py` that contains a method `predict_with_model` with signature\n",
    "    ```python\n",
    "    def predict_with_model(task: fev.Task, **kwargs) -> tuple[list[datasets.DatasetDict], float, dict]:\n",
    "        \"\"\"Returns model predictions, inference time and potentially extra information about the model.\"\"\"\n",
    "        ...\n",
    "    ```\n",
    "- `requirements.txt` file containing the required dependencies for your model.\n",
    "\n",
    "Defining the method `predict_with_model` is the most complex part of this process. We recommend looking at the implementations of some existing models to see how this can be done.\n",
    "\n",
    "The only hard requirement for this method is that it should return a tuple consisting of 3 elements:\n",
    "1. `predictions` (`list[datasets.DatasetDict]`) object containing the model predictions for each evaluation window.  \n",
    "2. `inference_time` (`float`) inference time of the model for the entire task (in seconds).\n",
    "3. `extra_info` (`dict | None`) optional information about the model such as model configuration.\n",
    "\n",
    "Predictions should follow the schema provided by `task.predictions_schema`.\n",
    "\n",
    "Each entry of `predictions` must contain a list of length `task.horizon`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shchuro/envs/fev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': Sequence(feature=Value(dtype='float64', id=None), length=30, id=None)}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fev\n",
    "\n",
    "task = fev.Task(\n",
    "    dataset_path=\"autogluon/chronos_datasets\",\n",
    "    dataset_config=\"monash_rideshare\",\n",
    "    target=\"price_mean\",\n",
    "    horizon=30,\n",
    ")\n",
    "task.predictions_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  For a probabilistic forecasting task (if `task.quantile_levels` are provided), each entry of `predictions` must additionally contain the quantile forecasts. For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': Sequence(feature=Value(dtype='float64', id=None), length=30, id=None),\n",
       " '0.1': Sequence(feature=Value(dtype='float64', id=None), length=30, id=None),\n",
       " '0.5': Sequence(feature=Value(dtype='float64', id=None), length=30, id=None),\n",
       " '0.9': Sequence(feature=Value(dtype='float64', id=None), length=30, id=None)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = fev.Task(\n",
    "    dataset_path=\"autogluon/chronos_datasets\",\n",
    "    dataset_config=\"monash_rideshare\",\n",
    "    target=\"price_mean\",\n",
    "    horizon=30,\n",
    "    quantile_levels=[0.1, 0.5, 0.9],\n",
    ")\n",
    "task.predictions_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predictions` cannot contain any missing values represented by `NaN`, otherwise an exception will be raised.\n",
    "\n",
    "Other than what's described above, there are no hard restrictions on how the `predict_with_model` method needs to be implemented. For example, it's completely up to you whether the method uses any datasets columns except the target or how the data is preprocessed.\n",
    "\n",
    "Still, here is some general advice:\n",
    "- If your model is capable of generating probabilistic forecasts, make sure that you correct the \"optimal\" forecast for the `task.eval_metric`. For example, metrics like `\"MSE\"` or `\"RMSSE\"`, the mean forecast is preferred, while metrics like `\"MASE\"` are optimized by the median forecast.\n",
    "- Use `fev.convert_input_data()` to take advantage of the adapters and reduce the boilerplate preprocessing code.\n",
    "- Make sure that your wrapper can deal with missing values (or at least imputes them before passing the data to your model).\n",
    "- Make sure that your wrapper takes advantage of the extra features of the task. For example, the following attributes might be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task.static_columns=[]\n",
      "task.dynamic_columns=[]\n",
      "task.known_dynamic_columns=[]\n",
      "task.past_dynamic_columns=[]\n",
      "task.freq='h'\n"
     ]
    }
   ],
   "source": [
    "print(f\"{task.static_columns=}\")\n",
    "print(f\"{task.dynamic_columns=}\")\n",
    "print(f\"{task.known_dynamic_columns=}\")\n",
    "print(f\"{task.past_dynamic_columns=}\")\n",
    "# Attributes available after `task.load_full_dataset` is called\n",
    "task.load_full_dataset()\n",
    "print(f\"{task.freq=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting the results for your model to the [fev-leaderboard](https://huggingface.co/spaces/autogluon/fev-leaderboard)\n",
    "After you've implemented the wrapper for your model in `fev/examples`, complete the following steps:\n",
    "1. Fork [`autogluon/fev`](https://github.com/autogluon/fev) and clone your fork to your machine.\n",
    "2. Implement your model's wrapper in `fev/examples`.\n",
    "3. Run the model on all tasks from the benchmark and save the results to `fev/benchmarks/fev_bench/results/{model_name}.csv`.\n",
    "4. Open a pull request to `autogluon/fev` containing the following files:\n",
    "    - `fev/examples/{model_name}/evaluate_model.py`\n",
    "    - `fev/examples/{model_name}/requirements.txt`\n",
    "    - `fev/benchmarks/fev_bench/results/{model_name}.csv`\n",
    "5. We will independently reproduce the results using the code you provided and add the results to the leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Example code from fev/examples/my_amazing_model/evaluate_model.py\n",
    "\n",
    "def predict_with_model(task: fev.Task, **kwargs) -> tuple[list[datasets.DatasetDict], float, dict]:\n",
    "    \"\"\"Wrapper for my_amazing_model\"\"\"\n",
    "    ...\n",
    "    return predictions_per_window, inference_time, extra_info\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"my_amazing_model\"\n",
    "    benchmark = fev.Benchmark.from_yaml(\n",
    "        \"https://raw.githubusercontent.com/autogluon/fev/refs/heads/main/benchmarks/fev_bench/tasks.yaml\"\n",
    "    )\n",
    "\n",
    "    summaries = []\n",
    "    for task in benchmark.tasks:\n",
    "        predictions_per_window, inference_time, extra_info = predict_with_model(task)\n",
    "        evaluation_summary = task.evaluation_summary(\n",
    "            predictions_per_window,\n",
    "            model_name=model_name,\n",
    "            inference_time_s=inference_time,\n",
    "            extra_info=extra_info,\n",
    "            trained_on_this_dataset=True,  # True if model has seen this dataset during training, False otherwise. Please try to be honest!\n",
    "        )\n",
    "        summaries.append(evaluation_summary)\n",
    "\n",
    "    summary_df = pd.DataFrame(summaries)\n",
    "    print(summary_df)\n",
    "    summary_df.to_csv(f\"{model_name}.csv\", index=False)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
