{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"fev: Forecast evaluation library","text":"<p><code>fev</code> is a lightweight library that makes it easy to benchmark time series forecasting models.</p> <ul> <li>Extensible: Easy to define your own forecasting tasks and benchmarks.</li> <li>Reproducible: Ensures that the results obtained by different users are comparable.</li> <li>Easy to use: Compatible with most popular forecasting libraries.</li> <li>Minimal dependencies: Just a thin wrapper on top of \ud83e\udd17<code>datasets</code>.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install fev\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":"<pre><code>import fev\n\n# Create a forecasting task\ntask = fev.Task(\n   dataset_path=\"autogluon/chronos_datasets\",\n   dataset_config=\"m4_hourly\",\n   horizon=24,\n)\n\n# Evaluate your model\npredictions_per_window = []\nfor window in task.iter_windows():\n   past_data, future_data = window.get_input_data()\n   # Make predictions\n   predictions_per_window.append(model.predict(past_data, future_data))\n\n# Get reproducible evaluation summary with all task details &amp; metrics\nsummary = task.evaluation_summary(predictions_per_window, \"my_model\")\n</code></pre>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li>\ud83d\ude80 Quickstart - Get started with your first forecasting task</li> <li>\ud83d\udcca Dataset Format - Learn how to use your own datasets</li> <li>\u2699\ufe0f Tasks &amp; Benchmarks - Advanced task configuration</li> <li>\ud83e\udde9 Adapters - Convert data to formats used by other forecasting libraries</li> <li>\ud83e\udd16 Models - Contribute your model to <code>fev-leaderboard</code></li> </ul>"},{"location":"api/analysis/","title":"Analysis methods","text":"<p><code>fev</code> provides 3 main methods for aggregating the evaluation summaries produced by <code>Task.evaluation_summary()</code>:</p> <ul> <li><code>pivot_table()</code> - A table of model scores with tasks as index and model names as columns.</li> <li><code>leaderboard()</code> - Aggregate performance for each individual model.</li> <li><code>pairwise_comparison()</code> - Aggregate performance for each pair of models.</li> </ul> <p>On this page <code>SummaryType</code> is an alias for one of the following types: </p>"},{"location":"api/analysis/#fev.analysis.SummaryType","title":"<code>SummaryType: TypeAlias = pd.DataFrame | list[dict] | str | pathlib.Path</code>  <code>module-attribute</code>","text":""},{"location":"api/analysis/#functions","title":"Functions","text":""},{"location":"api/analysis/#fev.leaderboard","title":"<code>leaderboard(summaries: SummaryType | list[SummaryType], metric_column: str = 'test_error', missing_strategy: Literal['error', 'drop', 'impute'] = 'error', baseline_model: str = 'seasonal_naive', min_relative_error: float | None = 0.01, max_relative_error: float | None = 100.0, included_models: list[str] | None = None, excluded_models: list[str] | None = None, leakage_imputation_model: str | None = None, n_resamples: int | None = None, seed: int = 123, normalize_time_per_n_forecasts: int | None = None)</code>","text":"<p>Generate a leaderboard with aggregate performance metrics for all models.</p> <p>Computes skill score (1 - geometric mean relative error) and win rate with bootstrap confidence intervals across all tasks. Models are ranked by skill score.</p> <p>Parameters:</p> Name Type Description Default <code>summaries</code> <code>SummaryType | list[SummaryType]</code> <p>Evaluation summaries as DataFrame, list of dicts, or file path(s)</p> required <code>metric_column</code> <code>str</code> <p>Column name containing the metric to evaluate</p> <code>\"test_error\"</code> <code>baseline_model</code> <code>str</code> <p>Model name to use for relative error computation</p> <code>\"SeasonalNaive\"</code> <code>missing_strategy</code> <code>Literal['error', 'drop', 'impute']</code> <p>How to handle missing results:</p> <ul> <li><code>\"error\"</code>: Raise error if any results are missing</li> <li><code>\"drop\"</code>: Remove tasks where any model failed</li> <li><code>\"impute\"</code>: Fill missing results with <code>baseline_model</code> scores</li> </ul> <code>\"error\"</code> <code>min_relative_error</code> <code>float</code> <p>Lower bound for clipping relative errors w.r.t. the <code>baseline_model</code></p> <code>1e-2</code> <code>max_relative_error</code> <code>float</code> <p>Upper bound for clipping relative errors w.r.t. the <code>baseline_model</code></p> <code>100</code> <code>included_models</code> <code>list[str]</code> <p>Models to include (mutually exclusive with <code>excluded_models</code>)</p> <code>None</code> <code>excluded_models</code> <code>list[str]</code> <p>Models to exclude (mutually exclusive with <code>included_models</code>)</p> <code>None</code> <code>leakage_imputation_model</code> <code>str</code> <p>Zero-shot model used to replace results when data leakage is detected. Applied before <code>missing_strategy</code>.</p> <code>None</code> <code>n_resamples</code> <code>int | None</code> <p>Number of bootstrap samples for confidence intervals. If None, confidence intervals are not computed</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for reproducible bootstrap sampling</p> <code>123</code> <code>normalize_time_per_n_forecasts</code> <code>int</code> <p>If set, rescale each task's runtime to represent the time for this many forecasts (by dividing by the task's num_forecasts and multiplying by this value). Inference and training time column names will have suffix <code>\"_per{value}\"</code> added. If None, no normalization is performed.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Leaderboard sorted by <code>win_rate</code>, with columns:</p> <ul> <li><code>win_rate</code>: Fraction of pairwise comparisons won against other models</li> <li><code>win_rate_lower</code>: Lower bound of 95% confidence interval (only if n_resamples is not None)</li> <li><code>win_rate_upper</code>: Upper bound of 95% confidence interval (only if n_resamples is not None)</li> <li><code>skill_score</code>: Skill score (1 - geometric mean relative error)</li> <li><code>skill_score_lower</code>: Lower bound of 95% confidence interval (only if n_resamples is not None)</li> <li><code>skill_score_upper</code>: Upper bound of 95% confidence interval (only if n_resamples is not None)</li> <li><code>median_training_time_s</code>: Median training time across tasks. If <code>normalize_time_per_n_forecasts</code> is set, each task's time normalized by <code>num_forecasts</code> before taking the median</li> <li><code>median_inference_time_s</code>: Median inference time across tasks. If <code>normalize_time_per_n_forecasts</code> is set, each task's time normalized by <code>num_forecasts</code> before taking the median</li> <li><code>training_corpus_overlap</code>: Mean fraction of tasks where model was trained on the dataset</li> <li><code>num_failures</code>: Number of tasks where the model failed</li> </ul> Source code in <code>src/fev/analysis.py</code> <pre><code>def leaderboard(\n    summaries: SummaryType | list[SummaryType],\n    metric_column: str = \"test_error\",\n    missing_strategy: Literal[\"error\", \"drop\", \"impute\"] = \"error\",\n    baseline_model: str = \"seasonal_naive\",\n    min_relative_error: float | None = 1e-2,\n    max_relative_error: float | None = 100.0,\n    included_models: list[str] | None = None,\n    excluded_models: list[str] | None = None,\n    leakage_imputation_model: str | None = None,\n    n_resamples: int | None = None,\n    seed: int = 123,\n    normalize_time_per_n_forecasts: int | None = None,\n):\n    \"\"\"Generate a leaderboard with aggregate performance metrics for all models.\n\n    Computes skill score (1 - geometric mean relative error) and win rate with bootstrap confidence\n    intervals across all tasks. Models are ranked by skill score.\n\n    Parameters\n    ----------\n    summaries : SummaryType | list[SummaryType]\n        Evaluation summaries as DataFrame, list of dicts, or file path(s)\n    metric_column : str, default \"test_error\"\n        Column name containing the metric to evaluate\n    baseline_model : str, default \"SeasonalNaive\"\n        Model name to use for relative error computation\n    missing_strategy : Literal[\"error\", \"drop\", \"impute\"], default \"error\"\n        How to handle missing results:\n\n        - `\"error\"`: Raise error if any results are missing\n        - `\"drop\"`: Remove tasks where any model failed\n        - `\"impute\"`: Fill missing results with `baseline_model` scores\n    min_relative_error : float, default 1e-2\n        Lower bound for clipping relative errors w.r.t. the `baseline_model`\n    max_relative_error : float, default 100\n        Upper bound for clipping relative errors w.r.t. the `baseline_model`\n    included_models : list[str], optional\n        Models to include (mutually exclusive with `excluded_models`)\n    excluded_models : list[str], optional\n        Models to exclude (mutually exclusive with `included_models`)\n    leakage_imputation_model : str, optional\n        Zero-shot model used to replace results when data leakage is detected. Applied before `missing_strategy`.\n    n_resamples : int | None, default None\n        Number of bootstrap samples for confidence intervals. If None, confidence intervals are not computed\n    seed : int, default 123\n        Random seed for reproducible bootstrap sampling\n    normalize_time_per_n_forecasts : int, optional\n        If set, rescale each task's runtime to represent the time for this many forecasts (by dividing by the task's\n        num_forecasts and multiplying by this value). Inference and training time column names will have suffix `\"_per{value}\"` added.\n        If None, no normalization is performed.\n\n    Returns\n    -------\n    pd.DataFrame\n        Leaderboard sorted by `win_rate`, with columns:\n\n        - `win_rate`: Fraction of pairwise comparisons won against other models\n        - `win_rate_lower`: Lower bound of 95% confidence interval (only if n_resamples is not None)\n        - `win_rate_upper`: Upper bound of 95% confidence interval (only if n_resamples is not None)\n        - `skill_score`: Skill score (1 - geometric mean relative error)\n        - `skill_score_lower`: Lower bound of 95% confidence interval (only if n_resamples is not None)\n        - `skill_score_upper`: Upper bound of 95% confidence interval (only if n_resamples is not None)\n        - `median_training_time_s`: Median training time across tasks. If `normalize_time_per_n_forecasts` is set, each task's time normalized by `num_forecasts` before taking the median\n        - `median_inference_time_s`: Median inference time across tasks. If `normalize_time_per_n_forecasts` is set, each task's time normalized by `num_forecasts` before taking the median\n        - `training_corpus_overlap`: Mean fraction of tasks where model was trained on the dataset\n        - `num_failures`: Number of tasks where the model failed\n    \"\"\"\n    summaries = _load_summaries(summaries, check_fev_version=True)\n    summaries = _filter_models(summaries, included_models=included_models, excluded_models=excluded_models)\n    errors_df = pivot_table(summaries, metric_column=metric_column, baseline_model=baseline_model)\n\n    training_time_df = pivot_table(summaries, metric_column=\"training_time_s\")\n    inference_time_df = pivot_table(summaries, metric_column=\"inference_time_s\")\n    training_corpus_overlap_df = pivot_table(summaries, metric_column=\"trained_on_this_dataset\")\n\n    if normalize_time_per_n_forecasts is not None:\n        num_forecasts_df = pivot_table(summaries, metric_column=\"num_forecasts\").astype(pd.Int64Dtype())\n        if not (num_forecasts_df.nunique(axis=1, dropna=True) == 1).all():\n            raise ValueError(\n                \"Column 'num_forecasts' has inconsistent values across models for the same task. \"\n                \"This indicates corrupted evaluation summaries.\"\n            )\n        # num_forecasts is per-task (not per-model), so all models should report the same value.\n        # Some models may have NaN (old fev versions). Use bfill to propagate non-NaN values across\n        # columns, then extract first column to get a Series with one num_forecasts per task.\n        num_forecasts = num_forecasts_df.bfill(axis=1).iloc[:, 0]\n        training_time_df = training_time_df.div(num_forecasts, axis=0) * normalize_time_per_n_forecasts\n        inference_time_df = inference_time_df.div(num_forecasts, axis=0) * normalize_time_per_n_forecasts\n\n    if leakage_imputation_model is not None:\n        errors_df = _handle_leakage_imputation(errors_df, training_corpus_overlap_df, leakage_imputation_model)\n    errors_df = errors_df.clip(lower=min_relative_error, upper=max_relative_error)\n\n    num_failures_per_model = errors_df.isna().sum()\n    if missing_strategy == \"drop\":\n        errors_df = errors_df.dropna()\n        if len(errors_df) == 0:\n            raise ValueError(\"All results are missing for some models.\")\n        print(f\"{len(errors_df)} tasks left after removing failures\")\n    elif missing_strategy == \"impute\":\n        # For leaderboard, baseline scores are already 1.0 after normalization, so fill with 1.0\n        errors_df = errors_df.fillna(1.0)\n    elif missing_strategy == \"error\":\n        if num_failures_per_model.sum():\n            raise ValueError(\n                f\"Summaries contain {len(errors_df)} tasks. Results are missing for the following models:\"\n                f\"\\n{num_failures_per_model[num_failures_per_model &gt; 0]}\"\n            )\n    else:\n        raise ValueError(f\"Invalid {missing_strategy=}, expected one of ['error', 'drop', 'impute']\")\n    bootstrap_resamples = 1 if n_resamples is None else n_resamples\n    win_rate, win_rate_lower, win_rate_upper = bootstrap(\n        errors_df.to_numpy(), statistic=_win_rate, n_resamples=bootstrap_resamples, seed=seed\n    )\n    skill_score, skill_score_lower, skill_score_upper = bootstrap(\n        errors_df.to_numpy(), statistic=_skill_score, n_resamples=bootstrap_resamples, seed=seed\n    )\n\n    result_df = pd.DataFrame(\n        {\n            \"win_rate\": win_rate,\n            \"win_rate_lower\": win_rate_lower,\n            \"win_rate_upper\": win_rate_upper,\n            \"skill_score\": skill_score,\n            \"skill_score_lower\": skill_score_lower,\n            \"skill_score_upper\": skill_score_upper,\n            # Select only tasks that are also in errors_df (in case some tasks were dropped with missing_strategy=\"drop\")\n            \"median_training_time_s\": training_time_df.loc[errors_df.index].median(),\n            \"median_inference_time_s\": inference_time_df.loc[errors_df.index].median(),\n            \"training_corpus_overlap\": training_corpus_overlap_df.loc[errors_df.index].mean(),\n            \"num_failures\": num_failures_per_model,\n        },\n        index=errors_df.columns,\n    )\n    if n_resamples is None:\n        result_df = result_df.drop(\n            columns=[\"skill_score_lower\", \"skill_score_upper\", \"win_rate_lower\", \"win_rate_upper\"]\n        )\n    if normalize_time_per_n_forecasts is not None:\n        result_df = result_df.rename(\n            columns={\n                col: col + f\"_per{int(normalize_time_per_n_forecasts)}\"\n                for col in [\"median_training_time_s\", \"median_inference_time_s\"]\n            }\n        )\n    return result_df.sort_values(by=\"win_rate\", ascending=False)\n</code></pre>"},{"location":"api/analysis/#fev.pairwise_comparison","title":"<code>pairwise_comparison(summaries: SummaryType | list[SummaryType], metric_column: str = 'test_error', missing_strategy: Literal['error', 'drop', 'impute'] = 'error', baseline_model: str | None = None, min_relative_error: float | None = 0.01, max_relative_error: float | None = 100.0, included_models: list[str] | None = None, excluded_models: list[str] | None = None, leakage_imputation_model: str | None = None, n_resamples: int | None = None, seed: int = 123) -&gt; pd.DataFrame</code>","text":"<p>Compute pairwise performance comparisons between all model pairs.</p> <p>For each pair of models, calculates skill score (1 - geometric mean relative error) and win rate with bootstrap confidence intervals across all tasks.</p> <p>Parameters:</p> Name Type Description Default <code>summaries</code> <code>SummaryType | list[SummaryType]</code> <p>Evaluation summaries as DataFrame, list of dicts, or file path(s)</p> required <code>metric_column</code> <code>str</code> <p>Column name containing the metric to evaluate</p> <code>\"test_error\"</code> <code>missing_strategy</code> <code>Literal['error', 'drop', 'impute']</code> <p>How to handle missing results:</p> <ul> <li><code>\"error\"</code>: Raise error if any results are missing</li> <li><code>\"drop\"</code>: Remove tasks where any model failed</li> <li><code>\"impute\"</code>: Fill missing results with <code>baseline_model</code> scores</li> </ul> <code>\"error\"</code> <code>baseline_model</code> <code>str</code> <p>Required only when missing_strategy=\"impute\"</p> <code>None</code> <code>min_relative_error</code> <code>float</code> <p>Lower bound for clipping error ratios in pairwise comparisons</p> <code>1e-2</code> <code>max_relative_error</code> <code>float</code> <p>Upper bound for clipping error ratios in pairwise comparisons</p> <code>100.0</code> <code>included_models</code> <code>list[str]</code> <p>Models to include (mutually exclusive with <code>excluded_models</code>)</p> <code>None</code> <code>excluded_models</code> <code>list[str]</code> <p>Models to exclude (mutually exclusive with <code>included_models</code>)</p> <code>None</code> <code>leakage_imputation_model</code> <code>str</code> <p>Zero-shot model used to replace results when data leakage is detected. Applied before <code>missing_strategy</code>.</p> <code>None</code> <code>n_resamples</code> <code>int | None</code> <p>Number of bootstrap samples for confidence intervals. If None, confidence intervals are not computed</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for reproducible bootstrap sampling</p> <code>123</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pairwise comparison results with <code>pd.MultiIndex</code> <code>(model_1, model_2)</code> and columns:</p> <ul> <li><code>win_rate</code>: Fraction of tasks where <code>model_1</code> outperforms <code>model_2</code></li> <li><code>win_rate_lower</code>: Lower bound of 95% confidence interval (only if n_resamples is not None)</li> <li><code>win_rate_upper</code>: Upper bound of 95% confidence interval (only if n_resamples is not None)</li> <li><code>skill_score</code>: 1 - geometric mean of <code>model_1/model_2</code> error ratios</li> <li><code>skill_score_lower</code>: Lower bound of 95% confidence interval (only if n_resamples is not None)</li> <li><code>skill_score_upper</code>: Upper bound of 95% confidence interval (only if n_resamples is not None)</li> </ul> Source code in <code>src/fev/analysis.py</code> <pre><code>def pairwise_comparison(\n    summaries: SummaryType | list[SummaryType],\n    metric_column: str = \"test_error\",\n    missing_strategy: Literal[\"error\", \"drop\", \"impute\"] = \"error\",\n    baseline_model: str | None = None,\n    min_relative_error: float | None = 1e-2,\n    max_relative_error: float | None = 100.0,\n    included_models: list[str] | None = None,\n    excluded_models: list[str] | None = None,\n    leakage_imputation_model: str | None = None,\n    n_resamples: int | None = None,\n    seed: int = 123,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute pairwise performance comparisons between all model pairs.\n\n    For each pair of models, calculates skill score (1 - geometric mean relative error) and\n    win rate with bootstrap confidence intervals across all tasks.\n\n    Parameters\n    ----------\n    summaries : SummaryType | list[SummaryType]\n        Evaluation summaries as DataFrame, list of dicts, or file path(s)\n    metric_column : str, default \"test_error\"\n        Column name containing the metric to evaluate\n    missing_strategy : Literal[\"error\", \"drop\", \"impute\"], default \"error\"\n        How to handle missing results:\n\n        - `\"error\"`: Raise error if any results are missing\n        - `\"drop\"`: Remove tasks where any model failed\n        - `\"impute\"`: Fill missing results with `baseline_model` scores\n    baseline_model : str, optional\n        Required only when missing_strategy=\"impute\"\n    min_relative_error : float, optional, default 1e-2\n        Lower bound for clipping error ratios in pairwise comparisons\n    max_relative_error : float, optional, default 100.0\n        Upper bound for clipping error ratios in pairwise comparisons\n    included_models : list[str], optional\n        Models to include (mutually exclusive with `excluded_models`)\n    excluded_models : list[str], optional\n        Models to exclude (mutually exclusive with `included_models`)\n    leakage_imputation_model : str, optional\n        Zero-shot model used to replace results when data leakage is detected. Applied before `missing_strategy`.\n    n_resamples : int | None, default None\n        Number of bootstrap samples for confidence intervals. If None, confidence intervals are not computed\n    seed : int, default 123\n        Random seed for reproducible bootstrap sampling\n\n    Returns\n    -------\n    pd.DataFrame\n        Pairwise comparison results with `pd.MultiIndex` `(model_1, model_2)` and columns:\n\n        - `win_rate`: Fraction of tasks where `model_1` outperforms `model_2`\n        - `win_rate_lower`: Lower bound of 95% confidence interval (only if n_resamples is not None)\n        - `win_rate_upper`: Upper bound of 95% confidence interval (only if n_resamples is not None)\n        - `skill_score`: 1 - geometric mean of `model_1/model_2` error ratios\n        - `skill_score_lower`: Lower bound of 95% confidence interval (only if n_resamples is not None)\n        - `skill_score_upper`: Upper bound of 95% confidence interval (only if n_resamples is not None)\n    \"\"\"\n    summaries = _load_summaries(summaries, check_fev_version=True)\n    summaries = _filter_models(summaries, included_models=included_models, excluded_models=excluded_models)\n    errors_df = pivot_table(summaries, metric_column=metric_column)\n    num_failures_per_model = errors_df.isna().sum()\n\n    training_corpus_overlap_df = pivot_table(summaries, metric_column=\"trained_on_this_dataset\")\n    if leakage_imputation_model is not None:\n        errors_df = _handle_leakage_imputation(errors_df, training_corpus_overlap_df, leakage_imputation_model)\n\n    if missing_strategy == \"drop\":\n        errors_df = errors_df.dropna()\n        if len(errors_df) == 0:\n            raise ValueError(\"All results are missing for some models.\")\n        print(f\"{len(errors_df)} tasks left after removing failures\")\n    elif missing_strategy == \"impute\":\n        if baseline_model is None:\n            raise ValueError(\"baseline_model is required when missing_strategy='impute'\")\n        if baseline_model not in errors_df.columns:\n            raise ValueError(\n                f\"baseline_model '{baseline_model}' is missing. Available models: {errors_df.columns.to_list()}\"\n            )\n        for col in errors_df.columns:\n            if col != baseline_model:\n                errors_df[col] = errors_df[col].fillna(errors_df[baseline_model])\n    elif missing_strategy == \"error\":\n        if num_failures_per_model.sum():\n            raise ValueError(\n                f\"Summaries contain {len(errors_df)} tasks. Results are missing for the following models:\"\n                f\"\\n{num_failures_per_model[num_failures_per_model &gt; 0]}\"\n            )\n    else:\n        raise ValueError(f\"Invalid {missing_strategy=}, expected one of ['error', 'drop', 'impute']\")\n    model_order = errors_df.rank(axis=1).mean().sort_values().index\n    errors_df = errors_df[model_order]\n\n    bootstrap_resamples = 1 if n_resamples is None else n_resamples\n    skill_score, skill_score_lower, skill_score_upper = bootstrap(\n        errors_df.to_numpy(),\n        statistic=lambda x: _pairwise_skill_score(x, min_relative_error, max_relative_error),\n        n_resamples=bootstrap_resamples,\n        seed=seed,\n    )\n    win_rate, win_rate_lower, win_rate_upper = bootstrap(\n        errors_df.to_numpy(),\n        statistic=_pairwise_win_rate,\n        n_resamples=bootstrap_resamples,\n        seed=seed,\n    )\n\n    result_df = pd.DataFrame(\n        {\n            \"win_rate\": win_rate.flatten(),\n            \"win_rate_lower\": win_rate_lower.flatten(),\n            \"win_rate_upper\": win_rate_upper.flatten(),\n            \"skill_score\": skill_score.flatten(),\n            \"skill_score_lower\": skill_score_lower.flatten(),\n            \"skill_score_upper\": skill_score_upper.flatten(),\n        },\n        index=pd.MultiIndex.from_product([errors_df.columns, errors_df.columns], names=[\"model_1\", \"model_2\"]),\n    )\n    if n_resamples is None:\n        result_df = result_df.drop(\n            columns=[\"skill_score_lower\", \"skill_score_upper\", \"win_rate_lower\", \"win_rate_upper\"]\n        )\n    return result_df\n</code></pre>"},{"location":"api/analysis/#fev.pivot_table","title":"<code>pivot_table(summaries: SummaryType | list[SummaryType], metric_column: str = 'test_error', task_columns: str | list[str] = TASK_DEF_COLUMNS.copy(), baseline_model: str | None = None, check_fev_version: bool = False) -&gt; pd.DataFrame</code>","text":"<p>Convert evaluation summaries into a pivot table for analysis.</p> <p>Creates a matrix where rows represent tasks and columns represent models, with each cell containing the specified metric value. Optionally normalizes all scores relative to a baseline model.</p> <p>Parameters:</p> Name Type Description Default <code>summaries</code> <code>SummaryType | list[SummaryType]</code> <p>Evaluation summaries as DataFrame, list of dicts, or file path(s)</p> required <code>metric_column</code> <code>str</code> <p>Column name containing the metric to pivot</p> <code>\"test_error\"</code> <code>task_columns</code> <code>str | list[str]</code> <p>Column(s) defining unique tasks. Used as the pivot table index</p> <code>TASK_DEF_COLUMNS</code> <code>baseline_model</code> <code>str</code> <p>If provided, divide all scores by this model's scores to get relative performance</p> <code>None</code> <code>check_fev_version</code> <code>bool</code> <p>If True, check that fev_version in the summary is &gt;= LAST_BREAKING_VERSION.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pivot table with task combinations as index and model names as columns. Values are raw scores or relative scores (if <code>baseline_model</code> specified)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If duplicate model/task combinations exist, or results for <code>baseline_model</code> are missing when <code>baseline_model</code> is provided.</p> Source code in <code>src/fev/analysis.py</code> <pre><code>def pivot_table(\n    summaries: SummaryType | list[SummaryType],\n    metric_column: str = \"test_error\",\n    task_columns: str | list[str] = TASK_DEF_COLUMNS.copy(),\n    baseline_model: str | None = None,\n    check_fev_version: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Convert evaluation summaries into a pivot table for analysis.\n\n    Creates a matrix where rows represent tasks and columns represent models, with each\n    cell containing the specified metric value. Optionally normalizes all scores relative\n    to a baseline model.\n\n    Parameters\n    ----------\n    summaries : SummaryType | list[SummaryType]\n        Evaluation summaries as DataFrame, list of dicts, or file path(s)\n    metric_column : str, default \"test_error\"\n        Column name containing the metric to pivot\n    task_columns : str | list[str], default TASK_DEF_COLUMNS\n        Column(s) defining unique tasks. Used as the pivot table index\n    baseline_model : str, optional\n        If provided, divide all scores by this model's scores to get relative performance\n    check_fev_version : bool, default False\n        If True, check that fev_version in the summary is &gt;= LAST_BREAKING_VERSION.\n\n    Returns\n    -------\n    pd.DataFrame\n        Pivot table with task combinations as index and model names as columns.\n        Values are raw scores or relative scores (if `baseline_model` specified)\n\n    Raises\n    ------\n    ValueError\n        If duplicate model/task combinations exist, or results for `baseline_model` are missing when `baseline_model`\n        is provided.\n    \"\"\"\n    summaries = _load_summaries(summaries, check_fev_version=check_fev_version).astype({metric_column: \"float64\"})\n\n    if isinstance(task_columns, str):\n        task_columns = [task_columns]\n    metric_with_index = summaries.set_index(task_columns + [MODEL_COLUMN])[metric_column]\n    duplicates = metric_with_index.index.duplicated()\n    if duplicates.any():\n        duplicate_indices = metric_with_index.index[duplicates]\n        raise ValueError(\n            f\"Cannot unstack: duplicate index combinations found. First duplicates: {duplicate_indices[:5].tolist()}\"\n        )\n    pivot_df = metric_with_index.unstack()\n    if baseline_model is not None:\n        if baseline_model not in pivot_df.columns:\n            raise ValueError(\n                f\"baseline_model '{baseline_model}' not found. Available models: {pivot_df.columns.tolist()}\"\n            )\n        pivot_df = pivot_df.divide(pivot_df[baseline_model], axis=0)\n        if num_baseline_failures := pivot_df[baseline_model].isna().sum():\n            raise ValueError(\n                f\"Results for baseline_model '{baseline_model}' are missing for \"\n                f\"{num_baseline_failures} out of {len(pivot_df)} tasks.\"\n            )\n    return pivot_df\n</code></pre>"},{"location":"api/benchmark/","title":"Benchmark","text":""},{"location":"api/benchmark/#fev.Benchmark","title":"<code>Benchmark</code>","text":"<p>A time series forecasting benchmark consisting of multiple <code>Task</code> objects.</p> <p>Attributes:</p> Name Type Description <code>tasks</code> <code>list[Task]</code> <p>Collection of tasks in the benchmark.</p> Source code in <code>src/fev/benchmark.py</code> <pre><code>class Benchmark:\n    \"\"\"A time series forecasting benchmark consisting of multiple [`Task`][fev.Task] objects.\n\n    Attributes\n    ----------\n    tasks : list[Task]\n        Collection of tasks in the benchmark.\n    \"\"\"\n\n    def __init__(self, tasks: list[Task]):\n        for t in tasks:\n            if not isinstance(t, Task):\n                raise ValueError(f\"`tasks` must be a list of `Task` objects (got {type(t)})\")\n        self.tasks: list[Task] = tasks  # declare type explicitly to correctly show up in the docs\n\n    @classmethod\n    def from_yaml(cls, file_path: str | Path) -&gt; \"Benchmark\":\n        \"\"\"Load benchmark definition from a YAML file.\n\n        The YAML file should contain the key `'tasks'` with a list of dictionaries with task definitions.\n\n            tasks:\n            - dataset_path: autogluon/chronos_datasets\n              dataset_config: m4_hourly\n              horizon: 24\n              num_windows: 2\n            - dataset_path: autogluon/chronos_datasets\n              dataset_config: monash_cif_2016\n              horizon: 12\n\n        Parameters\n        ----------\n        file_path : str | Path\n            URL or path of a YAML file containing the task definitions.\n        \"\"\"\n        try:\n            if str(file_path).startswith((\"http://\", \"https://\")):\n                response = requests.get(file_path)\n                response.raise_for_status()\n                config = yaml.safe_load(response.text)\n            else:\n                with open(file_path) as file:\n                    config = yaml.safe_load(file)\n        except Exception:\n            raise ValueError(\"Failed to load the file\")\n\n        return cls.from_list(config[\"tasks\"])\n\n    @classmethod\n    def from_list(cls, task_configs: list[dict]) -&gt; \"Benchmark\":\n        \"\"\"Load benchmark definition from a list of dictionaries.\n\n        Each dictionary must follow the schema compatible with a `fev.task.Task`.\n        \"\"\"\n        return cls(tasks=[Task(**conf) for conf in task_configs])\n</code></pre>"},{"location":"api/benchmark/#fev.Benchmark-attributes","title":"Attributes","text":""},{"location":"api/benchmark/#fev.Benchmark.tasks","title":"<code>tasks: list[Task] = tasks</code>  <code>instance-attribute</code>","text":""},{"location":"api/benchmark/#fev.Benchmark-functions","title":"Functions","text":""},{"location":"api/benchmark/#fev.Benchmark.from_yaml","title":"<code>from_yaml(file_path: str | Path) -&gt; Benchmark</code>  <code>classmethod</code>","text":"<p>Load benchmark definition from a YAML file.</p> <p>The YAML file should contain the key <code>'tasks'</code> with a list of dictionaries with task definitions.</p> <pre><code>tasks:\n- dataset_path: autogluon/chronos_datasets\n  dataset_config: m4_hourly\n  horizon: 24\n  num_windows: 2\n- dataset_path: autogluon/chronos_datasets\n  dataset_config: monash_cif_2016\n  horizon: 12\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>URL or path of a YAML file containing the task definitions.</p> required Source code in <code>src/fev/benchmark.py</code> <pre><code>@classmethod\ndef from_yaml(cls, file_path: str | Path) -&gt; \"Benchmark\":\n    \"\"\"Load benchmark definition from a YAML file.\n\n    The YAML file should contain the key `'tasks'` with a list of dictionaries with task definitions.\n\n        tasks:\n        - dataset_path: autogluon/chronos_datasets\n          dataset_config: m4_hourly\n          horizon: 24\n          num_windows: 2\n        - dataset_path: autogluon/chronos_datasets\n          dataset_config: monash_cif_2016\n          horizon: 12\n\n    Parameters\n    ----------\n    file_path : str | Path\n        URL or path of a YAML file containing the task definitions.\n    \"\"\"\n    try:\n        if str(file_path).startswith((\"http://\", \"https://\")):\n            response = requests.get(file_path)\n            response.raise_for_status()\n            config = yaml.safe_load(response.text)\n        else:\n            with open(file_path) as file:\n                config = yaml.safe_load(file)\n    except Exception:\n        raise ValueError(\"Failed to load the file\")\n\n    return cls.from_list(config[\"tasks\"])\n</code></pre>"},{"location":"api/benchmark/#fev.Benchmark.from_list","title":"<code>from_list(task_configs: list[dict]) -&gt; Benchmark</code>  <code>classmethod</code>","text":"<p>Load benchmark definition from a list of dictionaries.</p> <p>Each dictionary must follow the schema compatible with a <code>fev.task.Task</code>.</p> Source code in <code>src/fev/benchmark.py</code> <pre><code>@classmethod\ndef from_list(cls, task_configs: list[dict]) -&gt; \"Benchmark\":\n    \"\"\"Load benchmark definition from a list of dictionaries.\n\n    Each dictionary must follow the schema compatible with a `fev.task.Task`.\n    \"\"\"\n    return cls(tasks=[Task(**conf) for conf in task_configs])\n</code></pre>"},{"location":"api/evaluation-window/","title":"EvaluationWindow","text":""},{"location":"api/evaluation-window/#fev.EvaluationWindow","title":"<code>EvaluationWindow</code>  <code>dataclass</code>","text":"<p>A single evaluation window on which the forecast accuracy is measured.</p> <p>Corresponds to a single train/test split of the time series data at the provided <code>cutoff</code>.</p> <p>You should never manually create <code>EvaluationWindow</code> objects. Instead, use <code>Task.iter_windows()</code> or <code>Task.get_window()</code> to obtain the evaluation windows corresponding to the task.</p> Source code in <code>src/fev/task.py</code> <pre><code>@dataclasses.dataclass\nclass EvaluationWindow:\n    \"\"\"\n    A single evaluation window on which the forecast accuracy is measured.\n\n    Corresponds to a single train/test split of the time series data at the provided `cutoff`.\n\n    You should never manually create `EvaluationWindow` objects. Instead, use [`Task.iter_windows()`][fev.Task.iter_windows]\n    or [`Task.get_window()`][fev.Task.get_window] to obtain the evaluation windows corresponding to the task.\n    \"\"\"\n\n    full_dataset: datasets.Dataset = dataclasses.field(repr=False)\n    cutoff: int | str\n    horizon: int\n    min_context_length: int\n    max_context_length: int | None\n    # Dataset info\n    id_column: str\n    timestamp_column: str\n    target_columns: list[str]\n    known_dynamic_columns: list[str]\n    past_dynamic_columns: list[str]\n    static_columns: list[str]\n\n    def _get_past_future_test_data(self) -&gt; tuple[datasets.Dataset, datasets.Dataset, datasets.Dataset]:\n        dataset = self.full_dataset.select_columns(\n            [self.id_column, self.timestamp_column]\n            + self.target_columns\n            + self.known_dynamic_columns\n            + self.past_dynamic_columns\n            + self.static_columns\n        )\n\n        past_data, future_data = utils.past_future_split(\n            dataset,\n            timestamp_column=self.timestamp_column,\n            cutoff=self.cutoff,\n            horizon=self.horizon,\n            min_context_length=self.min_context_length,\n            max_context_length=self.max_context_length,\n        )\n        if len(past_data) == 0:\n            raise ValueError(\n                \"All time series in the dataset are too short for the chosen cutoff, horizon and min_context_length\"\n            )\n\n        future_known = future_data.remove_columns(self.target_columns + self.past_dynamic_columns)\n        test_data = future_data.select_columns([self.id_column, self.timestamp_column] + self.target_columns)\n        return past_data, future_known, test_data\n\n    def get_input_data(self) -&gt; tuple[datasets.Dataset, datasets.Dataset]:\n        \"\"\"Get data available to the model at prediction time for this evaluation window.\n\n        To convert the input data to a different format, use [`fev.convert_input_data`][fev.convert_input_data].\n\n        Returns\n        -------\n        past_data : datasets.Dataset\n            Historical observations up to the cutoff point.\n            Contains: id, timestamps, target values, static covariates, and all dynamic covariates.\n\n            Columns corresponding to `id_column`, `timestamp_column`, `target_columns`, `static_columns`,\n            `past_dynamic_columns`, `known_dynamic_columns`.\n        future_data : datasets.Dataset\n            Known future information for the forecast horizon.\n\n            Columns corresponding to `id_column`, `timestamp_column`, `static_columns`, `known_dynamic_columns`.\n        \"\"\"\n        past_data, future_known, _ = self._get_past_future_test_data()\n        num_items_before = len(self.full_dataset)\n        num_items_after = len(past_data)\n\n        if num_items_after &lt; num_items_before:\n            logger.info(\n                f\"Dropped {num_items_before - num_items_after} out of {num_items_before} time series \"\n                f\"because they had fewer than min_context_length ({self.min_context_length}) \"\n                f\"observations before cutoff ({self.cutoff}) \"\n                f\"or fewer than horizon ({self.horizon}) \"\n                f\"observations after cutoff.\"\n            )\n\n        return past_data, future_known\n\n    def get_ground_truth(self) -&gt; datasets.Dataset:\n        \"\"\"Get ground truth future test data.\n\n        **This data should never be provided to the model!**\n\n        This is a convenience method that exists for debugging and additional evaluation.\n        \"\"\"\n        _, _, test_data = self._get_past_future_test_data()\n        return test_data\n\n    def compute_metrics(\n        self,\n        predictions: datasets.DatasetDict,\n        metrics: list[Metric],\n        seasonality: int,\n        quantile_levels: list[float],\n    ) -&gt; dict[str, float]:\n        \"\"\"Compute accuracy metrics on the predictions made for this window.\n\n        To compute metrics on your predictions, use [`Task.evaluation_summary`][fev.Task.evaluation_summary] instead.\n\n        This is a convenience method that exists for debugging and additional evaluation.\n        \"\"\"\n        past_data, _, test_data = self._get_past_future_test_data()\n        past_data.set_format(\"numpy\")\n        test_data.set_format(\"numpy\")\n\n        for target_column, predictions_for_column in predictions.items():\n            if len(predictions_for_column) != len(test_data):\n                raise ValueError(\n                    f\"Length of predictions for column {target_column} ({len(predictions)}) must \"\n                    f\"match the length of test data ({len(test_data)})\"\n                )\n\n        test_scores: dict[str, float] = {}\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n            for metric in metrics:\n                scores = []\n                for col in self.target_columns:\n                    scores.append(\n                        metric.compute(\n                            test_data=test_data,\n                            predictions=predictions[col],\n                            past_data=past_data,\n                            seasonality=seasonality,\n                            quantile_levels=quantile_levels,\n                            target_column=col,\n                        )\n                    )\n                test_scores[metric.name] = float(np.mean(scores))\n        return test_scores\n</code></pre>"},{"location":"api/evaluation-window/#fev.EvaluationWindow-attributes","title":"Attributes","text":""},{"location":"api/evaluation-window/#fev.EvaluationWindow.cutoff","title":"<code>cutoff: int | str</code>  <code>instance-attribute</code>","text":""},{"location":"api/evaluation-window/#fev.EvaluationWindow.horizon","title":"<code>horizon: int</code>  <code>instance-attribute</code>","text":""},{"location":"api/evaluation-window/#fev.EvaluationWindow.min_context_length","title":"<code>min_context_length: int</code>  <code>instance-attribute</code>","text":""},{"location":"api/evaluation-window/#fev.EvaluationWindow.max_context_length","title":"<code>max_context_length: int | None</code>  <code>instance-attribute</code>","text":""},{"location":"api/evaluation-window/#fev.EvaluationWindow.id_column","title":"<code>id_column: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/evaluation-window/#fev.EvaluationWindow.timestamp_column","title":"<code>timestamp_column: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/evaluation-window/#fev.EvaluationWindow.target_columns","title":"<code>target_columns: list[str]</code>  <code>instance-attribute</code>","text":""},{"location":"api/evaluation-window/#fev.EvaluationWindow.known_dynamic_columns","title":"<code>known_dynamic_columns: list[str]</code>  <code>instance-attribute</code>","text":""},{"location":"api/evaluation-window/#fev.EvaluationWindow.past_dynamic_columns","title":"<code>past_dynamic_columns: list[str]</code>  <code>instance-attribute</code>","text":""},{"location":"api/evaluation-window/#fev.EvaluationWindow.static_columns","title":"<code>static_columns: list[str]</code>  <code>instance-attribute</code>","text":""},{"location":"api/evaluation-window/#fev.EvaluationWindow-functions","title":"Functions","text":""},{"location":"api/evaluation-window/#fev.EvaluationWindow.get_input_data","title":"<code>get_input_data() -&gt; tuple[datasets.Dataset, datasets.Dataset]</code>","text":"<p>Get data available to the model at prediction time for this evaluation window.</p> <p>To convert the input data to a different format, use <code>fev.convert_input_data</code>.</p> <p>Returns:</p> Name Type Description <code>past_data</code> <code>Dataset</code> <p>Historical observations up to the cutoff point. Contains: id, timestamps, target values, static covariates, and all dynamic covariates.</p> <p>Columns corresponding to <code>id_column</code>, <code>timestamp_column</code>, <code>target_columns</code>, <code>static_columns</code>, <code>past_dynamic_columns</code>, <code>known_dynamic_columns</code>.</p> <code>future_data</code> <code>Dataset</code> <p>Known future information for the forecast horizon.</p> <p>Columns corresponding to <code>id_column</code>, <code>timestamp_column</code>, <code>static_columns</code>, <code>known_dynamic_columns</code>.</p> Source code in <code>src/fev/task.py</code> <pre><code>def get_input_data(self) -&gt; tuple[datasets.Dataset, datasets.Dataset]:\n    \"\"\"Get data available to the model at prediction time for this evaluation window.\n\n    To convert the input data to a different format, use [`fev.convert_input_data`][fev.convert_input_data].\n\n    Returns\n    -------\n    past_data : datasets.Dataset\n        Historical observations up to the cutoff point.\n        Contains: id, timestamps, target values, static covariates, and all dynamic covariates.\n\n        Columns corresponding to `id_column`, `timestamp_column`, `target_columns`, `static_columns`,\n        `past_dynamic_columns`, `known_dynamic_columns`.\n    future_data : datasets.Dataset\n        Known future information for the forecast horizon.\n\n        Columns corresponding to `id_column`, `timestamp_column`, `static_columns`, `known_dynamic_columns`.\n    \"\"\"\n    past_data, future_known, _ = self._get_past_future_test_data()\n    num_items_before = len(self.full_dataset)\n    num_items_after = len(past_data)\n\n    if num_items_after &lt; num_items_before:\n        logger.info(\n            f\"Dropped {num_items_before - num_items_after} out of {num_items_before} time series \"\n            f\"because they had fewer than min_context_length ({self.min_context_length}) \"\n            f\"observations before cutoff ({self.cutoff}) \"\n            f\"or fewer than horizon ({self.horizon}) \"\n            f\"observations after cutoff.\"\n        )\n\n    return past_data, future_known\n</code></pre>"},{"location":"api/evaluation-window/#fev.EvaluationWindow.get_ground_truth","title":"<code>get_ground_truth() -&gt; datasets.Dataset</code>","text":"<p>Get ground truth future test data.</p> <p>This data should never be provided to the model!</p> <p>This is a convenience method that exists for debugging and additional evaluation.</p> Source code in <code>src/fev/task.py</code> <pre><code>def get_ground_truth(self) -&gt; datasets.Dataset:\n    \"\"\"Get ground truth future test data.\n\n    **This data should never be provided to the model!**\n\n    This is a convenience method that exists for debugging and additional evaluation.\n    \"\"\"\n    _, _, test_data = self._get_past_future_test_data()\n    return test_data\n</code></pre>"},{"location":"api/evaluation-window/#fev.EvaluationWindow.compute_metrics","title":"<code>compute_metrics(predictions: datasets.DatasetDict, metrics: list[Metric], seasonality: int, quantile_levels: list[float]) -&gt; dict[str, float]</code>","text":"<p>Compute accuracy metrics on the predictions made for this window.</p> <p>To compute metrics on your predictions, use <code>Task.evaluation_summary</code> instead.</p> <p>This is a convenience method that exists for debugging and additional evaluation.</p> Source code in <code>src/fev/task.py</code> <pre><code>def compute_metrics(\n    self,\n    predictions: datasets.DatasetDict,\n    metrics: list[Metric],\n    seasonality: int,\n    quantile_levels: list[float],\n) -&gt; dict[str, float]:\n    \"\"\"Compute accuracy metrics on the predictions made for this window.\n\n    To compute metrics on your predictions, use [`Task.evaluation_summary`][fev.Task.evaluation_summary] instead.\n\n    This is a convenience method that exists for debugging and additional evaluation.\n    \"\"\"\n    past_data, _, test_data = self._get_past_future_test_data()\n    past_data.set_format(\"numpy\")\n    test_data.set_format(\"numpy\")\n\n    for target_column, predictions_for_column in predictions.items():\n        if len(predictions_for_column) != len(test_data):\n            raise ValueError(\n                f\"Length of predictions for column {target_column} ({len(predictions)}) must \"\n                f\"match the length of test data ({len(test_data)})\"\n            )\n\n    test_scores: dict[str, float] = {}\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n        for metric in metrics:\n            scores = []\n            for col in self.target_columns:\n                scores.append(\n                    metric.compute(\n                        test_data=test_data,\n                        predictions=predictions[col],\n                        past_data=past_data,\n                        seasonality=seasonality,\n                        quantile_levels=quantile_levels,\n                        target_column=col,\n                    )\n                )\n            test_scores[metric.name] = float(np.mean(scores))\n    return test_scores\n</code></pre>"},{"location":"api/metrics/","title":"Metrics","text":"<p>For the precise mathematical definitions of metrics, see AutoGluon documentation.</p> <p>Note: Currently, multivariate metrics are computed by first computing the univariate metric on each target column and then averaging the results, similar to the following: <pre><code>metric_value = np.mean(\n    [metric.compute_metric(test_data[col], predictions[col])\n    for col in task.target_columns]\n)\n</code></pre> For some metrics like WAPE, this leads to results that are different from first concatenating all target columns into a single array and computing the metric on it.</p>"},{"location":"api/metrics/#fev.metrics","title":"<code>metrics</code>","text":""},{"location":"api/metrics/#fev.metrics-classes","title":"Classes","text":""},{"location":"api/metrics/#fev.metrics.MAE","title":"<code>MAE</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Mean absolute error.</p> Source code in <code>src/fev/metrics.py</code> <pre><code>class MAE(Metric):\n    \"\"\"Mean absolute error.\"\"\"\n\n    def compute(\n        self,\n        *,\n        test_data: datasets.Dataset,\n        predictions: datasets.Dataset,\n        past_data: datasets.Dataset,\n        seasonality: int,\n        quantile_levels: list[float],\n        target_column: str = \"target\",\n    ):\n        y_test = self._get_y_test(test_data, target_column=target_column)\n        y_pred = np.array(predictions[PREDICTIONS])\n        return np.nanmean(np.abs(y_test - y_pred))\n</code></pre>"},{"location":"api/metrics/#fev.metrics.MAPE","title":"<code>MAPE</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Mean absolute percentage error.</p> Source code in <code>src/fev/metrics.py</code> <pre><code>class MAPE(Metric):\n    \"\"\"Mean absolute percentage error.\"\"\"\n\n    def compute(\n        self,\n        *,\n        test_data: datasets.Dataset,\n        predictions: datasets.Dataset,\n        past_data: datasets.Dataset,\n        seasonality: int,\n        quantile_levels: list[float],\n        target_column: str = \"target\",\n    ):\n        y_test = self._get_y_test(test_data, target_column=target_column)\n        y_pred = np.array(predictions[PREDICTIONS])\n        ratio = np.abs(y_test - y_pred) / np.abs(y_test)\n        return self._safemean(ratio)\n</code></pre>"},{"location":"api/metrics/#fev.metrics.MASE","title":"<code>MASE</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Mean absolute scaled error.</p> <p>Warning:     Items with undefined in-sample seasonal error (e.g., history shorter than <code>seasonality</code>,     all-NaN history, or zero seasonal error) are excluded from aggregation.</p> Source code in <code>src/fev/metrics.py</code> <pre><code>class MASE(Metric):\n    \"\"\"Mean absolute scaled error.\n\n    Warning:\n        Items with undefined in-sample seasonal error (e.g., history shorter than `seasonality`,\n        all-NaN history, or zero seasonal error) are excluded from aggregation.\n    \"\"\"\n\n    def __init__(self, epsilon: float = 0.0) -&gt; None:\n        self.epsilon = epsilon\n\n    def compute(\n        self,\n        *,\n        test_data: datasets.Dataset,\n        predictions: datasets.Dataset,\n        past_data: datasets.Dataset,\n        seasonality: int,\n        quantile_levels: list[float],\n        target_column: str = \"target\",\n    ):\n        y_test = self._get_y_test(test_data, target_column=target_column)\n        y_pred = np.array(predictions[PREDICTIONS])\n\n        seasonal_error = _abs_seasonal_error_per_item(\n            past_data=past_data, seasonality=seasonality, target_column=target_column\n        )\n        seasonal_error = np.clip(seasonal_error, self.epsilon, None)\n        return self._safemean(np.abs(y_test - y_pred) / seasonal_error[:, None])\n</code></pre>"},{"location":"api/metrics/#fev.metrics.MQL","title":"<code>MQL</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Mean quantile loss.</p> Source code in <code>src/fev/metrics.py</code> <pre><code>class MQL(Metric):\n    \"\"\"Mean quantile loss.\"\"\"\n\n    needs_quantiles: bool = True\n\n    def compute(\n        self,\n        *,\n        test_data: datasets.Dataset,\n        predictions: datasets.Dataset,\n        past_data: datasets.Dataset,\n        seasonality: int,\n        quantile_levels: list[float],\n        target_column: str = \"target\",\n    ):\n        if quantile_levels is None or len(quantile_levels) == 0:\n            raise ValueError(f\"{self.__class__.__name__} cannot be computed if quantile_levels is None\")\n        ql = _quantile_loss(\n            test_data=test_data,\n            predictions=predictions,\n            quantile_levels=quantile_levels,\n            target_column=target_column,\n        )\n        return np.nanmean(ql)\n</code></pre>"},{"location":"api/metrics/#fev.metrics.MSE","title":"<code>MSE</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Mean squared error.</p> Source code in <code>src/fev/metrics.py</code> <pre><code>class MSE(Metric):\n    \"\"\"Mean squared error.\"\"\"\n\n    def compute(\n        self,\n        *,\n        test_data: datasets.Dataset,\n        predictions: datasets.Dataset,\n        past_data: datasets.Dataset,\n        seasonality: int,\n        quantile_levels: list[float],\n        target_column: str = \"target\",\n    ):\n        y_test = self._get_y_test(test_data, target_column=target_column)\n        y_pred = np.array(predictions[PREDICTIONS])\n        return np.nanmean((y_test - y_pred) ** 2)\n</code></pre>"},{"location":"api/metrics/#fev.metrics.RMSE","title":"<code>RMSE</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Root mean squared error.</p> Source code in <code>src/fev/metrics.py</code> <pre><code>class RMSE(Metric):\n    \"\"\"Root mean squared error.\"\"\"\n\n    def compute(\n        self,\n        *,\n        test_data: datasets.Dataset,\n        predictions: datasets.Dataset,\n        past_data: datasets.Dataset,\n        seasonality: int,\n        quantile_levels: list[float],\n        target_column: str = \"target\",\n    ):\n        y_test = self._get_y_test(test_data, target_column=target_column)\n        y_pred = np.array(predictions[PREDICTIONS])\n        return np.sqrt(np.nanmean((y_test - y_pred) ** 2))\n</code></pre>"},{"location":"api/metrics/#fev.metrics.RMSLE","title":"<code>RMSLE</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Root mean squared logarithmic error.</p> Source code in <code>src/fev/metrics.py</code> <pre><code>class RMSLE(Metric):\n    \"\"\"Root mean squared logarithmic error.\"\"\"\n\n    def compute(\n        self,\n        *,\n        test_data: datasets.Dataset,\n        predictions: datasets.Dataset,\n        past_data: datasets.Dataset,\n        seasonality: int,\n        quantile_levels: list[float],\n        target_column: str = \"target\",\n    ):\n        y_test = self._get_y_test(test_data, target_column=target_column)\n        y_pred = np.array(predictions[PREDICTIONS])\n        return np.sqrt(np.nanmean((np.log1p(y_test) - np.log1p(y_pred)) ** 2))\n</code></pre>"},{"location":"api/metrics/#fev.metrics.RMSSE","title":"<code>RMSSE</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Root mean squared scaled error.</p> <p>Warning:     Items with undefined in-sample seasonal error (e.g., history shorter than <code>seasonality</code>,     all-NaN history, or zero seasonal error) are excluded from aggregation.</p> Source code in <code>src/fev/metrics.py</code> <pre><code>class RMSSE(Metric):\n    \"\"\"Root mean squared scaled error.\n\n    Warning:\n        Items with undefined in-sample seasonal error (e.g., history shorter than `seasonality`,\n        all-NaN history, or zero seasonal error) are excluded from aggregation.\n    \"\"\"\n\n    def __init__(self, epsilon: float = 0.0) -&gt; None:\n        self.epsilon = epsilon\n\n    def compute(\n        self,\n        *,\n        test_data: datasets.Dataset,\n        predictions: datasets.Dataset,\n        past_data: datasets.Dataset,\n        seasonality: int,\n        quantile_levels: list[float],\n        target_column: str = \"target\",\n    ):\n        y_test = self._get_y_test(test_data, target_column=target_column)\n        y_pred = np.array(predictions[PREDICTIONS])\n        seasonal_error = _squared_seasonal_error_per_item(\n            past_data, seasonality=seasonality, target_column=target_column\n        )\n        seasonal_error = np.clip(seasonal_error, self.epsilon, None)\n        return np.sqrt(self._safemean((y_test - y_pred) ** 2 / seasonal_error[:, None]))\n</code></pre>"},{"location":"api/metrics/#fev.metrics.SMAPE","title":"<code>SMAPE</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Symmetric mean absolute percentage error.</p> Source code in <code>src/fev/metrics.py</code> <pre><code>class SMAPE(Metric):\n    \"\"\"Symmetric mean absolute percentage error.\"\"\"\n\n    def compute(\n        self,\n        *,\n        test_data: datasets.Dataset,\n        predictions: datasets.Dataset,\n        past_data: datasets.Dataset,\n        seasonality: int,\n        quantile_levels: list[float],\n        target_column: str = \"target\",\n    ):\n        y_test = self._get_y_test(test_data, target_column=target_column)\n        y_pred = np.array(predictions[PREDICTIONS])\n        return self._safemean(2 * np.abs(y_test - y_pred) / (np.abs(y_test) + np.abs(y_pred)))\n</code></pre>"},{"location":"api/metrics/#fev.metrics.SQL","title":"<code>SQL</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Scaled quantile loss.</p> <p>Warning:     Items with undefined in-sample seasonal error (e.g., history shorter than <code>seasonality</code>,     all-NaN history, or zero seasonal error) are excluded from aggregation.</p> Source code in <code>src/fev/metrics.py</code> <pre><code>class SQL(Metric):\n    \"\"\"Scaled quantile loss.\n\n    Warning:\n        Items with undefined in-sample seasonal error (e.g., history shorter than `seasonality`,\n        all-NaN history, or zero seasonal error) are excluded from aggregation.\n    \"\"\"\n\n    needs_quantiles: bool = True\n\n    def __init__(self, epsilon: float = 0.0) -&gt; None:\n        self.epsilon = epsilon\n\n    def compute(\n        self,\n        *,\n        test_data: datasets.Dataset,\n        predictions: datasets.Dataset,\n        past_data: datasets.Dataset,\n        seasonality: int,\n        quantile_levels: list[float],\n        target_column: str = \"target\",\n    ):\n        ql = _quantile_loss(\n            test_data=test_data,\n            predictions=predictions,\n            quantile_levels=quantile_levels,\n            target_column=target_column,\n        )\n        ql_per_time_step = np.nanmean(ql, axis=2)  # [num_items, horizon]\n        seasonal_error = _abs_seasonal_error_per_item(\n            past_data=past_data, seasonality=seasonality, target_column=target_column\n        )\n        seasonal_error = np.clip(seasonal_error, self.epsilon, None)\n        return self._safemean(ql_per_time_step / seasonal_error[:, None])\n</code></pre>"},{"location":"api/metrics/#fev.metrics.WAPE","title":"<code>WAPE</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Weighted absolute percentage error.</p> Source code in <code>src/fev/metrics.py</code> <pre><code>class WAPE(Metric):\n    \"\"\"Weighted absolute percentage error.\"\"\"\n\n    def __init__(self, epsilon: float = 0.0) -&gt; None:\n        self.epsilon = epsilon\n\n    def compute(\n        self,\n        *,\n        test_data: datasets.Dataset,\n        predictions: datasets.Dataset,\n        past_data: datasets.Dataset,\n        seasonality: int,\n        quantile_levels: list[float],\n        target_column: str = \"target\",\n    ):\n        y_test = self._get_y_test(test_data, target_column=target_column)\n        y_pred = np.array(predictions[PREDICTIONS])\n\n        return np.nanmean(np.abs(y_test - y_pred)) / max(self.epsilon, np.nanmean(np.abs(y_test)))\n</code></pre>"},{"location":"api/metrics/#fev.metrics.WQL","title":"<code>WQL</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Weighted quantile loss.</p> Source code in <code>src/fev/metrics.py</code> <pre><code>class WQL(Metric):\n    \"\"\"Weighted quantile loss.\"\"\"\n\n    needs_quantiles: bool = True\n\n    def __init__(self, epsilon: float = 0.0) -&gt; None:\n        self.epsilon = epsilon\n\n    def compute(\n        self,\n        *,\n        test_data: datasets.Dataset,\n        predictions: datasets.Dataset,\n        past_data: datasets.Dataset,\n        seasonality: int,\n        quantile_levels: list[float],\n        target_column: str = \"target\",\n    ):\n        ql = _quantile_loss(\n            test_data=test_data,\n            predictions=predictions,\n            quantile_levels=quantile_levels,\n            target_column=target_column,\n        )\n        return np.nanmean(ql) / max(self.epsilon, np.nanmean(np.abs(np.array(test_data[target_column]))))\n</code></pre>"},{"location":"api/metrics/#fev.metrics-functions","title":"Functions","text":""},{"location":"api/metrics/#fev.metrics.get_metric","title":"<code>get_metric(metric: MetricConfig) -&gt; Metric</code>","text":"<p>Get a metric class by name or configuration.</p> Source code in <code>src/fev/metrics.py</code> <pre><code>def get_metric(metric: MetricConfig) -&gt; Metric:\n    \"\"\"Get a metric class by name or configuration.\"\"\"\n    metric_name = metric if isinstance(metric, str) else metric[\"name\"]\n    try:\n        metric_type = AVAILABLE_METRICS[metric_name.upper()]\n    except KeyError:\n        raise ValueError(\n            f\"Evaluation metric '{metric_name}' is not available. Available metrics: {sorted(AVAILABLE_METRICS)}\"\n        )\n\n    if isinstance(metric, str):\n        return metric_type()\n    elif isinstance(metric, dict):\n        return metric_type(**{k: v for k, v in metric.items() if k != \"name\"})\n    else:\n        raise ValueError(f\"Invalid metric configuration: {metric}\")\n</code></pre>"},{"location":"api/task/","title":"Task","text":""},{"location":"api/task/#fev.Task","title":"<code>Task</code>","text":"<p>A univariate or multivariate time series forecasting task.</p> <p>A <code>Task</code> stores all information uniquely identifying the task, such as path to the dataset, forecast horizon, evaluation metric and names of the target &amp; covariate columns.</p> <p>This object handles dataset loading, train/test splitting, and prediction evaluation for time series forecasting tasks.</p> <p>A single <code>Task</code> consists of one or more <code>EvaluationWindow</code> objects that can be accessed using <code>iter_windows()</code> or <code>get_window()</code> methods. After making predictions for each evaluation window, you can evaluate their accuracy using <code>evaluation_summary()</code>.</p> <p>Typical workflow: <pre><code>task = fev.Task(dataset_path=\"...\", num_windows=3, horizon=24)\n\npredictions_per_window = []\nfor window in task.iter_windows():\n    past_data, future_data = window.get_input_data()\n    predictions = model.predict(past_data, future_data)\n    predictions_per_window.append(predictions)\n\nsummary = task.evaluation_summary(predictions_per_window, model_name=\"my_model\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>Path to the time series dataset stored locally, on S3, or on Hugging Face Hub. See the Examples section below for information on how to load datasets from different sources.</p> required <code>dataset_config</code> <code>str | None</code> <p>Name of the configuration used when loading datasets from Hugging Face Hub. If <code>dataset_config</code> is provided, the datasets will be loaded from HF Hub. If <code>dataset_config=None</code>, the dataset will be loaded from a local or S3 path.</p> <code>None</code> <code>horizon</code> <code>int</code> <p>Length of the forecast horizon (in time steps).</p> <code>1</code> <code>num_windows</code> <code>int</code> <p>Number of rolling evaluation windows included in the task.</p> <code>1</code> <code>initial_cutoff</code> <code>int | str | None</code> <p>Starting position for the first evaluation window that separates past from future data.</p> <p>Can be specified as:</p> <ul> <li>Integer: Index position using Python indexing. <code>y[:initial_cutoff]</code> becomes the past/training data,     and <code>y[initial_cutoff:initial_cutoff+horizon]</code> becomes the first forecast horizon to predict.     Negative values are interpreted as steps from the end of the series.</li> <li>Timestamp string: Date or datetime (e.g., <code>\"2024-02-01\"</code>). Data up to and including this timestamp becomes     past data, and the next <code>horizon</code> observations become the first forecast horizon.</li> </ul> <p>If <code>None</code>, defaults to <code>-horizon - (num_windows - 1) * window_step_size</code>.</p> <p>Note: Time series that are too short for any evaluation window (i.e., have fewer than <code>min_context_length</code> observations before a cutoff or fewer than <code>horizon</code> observations after a cutoff) will be filtered out during data loading.</p> <code>None</code> <code>window_step_size</code> <code>int | str | None</code> <p>Step size between consecutive evaluation windows. Must be an integer if <code>initial_cutoff</code> is an integer. Can be an integer or pandas offset string (e.g., <code>'D'</code>, <code>'15min'</code>) if <code>initial_cutoff</code> is a timestamp. Defaults to <code>horizon</code>.</p> <code>horizon</code> <code>min_context_length</code> <code>int</code> <p>Time series with fewer than <code>min_context_length</code> observations before a cutoff will be ignored during evaluation.</p> <code>1</code> <code>max_context_length</code> <code>int | None</code> <p>If provided, the past time series will be shortened to at most this many observations.</p> <code>None</code> <code>seasonality</code> <code>int</code> <p>Seasonal period of the dataset (e.g., 24 for hourly data, 12 for monthly data). This parameter is used when computing metrics like Mean Absolute Scaled Error (MASE).</p> <code>1</code> <code>eval_metric</code> <code>str | dict[str, Any]</code> <p>Evaluation metric used for ultimate evaluation on the test set. Can be specified as either a single string with the metric's name or a dictionary containing a \"name\" key and extra hyperparameters for the metric. For example, MASE can also be specified as <code>{\"name\": \"MASE\", \"epsilon\": 0.0001}</code> to prevent zero denominators when scaling errors.</p> <code>'MASE'</code> <code>extra_metrics</code> <code>list[str] | list[dict[str, Any]]</code> <p>Additional metrics to be included in the results. Can be specified as a list of strings with the metric's name or a list of dictionaries. See documentation for <code>eval_metric</code> for more details.</p> <code>[]</code> <code>quantile_levels</code> <code>list[float]</code> <p>Quantiles that must be predicted. List of floats between 0 and 1 (for example, <code>[0.1, 0.5, 0.9]</code>).</p> <code>[]</code> <code>id_column</code> <code>str</code> <p>Name of the column with the unique identifier of each time series. This column will be casted to <code>string</code> dtype and the dataset will be sorted according to it.</p> <code>'id'</code> <code>timestamp_column</code> <code>str</code> <p>Name of the column with the timestamps of the observations.</p> <code>'timestamp'</code> <code>target</code> <code>str | list[str]</code> <p>Name of the column that must be predicted. If a string is provided, a univariate forecasting task is created. If a list of strings is provided, a multivariate forecasting task is created.</p> <code>'target'</code> <code>generate_univariate_targets_from</code> <code>list[str] | Literal['__ALL__'] | None</code> <p>If provided, a separate univariate time series will be created from each of the <code>generate_univariate_targets_from</code> columns. Only valid for univariate tasks.</p> <p>If set to <code>\"__ALL__\"</code>, then a separate univariate instance will be created from each column of type <code>Sequence</code>.</p> <p>For example, if <code>generate_univariate_targets_from = [\"X\", \"Y\"]</code> then the raw multivariate time series <code>{\"id\": \"A\", \"timestamp\": [...], \"X\": [...], \"Y\": [...]}</code> will be split into two univariate time series <code>{\"id\": \"A_X\", \"timestamp\": [...], \"target\": [...]}</code> and <code>{\"id\": \"A_Y\", \"timestamp\": [...], \"target\": [...]}</code>.</p> <code>None</code> <code>past_dynamic_columns</code> <code>list[str]</code> <p>Names of covariate columns that are known only in the past. These will be available in the past data, but not in the future data. An error will be raised if these columns are missing from the dataset.</p> <code>[]</code> <code>known_dynamic_columns</code> <code>list[str]</code> <p>Names of covariate columns that are known in both past and future. These will be available in past data and future data. An error will be raised if these columns are missing from the dataset.</p> <code>[]</code> <code>static_columns</code> <code>list[str]</code> <p>Names of columns containing static covariates that don't change over time. An error will be raised if these columns are missing from the dataset.</p> <code>[]</code> <code>task_name</code> <code>str | None</code> <p>Human-readable name for the task. Defaults to <code>dataset_config</code> for datasets stored on HF hub, and to the name of 2 parent directories for local or S3-based datasets.</p> <p>This field is only here for convenience and is not used for any validation when computing the results.</p> <code>None</code> <p>Examples:</p> <p>Dataset stored on the Hugging Face Hub</p> <pre><code>&gt;&gt;&gt; Task(dataset_path=\"autogluon/chronos_datasets\", dataset_config=\"m4_hourly\", ...)\n</code></pre> <p>Dataset stored as a parquet file (local or S3)</p> <pre><code>&gt;&gt;&gt; Task(dataset_path=\"s3://my-bucket/m4_hourly/data.parquet\", ...)\n</code></pre> <p>Dataset consisting of multiple parquet files (local or S3)</p> <pre><code>&gt;&gt;&gt; Task(dataset_path=\"s3://my-bucket/m4_hourly/*.parquet\", ...)\n</code></pre> Source code in <code>src/fev/task.py</code> <pre><code>@pydantic.dataclasses.dataclass(config={\"extra\": \"forbid\"})\nclass Task:\n    \"\"\"A univariate or multivariate time series forecasting task.\n\n    A `Task` stores all information uniquely identifying the task, such as path to the dataset, forecast horizon,\n    evaluation metric and names of the target &amp; covariate columns.\n\n    This object handles dataset loading, train/test splitting, and prediction evaluation for time series forecasting tasks.\n\n    A single `Task` consists of one or more [`EvaluationWindow`][fev.task.EvaluationWindow] objects that can be\n    accessed using [`iter_windows()`][fev.Task.iter_windows] or [`get_window()`][fev.Task.get_window] methods.\n    After making predictions for each evaluation window, you can evaluate their accuracy using [`evaluation_summary()`][fev.Task.evaluation_summary].\n\n    Typical workflow:\n    ```python\n    task = fev.Task(dataset_path=\"...\", num_windows=3, horizon=24)\n\n    predictions_per_window = []\n    for window in task.iter_windows():\n        past_data, future_data = window.get_input_data()\n        predictions = model.predict(past_data, future_data)\n        predictions_per_window.append(predictions)\n\n    summary = task.evaluation_summary(predictions_per_window, model_name=\"my_model\")\n    ```\n\n    Parameters\n    ----------\n    dataset_path : str\n        Path to the time series dataset stored locally, on S3, or on Hugging Face Hub. See the Examples section below\n        for information on how to load datasets from different sources.\n    dataset_config : str | None, default None\n        Name of the configuration used when loading datasets from Hugging Face Hub. If `dataset_config` is provided,\n        the datasets will be loaded from HF Hub. If `dataset_config=None`, the dataset will be loaded from a local or\n        S3 path.\n    horizon : int, default 1\n        Length of the forecast horizon (in time steps).\n    num_windows : int, default 1\n        Number of rolling evaluation windows included in the task.\n    initial_cutoff : int | str | None, default None\n        Starting position for the first evaluation window that separates past from future data.\n\n        Can be specified as:\n\n        - *Integer*: Index position using Python indexing. `y[:initial_cutoff]` becomes the past/training data,\n            and `y[initial_cutoff:initial_cutoff+horizon]` becomes the first forecast horizon to predict.\n            Negative values are interpreted as steps from the end of the series.\n        - *Timestamp string*: Date or datetime (e.g., `\"2024-02-01\"`). Data up to and including this timestamp becomes\n            past data, and the next `horizon` observations become the first forecast horizon.\n\n        If `None`, defaults to `-horizon - (num_windows - 1) * window_step_size`.\n\n        **Note**: Time series that are too short for any evaluation window (i.e., have fewer than `min_context_length`\n        observations before a cutoff or fewer than `horizon` observations after a cutoff) will be filtered out during\n        data loading.\n    window_step_size : int | str | None, default horizon\n        Step size between consecutive evaluation windows. Must be an integer if `initial_cutoff` is an integer.\n        Can be an integer or pandas offset string (e.g., `'D'`, `'15min'`) if `initial_cutoff` is a timestamp.\n        Defaults to `horizon`.\n    min_context_length : int, default 1\n        Time series with fewer than `min_context_length` observations before a cutoff will be ignored during evaluation.\n    max_context_length : int | None, default None\n        If provided, the past time series will be shortened to at most this many observations.\n    seasonality : int, default 1\n        Seasonal period of the dataset (e.g., 24 for hourly data, 12 for monthly data). This parameter is used when\n        computing metrics like Mean Absolute Scaled Error (MASE).\n    eval_metric : str | dict[str, Any], default 'MASE'\n        Evaluation metric used for ultimate evaluation on the test set. Can be specified as either a single string\n        with the metric's name or a dictionary containing a \"name\" key and extra hyperparameters for the metric.\n        For example, MASE can also be specified as `{\"name\": \"MASE\", \"epsilon\": 0.0001}` to prevent zero\n        denominators when scaling errors.\n    extra_metrics : list[str] | list[dict[str, Any]], default []\n        Additional metrics to be included in the results. Can be specified as a list of strings with the metric's\n        name or a list of dictionaries. See documentation for `eval_metric` for more details.\n    quantile_levels : list[float], default []\n        Quantiles that must be predicted. List of floats between 0 and 1 (for example, `[0.1, 0.5, 0.9]`).\n    id_column : str, default 'id'\n        Name of the column with the unique identifier of each time series.\n        This column will be casted to `string` dtype and the dataset will be sorted according to it.\n    timestamp_column : str, default 'timestamp'\n        Name of the column with the timestamps of the observations.\n    target : str | list[str], default 'target'\n        Name of the column that must be predicted. If a string is provided, a univariate forecasting task is created.\n        If a list of strings is provided, a multivariate forecasting task is created.\n    generate_univariate_targets_from : list[str] | Literal[\"__ALL__\"] | None, default None\n        If provided, a separate univariate time series will be created from each of the\n        `generate_univariate_targets_from` columns. Only valid for univariate tasks.\n\n        If set to `\"__ALL__\"`, then a separate univariate instance will be created from each column of type `Sequence`.\n\n        For example, if `generate_univariate_targets_from = [\"X\", \"Y\"]` then the raw multivariate time series\n        `{\"id\": \"A\", \"timestamp\": [...], \"X\": [...], \"Y\": [...]}` will be split into two univariate time series\n        `{\"id\": \"A_X\", \"timestamp\": [...], \"target\": [...]}` and `{\"id\": \"A_Y\", \"timestamp\": [...], \"target\": [...]}`.\n    past_dynamic_columns : list[str], default []\n        Names of covariate columns that are known only in the past. These will be available in the past data, but not\n        in the future data. An error will be raised if these columns are missing from the dataset.\n    known_dynamic_columns : list[str], default []\n        Names of covariate columns that are known in both past and future. These will be available in past data\n        and future data. An error will be raised if these columns are missing from the dataset.\n    static_columns : list[str], default []\n        Names of columns containing static covariates that don't change over time. An error will be raised if these\n        columns are missing from the dataset.\n    task_name : str | None, default None\n        Human-readable name for the task. Defaults to `dataset_config` for datasets stored on HF hub, and to the\n        name of 2 parent directories for local or S3-based datasets.\n\n        This field is only here for convenience and is not used for any validation when computing the results.\n\n    Examples\n    --------\n    Dataset stored on the Hugging Face Hub\n\n    &gt;&gt;&gt; Task(dataset_path=\"autogluon/chronos_datasets\", dataset_config=\"m4_hourly\", ...)\n\n    Dataset stored as a parquet file (local or S3)\n\n    &gt;&gt;&gt; Task(dataset_path=\"s3://my-bucket/m4_hourly/data.parquet\", ...)\n\n    Dataset consisting of multiple parquet files (local or S3)\n\n    &gt;&gt;&gt; Task(dataset_path=\"s3://my-bucket/m4_hourly/*.parquet\", ...)\n    \"\"\"\n\n    dataset_path: str\n    dataset_config: str | None = None\n    # Forecast horizon parameters\n    horizon: int = 1\n    num_windows: int = 1\n    initial_cutoff: int | str | None = None\n    window_step_size: int | str | None = None\n    min_context_length: int = 1\n    max_context_length: int | None = None\n    # Evaluation parameters\n    seasonality: int = 1\n    eval_metric: str | dict[str, Any] = \"MASE\"\n    extra_metrics: list[str | dict[str, Any]] = dataclasses.field(default_factory=list)\n    quantile_levels: list[float] = dataclasses.field(default_factory=list)\n    # Feature information\n    id_column: str = \"id\"\n    timestamp_column: str = \"timestamp\"\n    target: str | list[str] = \"target\"\n    generate_univariate_targets_from: list[str] | Literal[\"__ALL__\"] | None = None\n    known_dynamic_columns: list[str] = dataclasses.field(default_factory=list)\n    past_dynamic_columns: list[str] = dataclasses.field(default_factory=list)\n    static_columns: list[str] = dataclasses.field(default_factory=list)\n    task_name: str | None = None\n\n    def __post_init__(self):\n        if self.task_name is None:\n            if self.dataset_config is not None:\n                # HF Hub dataset -&gt; name of dataset_config\n                self.task_name = self.dataset_config\n            else:\n                # File dataset -&gt; names of up to 2 parent directories\n                # e.g. /home/foo/bar/data.parquet -&gt; foo/bar\n                self.task_name = \"/\".join(Path(self.dataset_path).parts[-3:-1])\n\n        assert self.num_windows &gt;= 1, \"`num_windows` must satisfy &gt;= 1\"\n        if self.window_step_size is None:\n            self.window_step_size = self.horizon\n        if isinstance(self.window_step_size, int):\n            assert self.window_step_size &gt;= 1, \"`window_step_size` must satisfy &gt;= 1\"\n        else:\n            offset = pd.tseries.frequencies.to_offset(self.window_step_size)\n            assert offset.n &gt;= 1, \"If `window_step_size` is a string, it must correspond to a positive timedelta\"\n            self.window_step_size = offset.freqstr\n\n        if self.initial_cutoff is None:\n            assert isinstance(self.window_step_size, int), (\n                \"If `initial_cutoff` is None, `window_step_size` must be an int\"\n            )\n            self.initial_cutoff = -self.horizon - (self.num_windows - 1) * self.window_step_size\n\n        if isinstance(self.initial_cutoff, int):\n            if not isinstance(self.window_step_size, int):\n                raise ValueError(\"`window_step_size` must be an int if `initial_cutoff` is an int\")\n            assert self.window_step_size &gt;= 1\n            max_allowed_cutoff = -self.horizon - (self.num_windows - 1) * self.window_step_size\n            if self.initial_cutoff &lt; 0 and self.initial_cutoff &gt; max_allowed_cutoff:\n                raise ValueError(\n                    \"Negative `initial_cutoff` must be &lt;= `-horizon - (num_windows - 1) * window_step_size`\"\n                )\n        else:\n            self.initial_cutoff = pd.Timestamp(self.initial_cutoff).isoformat()\n\n        assert all(0 &lt; q &lt; 1 for q in self.quantile_levels), \"All quantile_levels must satisfy 0 &lt; q &lt; 1\"\n        self.quantile_levels = sorted(self.quantile_levels)\n\n        metrics = [get_metric(m) for m in [self.eval_metric] + self.extra_metrics]\n\n        metric_names = [m.name for m in metrics]\n        duplicate_metric_names = {x for x in metric_names if metric_names.count(x) &gt; 1}\n        if duplicate_metric_names:\n            raise ValueError(\n                f\"Duplicate metric names found: {duplicate_metric_names}. Please configure \"\n                \"only one instance for each metric.\"\n            )\n\n        if len(self.quantile_levels) == 0:\n            for m in metrics:\n                if m.needs_quantiles:\n                    raise ValueError(f\"Please provide quantile_levels when using a quantile metric '{m.name}'\")\n\n        if self.min_context_length &lt; 1:\n            raise ValueError(\"`min_context_length` must satisfy &gt;= 1\")\n        if self.max_context_length is not None:\n            if self.max_context_length &lt; 1:\n                raise ValueError(\"If provided, `max_context_length` must satisfy &gt;= 1\")\n\n        if isinstance(self.target, list):\n            if len(self.target) &lt; 1:\n                raise ValueError(\"For multivariate tasks `target` must contain at least one entry\")\n            # Ensure that column names are sorted alphabetically so that univariate adapters return sorted data\n            self.target = sorted(self.target)\n\n        # Ensure that column names are sorted alphabetically for deterministic task comparison\n        self.known_dynamic_columns = sorted(self.known_dynamic_columns)\n        self.past_dynamic_columns = sorted(self.past_dynamic_columns)\n        self.static_columns = sorted(self.static_columns)\n\n        if self.generate_univariate_targets_from is not None and self.is_multivariate:\n            raise ValueError(\n                \"`generate_univariate_targets_from` cannot be used for multivariate tasks (when `target` is a list)\"\n            )\n\n        # Attributes computed after the dataset is loaded\n        self._full_dataset: datasets.Dataset | None = None\n        self._freq: str | None = None\n        self._dataset_fingerprint: str | None = None\n\n    @property\n    def cutoffs(self) -&gt; list[int] | list[str]:\n        \"\"\"Cutoffs corresponding to each `EvaluationWindow` in the task.\n\n        Computed based on `num_windows`, `initial_cutoff` and `window_step_size` attributes of the task.\n        \"\"\"\n        if isinstance(self.initial_cutoff, int):\n            assert isinstance(self.window_step_size, int)\n            return [self.initial_cutoff + window_idx * self.window_step_size for window_idx in range(self.num_windows)]\n        else:\n            assert isinstance(self.initial_cutoff, str)\n            if isinstance(self.window_step_size, str):\n                offset = pd.tseries.frequencies.to_offset(self.window_step_size)\n            else:\n                assert isinstance(self.window_step_size, int)\n                offset = pd.tseries.frequencies.to_offset(self.freq) * self.window_step_size\n\n            cutoffs = []\n            for window_idx in range(self.num_windows):\n                cutoff = pd.Timestamp(self.initial_cutoff)\n                # We don't add the offset for window_idx=0 to avoid applying an \"anchored\" offset\n                # (e.g. `Timestamp(\"2020-01-01\") + i * to_offset(\"ME\")` returns \"2020-01-31\" for i=0 and i=1)\n                if window_idx != 0:\n                    cutoff += window_idx * offset\n                cutoffs.append(cutoff.isoformat())\n            return cutoffs\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert task definition to a dictionary.\"\"\"\n        return dataclasses.asdict(self)\n\n    def load_full_dataset(\n        self,\n        storage_options: dict | None = None,\n        trust_remote_code: bool | None = None,\n        num_proc: int = DEFAULT_NUM_PROC,\n    ) -&gt; datasets.Dataset:\n        \"\"\"Load the full raw dataset with preprocessing applied.\n\n        This method validates the data, loads and preprocesses the dataset according to the task configuration,\n        including generating univariate targets if `generate_univariate_targets_from` is provided.\n\n        **Note:** This method is only provided for information and debugging purposes. For model evaluation, use\n        [`iter_windows()`][fev.Task.iter_windows] instead to get properly split train/test data.\n\n        Parameters\n        ----------\n        storage_options : dict, optional\n            Passed to `datasets.load_dataset()` for accessing remote datasets (e.g., S3 credentials).\n        trust_remote_code : bool, optional\n            Passed to `datasets.load_dataset()` for trusting remote code from Hugging Face Hub.\n        num_proc : int, default DEFAULT_NUM_PROC\n            Number of processes to use for dataset preprocessing.\n\n        Returns\n        -------\n        datasets.Dataset\n            The preprocessed dataset with all time series.\n        \"\"\"\n        if self._full_dataset is None:\n            self._full_dataset = self._load_dataset(\n                storage_options=storage_options, trust_remote_code=trust_remote_code, num_proc=num_proc\n            )\n        return self._full_dataset\n\n    def iter_windows(\n        self,\n        storage_options: dict | None = None,\n        trust_remote_code: bool | None = None,\n        num_proc: int = DEFAULT_NUM_PROC,\n    ) -&gt; Iterable[EvaluationWindow]:\n        \"\"\"Iterate over the rolling evaluation windows in the task.\n\n        Each window contains train/test splits at different cutoff points for time series\n        cross-validation. Use this method for model evaluation and benchmarking.\n\n        Parameters\n        ----------\n        storage_options : dict, optional\n            Passed to `datasets.load_dataset()` for accessing remote datasets (e.g., S3 credentials).\n        trust_remote_code : bool, optional\n            Passed to `datasets.load_dataset()` for trusting remote code from Hugging Face Hub.\n        num_proc : int, default DEFAULT_NUM_PROC\n            Number of processes to use for dataset preprocessing.\n\n        Yields\n        ------\n        EvaluationWindow\n            A single evaluation window at a specific cutoff containing the data needed to make and evaluate forecasts.\n\n        Examples\n        --------\n        &gt;&gt;&gt; for window in task.iter_windows():\n        ...     past_data, future_data = window.get_input_data()\n        ...     # Make predictions using past_data and future_data\n        \"\"\"\n        for window_idx in range(self.num_windows):\n            yield self.get_window(\n                window_idx, storage_options=storage_options, trust_remote_code=trust_remote_code, num_proc=num_proc\n            )\n\n    def get_window(\n        self,\n        window_idx: int,\n        storage_options: dict | None = None,\n        trust_remote_code: bool | None = None,\n        num_proc: int = DEFAULT_NUM_PROC,\n    ) -&gt; EvaluationWindow:\n        \"\"\"Get a single evaluation window by index.\n\n        Parameters\n        ----------\n        window_idx : int\n            Index of the evaluation window in [0, 1, ..., num_windows - 1].\n        storage_options : dict, optional\n            Passed to `datasets.load_dataset()` for accessing remote datasets (e.g., S3 credentials).\n        trust_remote_code : bool, optional\n            Passed to `datasets.load_dataset()` for trusting remote code from Hugging Face Hub.\n        num_proc : int, default DEFAULT_NUM_PROC\n            Number of processes to use for dataset preprocessing.\n\n        Returns\n        -------\n        EvaluationWindow\n            A single evaluation window at a specific cutoff containing the data needed to make and evaluate forecasts.\n        \"\"\"\n        full_dataset = self.load_full_dataset(\n            storage_options=storage_options, trust_remote_code=trust_remote_code, num_proc=num_proc\n        )\n        if window_idx &gt;= self.num_windows:\n            raise ValueError(f\"Window index {window_idx} is out of range (num_windows={self.num_windows})\")\n        return EvaluationWindow(\n            full_dataset=full_dataset,\n            cutoff=self.cutoffs[window_idx],\n            horizon=self.horizon,\n            min_context_length=self.min_context_length,\n            max_context_length=self.max_context_length,\n            id_column=self.id_column,\n            timestamp_column=self.timestamp_column,\n            target_columns=self.target_columns,\n            known_dynamic_columns=self.known_dynamic_columns,\n            past_dynamic_columns=self.past_dynamic_columns,\n            static_columns=self.static_columns,\n        )\n\n    @pydantic.model_validator(mode=\"before\")\n    @classmethod\n    def handle_deprecated_fields(cls, data: ArgsKwargs) -&gt; ArgsKwargs:\n        def _warn_and_rename_kwarg(data: ArgsKwargs, old_name: str, new_name: str) -&gt; ArgsKwargs:\n            assert data.kwargs is not None\n            if old_name in data.kwargs:\n                warnings.warn(\n                    f\"Field '{old_name}' is deprecated and will be removed in a future release. \"\n                    f\"Please use '{new_name}' instead\",\n                    category=FutureWarning,\n                    stacklevel=4,\n                )\n                data.kwargs[new_name] = data.kwargs.pop(old_name)\n            return data\n\n        if data.kwargs is not None:\n            if \"lead_time\" in data.kwargs:\n                # 'lead_time' was never used before, quietly ignore\n                data.kwargs.pop(\"lead_time\")\n            for old_name, new_name in DEPRECATED_TASK_FIELDS.items():\n                data = _warn_and_rename_kwarg(data, old_name, new_name)\n        return data\n\n    @property\n    def freq(self) -&gt; str:\n        \"\"\"Pandas string corresponding to the frequency of the time series in the dataset.\n\n        See [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#cutoff-aliases)\n        for the list of possible values.\n\n        This attribute is available after the dataset is loaded with `load_full_dataset`, `iter_windows` or `get_window`.\n        \"\"\"\n        if self._freq is None:\n            raise ValueError(\"Please load dataset first with `task.load_full_dataset()`\")\n        return self._freq\n\n    @property\n    def dynamic_columns(self) -&gt; list[str]:\n        \"\"\"List of dynamic covariates available in the task. Does not include the target columns.\"\"\"\n        return self.known_dynamic_columns + self.past_dynamic_columns\n\n    def _load_dataset(\n        self,\n        storage_options: dict | None = None,\n        trust_remote_code: bool | None = None,\n        num_proc: int = DEFAULT_NUM_PROC,\n    ) -&gt; datasets.Dataset:\n        \"\"\"Load the raw dataset and apply initial preprocessing based on the Task definition.\"\"\"\n        if self.dataset_config is not None:\n            # Load dataset from HF Hub\n            path = self.dataset_path\n            name = self.dataset_config\n            data_files = None\n        else:\n            # Load dataset from a local or remote file\n            dataset_format = Path(self.dataset_path).suffix.lstrip(\".\")\n            allowed_formats = [\"parquet\", \"arrow\"]\n            if dataset_format not in allowed_formats:\n                raise ValueError(f\"When loading dataset from file, path must end in one of {allowed_formats}.\")\n            path = dataset_format\n            name = None\n            data_files = self.dataset_path\n\n        if storage_options is None:\n            storage_options = {}\n\n        load_dataset_kwargs = dict(\n            path=path,\n            name=name,\n            data_files=data_files,\n            split=datasets.Split.TRAIN,\n            storage_options=copy.deepcopy(storage_options),\n            trust_remote_code=trust_remote_code,\n        )\n        try:\n            ds = datasets.load_dataset(\n                **load_dataset_kwargs,\n                # PatchedDownloadConfig fixes https://github.com/huggingface/datasets/issues/6598\n                download_config=utils.PatchedDownloadConfig(storage_options=copy.deepcopy(storage_options)),\n            )\n        except Exception:\n            raise RuntimeError(\n                \"Failed to load the dataset when calling `datasets.load_dataset` with arguments\\n\"\n                f\"{pprint.pformat(load_dataset_kwargs)}\"\n            )\n        # Since we loaded with split=TRAIN and streaming=False, ds is a datasets.Dataset object\n        assert isinstance(ds, datasets.Dataset)\n        ds.set_format(\"numpy\")\n\n        required_columns = self.known_dynamic_columns + self.past_dynamic_columns + self.static_columns\n        if self.generate_univariate_targets_from is None:\n            required_columns += self.target_columns\n        elif self.generate_univariate_targets_from == ALL_AVAILABLE_COLUMNS:\n            pass\n        else:\n            required_columns += self.generate_univariate_targets_from\n\n        utils.validate_time_series_dataset(\n            ds,\n            id_column=self.id_column,\n            timestamp_column=self.timestamp_column,\n            required_columns=required_columns,\n            num_proc=num_proc,\n        )\n\n        # Create separate instances from columns listed in `generate_univariate_targets_from`\n        if self.generate_univariate_targets_from is not None:\n            if self.generate_univariate_targets_from == ALL_AVAILABLE_COLUMNS:\n                generate_univariate_targets_from = [\n                    col\n                    for col, feat in ds.features.items()\n                    if isinstance(feat, datasets.Sequence) and col != self.timestamp_column\n                ]\n            else:\n                generate_univariate_targets_from = self.generate_univariate_targets_from\n            assert isinstance(self.target, str)\n            ds = utils.generate_univariate_targets_from_multivariate(\n                ds,\n                id_column=self.id_column,\n                new_target_column=self.target,\n                generate_univariate_targets_from=generate_univariate_targets_from,\n                num_proc=num_proc,\n            )\n\n        # Ensure that IDs are sorted alphabetically for consistent ordering\n        if ds.features[self.id_column].dtype != \"string\":\n            ds = ds.cast_column(self.id_column, datasets.Value(\"string\"))\n        ds = ds.sort(self.id_column)\n        self._freq = pd.infer_freq(ds[0][self.timestamp_column])\n        if self._freq is None:\n            raise ValueError(\"Dataset contains irregular timestamps\")\n\n        available_dynamic_columns, available_static_columns = utils.infer_column_types(\n            ds, id_column=self.id_column, timestamp_column=self.timestamp_column\n        )\n        missing_dynamic = set(self.dynamic_columns) - set(available_dynamic_columns)\n        if len(missing_dynamic) &gt; 0:\n            raise ValueError(f\"Dynamic columns not found in dataset: {sorted(missing_dynamic)}\")\n        missing_static = set(self.static_columns) - set(available_static_columns)\n        if len(missing_static) &gt; 0:\n            raise ValueError(f\"Static columns not found in dataset: {sorted(missing_static)}\")\n        self._dataset_fingerprint = utils.generate_fingerprint(ds)\n        return ds\n\n    @property\n    def dataset_info(self) -&gt; dict:\n        return {\n            \"id_column\": self.id_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"target\": self.target,\n            \"static_columns\": self.static_columns,\n            \"dynamic_columns\": self.dynamic_columns,\n            \"known_dynamic_columns\": self.known_dynamic_columns,\n            \"past_dynamic_columns\": self.past_dynamic_columns,\n        }\n\n    @property\n    def predictions_schema(self) -&gt; datasets.Features:\n        \"\"\"Describes the format that the predictions must follow.\n\n        Forecast must always include the key `\"predictions\"` corresponding to the point forecast.\n\n        The predictions must also include a key for each of the `quantile_levels`.\n        For example, if `quantile_levels = [0.1, 0.9]`, then keys `\"0.1\"` and `\"0.9\"` must be included in the forecast.\n        \"\"\"\n        predictions_length = self.horizon\n        predictions_schema = {\n            PREDICTIONS: datasets.Sequence(datasets.Value(\"float64\"), length=predictions_length),\n        }\n        for q in sorted(self.quantile_levels):\n            predictions_schema[str(q)] = datasets.Sequence(datasets.Value(\"float64\"), length=predictions_length)\n        return datasets.Features(predictions_schema)\n\n    def clean_and_validate_predictions(\n        self, predictions: datasets.DatasetDict | dict[str, list[dict]] | datasets.Dataset | list[dict]\n    ) -&gt; datasets.DatasetDict:\n        \"\"\"Convert predictions for a single window into the format needed for computing the metrics.\n\n        The following formats are supported for both multivariate and univariate tasks:\n\n        - `DatasetDict`: Must contain a single key for each target in `task.target_columns`. Each value in\n            the dict must be a `datasets.Dataset` with schema compatible with `task.predictions_schema`. This is the\n            recommended format for providing predictions.\n        - `dict[str, list[dict]]`: A dictionary with one key for each target in `task.target_columns`. Each value in\n            the dict must be a list of dictionaries, each dict following the schema in `task.predictions_schema`.\n\n        Additionally for univariate tasks, the following formats are supported:\n\n        - `datasets.Dataset`: A single `datasets.Dataset` with schema compatible with `task.predictions_schema`.\n        - `list[dict]`: A list of dictionaries, where each dict follows the schema in `task.predictions_schema`.\n\n        Returns\n        -------\n        predictions :\n            A `DatasetDict` where each key is the name of the target column and the corresponding value is a\n            `datasets.Dataset` with the predictions.\n        \"\"\"\n\n        def _to_dataset(preds: datasets.Dataset | list[dict]) -&gt; datasets.Dataset:\n            if isinstance(preds, list):\n                try:\n                    preds = datasets.Dataset.from_list(list(preds))\n                except Exception:\n                    raise ValueError(\n                        \"`datasets.Dataset.from_list(predictions)` failed. Please convert predictions to `datasets.Dataset` format.\"\n                    )\n            if not isinstance(preds, datasets.Dataset):\n                raise ValueError(f\"predictions must be of type `datasets.Dataset` (received {type(preds)})\")\n            return preds\n\n        if isinstance(predictions, datasets.DatasetDict):\n            pass\n        elif isinstance(predictions, dict):\n            predictions = datasets.DatasetDict({col: _to_dataset(preds) for col, preds in predictions.items()})\n        elif isinstance(predictions, (list, datasets.Dataset)):\n            predictions = datasets.DatasetDict({self.target_columns[0]: _to_dataset(predictions)})\n        else:\n            raise ValueError(\n                f\"Expected predictions to be a `DatasetDict`, `Dataset`, `list` or `dict` (got {type(predictions)})\"\n            )\n        if missing_columns := set(self.target_columns) - set(predictions.keys()):\n            raise ValueError(f\"Missing predictions for columns {missing_columns} (got {sorted(predictions.keys())})\")\n        predictions = predictions.cast(self.predictions_schema).with_format(\"numpy\")\n        for target_column, predictions_for_column in predictions.items():\n            self._assert_all_columns_finite(predictions_for_column)\n        return predictions\n\n    @staticmethod\n    def _assert_all_columns_finite(predictions: datasets.Dataset) -&gt; None:\n        for col in predictions.column_names:\n            nan_row_idx, _ = np.where(~np.isfinite(np.array(predictions[col])))\n            if len(nan_row_idx) &gt; 0:\n                raise ValueError(\n                    \"Predictions contain NaN or Inf values. \"\n                    f\"First invalid value encountered in column {col} for item {nan_row_idx[0]}:\\n\"\n                    f\"{predictions[int(nan_row_idx[0])]}\"\n                )\n\n    def evaluation_summary(\n        self,\n        predictions_per_window: Iterable[datasets.Dataset | list[dict] | datasets.DatasetDict | dict[str, list[dict]]],\n        model_name: str,\n        training_time_s: float | None = None,\n        inference_time_s: float | None = None,\n        trained_on_this_dataset: bool = False,\n        extra_info: dict | None = None,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Get a summary of the model performance for the given forecasting task.\n\n        Parameters\n        ----------\n        predictions_per_window : Iterable[datasets.Dataset | list[dict] | datasets.DatasetDict | dict[str, list[dict]]]\n            Predictions generated by the model for each evaluation window in the task.\n\n            The length of `predictions_per_window` must be equal to `task.num_windows`.\n\n            The predictions for each window must be formatted as described in [clean_and_validate_predictions][fev.Task.clean_and_validate_predictions].\n        model_name : str\n            Name of the model that generated the predictions.\n        training_time_s : float | None\n            Training time of the model for this task (in seconds).\n        inference_time_s : float | None\n            Total inference time to generate all predictions (in seconds).\n        trained_on_this_dataset : bool\n            Was the model trained on the dataset associated with this task? Set to False if the model is used in\n            zero-shot mode.\n        extra_info : dict | None\n            Optional dictionary with additional information that will be appended to the evaluation summary.\n\n        Returns\n        -------\n        summary : dict\n            Dictionary that summarizes the model performance on this task. Includes following keys:\n\n            - `model_name` - name of the model\n            - `test_error` - value of the `task.eval_metric` averaged over all evaluation windows (lower is better)\n            - all `Task` attributes obtained via `task.to_dict()`.\n            - values of `task.extra_metrics` averaged all evaluation windows\n            - `num_forecasts` - total number of forecasts made across all windows (accounting for multivariate targets)\n            - `dataset_fingerprint` - fingerprint of the dataset generated by the HF `datasets` library\n            - `trained_on_this_dataset` - whether the model was trained on the dataset used in the task\n            - `fev_version` - version of the `fev` package used to obtain the summary\n        \"\"\"\n        summary: dict[str, Any] = {\"model_name\": model_name}\n        summary.update(self.to_dict())\n        metrics = [get_metric(m) for m in [self.eval_metric] + self.extra_metrics]\n        eval_metric = metrics[0]\n\n        metrics_per_window = {metric.name: [] for metric in metrics}\n        if isinstance(predictions_per_window, (datasets.Dataset, datasets.DatasetDict, dict)):\n            raise ValueError(\n                f\"predictions_per_window must be iterable (e.g., a list) but got {type(predictions_per_window)}\"\n            )\n        # Use strict=True to raise error if num_predictions does not match num_windows\n        num_forecasts = 0\n        for predictions, window in zip(predictions_per_window, self.iter_windows(), strict=True):\n            cleaned_predictions = self.clean_and_validate_predictions(predictions)\n            # Count total forecasts: num_items * num_target_columns (per window)\n            num_forecasts += len(cleaned_predictions) * len(next(iter(cleaned_predictions.values())))\n            metric_scores = window.compute_metrics(\n                cleaned_predictions,\n                metrics=metrics,\n                seasonality=self.seasonality,\n                quantile_levels=self.quantile_levels,\n            )\n            for metric, value in metric_scores.items():\n                metrics_per_window[metric].append(value)\n        metrics_averaged = {metric_name: float(np.mean(values)) for metric_name, values in metrics_per_window.items()}\n        summary.update(\n            {\n                \"test_error\": metrics_averaged[eval_metric.name],\n                \"training_time_s\": training_time_s,\n                \"inference_time_s\": inference_time_s,\n                \"num_forecasts\": num_forecasts,\n                \"dataset_fingerprint\": self._dataset_fingerprint,\n                \"trained_on_this_dataset\": trained_on_this_dataset,\n                \"fev_version\": FEV_VERSION,\n                **metrics_averaged,\n            }\n        )\n        if extra_info is not None:\n            summary.update(extra_info)\n        return summary\n\n    @property\n    def is_multivariate(self) -&gt; bool:\n        \"\"\"Returns `True` if `task.target` is a `list`, `False` otherwise.\"\"\"\n        return isinstance(self.target, list)\n\n    @property\n    def target_columns(self) -&gt; list[str]:\n        \"\"\"A list including names of all target columns for this task.\n\n        Unlike `task.target` that can be a string or a list, `task.target_columns` is always a list of strings.\n        \"\"\"\n        if isinstance(self.target, list):\n            return self.target\n        else:\n            return [self.target]\n</code></pre>"},{"location":"api/task/#fev.Task-attributes","title":"Attributes","text":""},{"location":"api/task/#fev.Task.predictions_schema","title":"<code>predictions_schema: datasets.Features</code>  <code>property</code>","text":"<p>Describes the format that the predictions must follow.</p> <p>Forecast must always include the key <code>\"predictions\"</code> corresponding to the point forecast.</p> <p>The predictions must also include a key for each of the <code>quantile_levels</code>. For example, if <code>quantile_levels = [0.1, 0.9]</code>, then keys <code>\"0.1\"</code> and <code>\"0.9\"</code> must be included in the forecast.</p>"},{"location":"api/task/#fev.Task.freq","title":"<code>freq: str</code>  <code>property</code>","text":"<p>Pandas string corresponding to the frequency of the time series in the dataset.</p> <p>See pandas documentation for the list of possible values.</p> <p>This attribute is available after the dataset is loaded with <code>load_full_dataset</code>, <code>iter_windows</code> or <code>get_window</code>.</p>"},{"location":"api/task/#fev.Task.cutoffs","title":"<code>cutoffs: list[int] | list[str]</code>  <code>property</code>","text":"<p>Cutoffs corresponding to each <code>EvaluationWindow</code> in the task.</p> <p>Computed based on <code>num_windows</code>, <code>initial_cutoff</code> and <code>window_step_size</code> attributes of the task.</p>"},{"location":"api/task/#fev.Task.target_columns","title":"<code>target_columns: list[str]</code>  <code>property</code>","text":"<p>A list including names of all target columns for this task.</p> <p>Unlike <code>task.target</code> that can be a string or a list, <code>task.target_columns</code> is always a list of strings.</p>"},{"location":"api/task/#fev.Task.is_multivariate","title":"<code>is_multivariate: bool</code>  <code>property</code>","text":"<p>Returns <code>True</code> if <code>task.target</code> is a <code>list</code>, <code>False</code> otherwise.</p>"},{"location":"api/task/#fev.Task.dynamic_columns","title":"<code>dynamic_columns: list[str]</code>  <code>property</code>","text":"<p>List of dynamic covariates available in the task. Does not include the target columns.</p>"},{"location":"api/task/#fev.Task-functions","title":"Functions","text":""},{"location":"api/task/#fev.Task.get_window","title":"<code>get_window(window_idx: int, storage_options: dict | None = None, trust_remote_code: bool | None = None, num_proc: int = DEFAULT_NUM_PROC) -&gt; EvaluationWindow</code>","text":"<p>Get a single evaluation window by index.</p> <p>Parameters:</p> Name Type Description Default <code>window_idx</code> <code>int</code> <p>Index of the evaluation window in [0, 1, ..., num_windows - 1].</p> required <code>storage_options</code> <code>dict</code> <p>Passed to <code>datasets.load_dataset()</code> for accessing remote datasets (e.g., S3 credentials).</p> <code>None</code> <code>trust_remote_code</code> <code>bool</code> <p>Passed to <code>datasets.load_dataset()</code> for trusting remote code from Hugging Face Hub.</p> <code>None</code> <code>num_proc</code> <code>int</code> <p>Number of processes to use for dataset preprocessing.</p> <code>DEFAULT_NUM_PROC</code> <p>Returns:</p> Type Description <code>EvaluationWindow</code> <p>A single evaluation window at a specific cutoff containing the data needed to make and evaluate forecasts.</p> Source code in <code>src/fev/task.py</code> <pre><code>def get_window(\n    self,\n    window_idx: int,\n    storage_options: dict | None = None,\n    trust_remote_code: bool | None = None,\n    num_proc: int = DEFAULT_NUM_PROC,\n) -&gt; EvaluationWindow:\n    \"\"\"Get a single evaluation window by index.\n\n    Parameters\n    ----------\n    window_idx : int\n        Index of the evaluation window in [0, 1, ..., num_windows - 1].\n    storage_options : dict, optional\n        Passed to `datasets.load_dataset()` for accessing remote datasets (e.g., S3 credentials).\n    trust_remote_code : bool, optional\n        Passed to `datasets.load_dataset()` for trusting remote code from Hugging Face Hub.\n    num_proc : int, default DEFAULT_NUM_PROC\n        Number of processes to use for dataset preprocessing.\n\n    Returns\n    -------\n    EvaluationWindow\n        A single evaluation window at a specific cutoff containing the data needed to make and evaluate forecasts.\n    \"\"\"\n    full_dataset = self.load_full_dataset(\n        storage_options=storage_options, trust_remote_code=trust_remote_code, num_proc=num_proc\n    )\n    if window_idx &gt;= self.num_windows:\n        raise ValueError(f\"Window index {window_idx} is out of range (num_windows={self.num_windows})\")\n    return EvaluationWindow(\n        full_dataset=full_dataset,\n        cutoff=self.cutoffs[window_idx],\n        horizon=self.horizon,\n        min_context_length=self.min_context_length,\n        max_context_length=self.max_context_length,\n        id_column=self.id_column,\n        timestamp_column=self.timestamp_column,\n        target_columns=self.target_columns,\n        known_dynamic_columns=self.known_dynamic_columns,\n        past_dynamic_columns=self.past_dynamic_columns,\n        static_columns=self.static_columns,\n    )\n</code></pre>"},{"location":"api/task/#fev.Task.iter_windows","title":"<code>iter_windows(storage_options: dict | None = None, trust_remote_code: bool | None = None, num_proc: int = DEFAULT_NUM_PROC) -&gt; Iterable[EvaluationWindow]</code>","text":"<p>Iterate over the rolling evaluation windows in the task.</p> <p>Each window contains train/test splits at different cutoff points for time series cross-validation. Use this method for model evaluation and benchmarking.</p> <p>Parameters:</p> Name Type Description Default <code>storage_options</code> <code>dict</code> <p>Passed to <code>datasets.load_dataset()</code> for accessing remote datasets (e.g., S3 credentials).</p> <code>None</code> <code>trust_remote_code</code> <code>bool</code> <p>Passed to <code>datasets.load_dataset()</code> for trusting remote code from Hugging Face Hub.</p> <code>None</code> <code>num_proc</code> <code>int</code> <p>Number of processes to use for dataset preprocessing.</p> <code>DEFAULT_NUM_PROC</code> <p>Yields:</p> Type Description <code>EvaluationWindow</code> <p>A single evaluation window at a specific cutoff containing the data needed to make and evaluate forecasts.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; for window in task.iter_windows():\n...     past_data, future_data = window.get_input_data()\n...     # Make predictions using past_data and future_data\n</code></pre> Source code in <code>src/fev/task.py</code> <pre><code>def iter_windows(\n    self,\n    storage_options: dict | None = None,\n    trust_remote_code: bool | None = None,\n    num_proc: int = DEFAULT_NUM_PROC,\n) -&gt; Iterable[EvaluationWindow]:\n    \"\"\"Iterate over the rolling evaluation windows in the task.\n\n    Each window contains train/test splits at different cutoff points for time series\n    cross-validation. Use this method for model evaluation and benchmarking.\n\n    Parameters\n    ----------\n    storage_options : dict, optional\n        Passed to `datasets.load_dataset()` for accessing remote datasets (e.g., S3 credentials).\n    trust_remote_code : bool, optional\n        Passed to `datasets.load_dataset()` for trusting remote code from Hugging Face Hub.\n    num_proc : int, default DEFAULT_NUM_PROC\n        Number of processes to use for dataset preprocessing.\n\n    Yields\n    ------\n    EvaluationWindow\n        A single evaluation window at a specific cutoff containing the data needed to make and evaluate forecasts.\n\n    Examples\n    --------\n    &gt;&gt;&gt; for window in task.iter_windows():\n    ...     past_data, future_data = window.get_input_data()\n    ...     # Make predictions using past_data and future_data\n    \"\"\"\n    for window_idx in range(self.num_windows):\n        yield self.get_window(\n            window_idx, storage_options=storage_options, trust_remote_code=trust_remote_code, num_proc=num_proc\n        )\n</code></pre>"},{"location":"api/task/#fev.Task.evaluation_summary","title":"<code>evaluation_summary(predictions_per_window: Iterable[datasets.Dataset | list[dict] | datasets.DatasetDict | dict[str, list[dict]]], model_name: str, training_time_s: float | None = None, inference_time_s: float | None = None, trained_on_this_dataset: bool = False, extra_info: dict | None = None) -&gt; dict[str, Any]</code>","text":"<p>Get a summary of the model performance for the given forecasting task.</p> <p>Parameters:</p> Name Type Description Default <code>predictions_per_window</code> <code>Iterable[Dataset | list[dict] | DatasetDict | dict[str, list[dict]]]</code> <p>Predictions generated by the model for each evaluation window in the task.</p> <p>The length of <code>predictions_per_window</code> must be equal to <code>task.num_windows</code>.</p> <p>The predictions for each window must be formatted as described in clean_and_validate_predictions.</p> required <code>model_name</code> <code>str</code> <p>Name of the model that generated the predictions.</p> required <code>training_time_s</code> <code>float | None</code> <p>Training time of the model for this task (in seconds).</p> <code>None</code> <code>inference_time_s</code> <code>float | None</code> <p>Total inference time to generate all predictions (in seconds).</p> <code>None</code> <code>trained_on_this_dataset</code> <code>bool</code> <p>Was the model trained on the dataset associated with this task? Set to False if the model is used in zero-shot mode.</p> <code>False</code> <code>extra_info</code> <code>dict | None</code> <p>Optional dictionary with additional information that will be appended to the evaluation summary.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>summary</code> <code>dict</code> <p>Dictionary that summarizes the model performance on this task. Includes following keys:</p> <ul> <li><code>model_name</code> - name of the model</li> <li><code>test_error</code> - value of the <code>task.eval_metric</code> averaged over all evaluation windows (lower is better)</li> <li>all <code>Task</code> attributes obtained via <code>task.to_dict()</code>.</li> <li>values of <code>task.extra_metrics</code> averaged all evaluation windows</li> <li><code>num_forecasts</code> - total number of forecasts made across all windows (accounting for multivariate targets)</li> <li><code>dataset_fingerprint</code> - fingerprint of the dataset generated by the HF <code>datasets</code> library</li> <li><code>trained_on_this_dataset</code> - whether the model was trained on the dataset used in the task</li> <li><code>fev_version</code> - version of the <code>fev</code> package used to obtain the summary</li> </ul> Source code in <code>src/fev/task.py</code> <pre><code>def evaluation_summary(\n    self,\n    predictions_per_window: Iterable[datasets.Dataset | list[dict] | datasets.DatasetDict | dict[str, list[dict]]],\n    model_name: str,\n    training_time_s: float | None = None,\n    inference_time_s: float | None = None,\n    trained_on_this_dataset: bool = False,\n    extra_info: dict | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Get a summary of the model performance for the given forecasting task.\n\n    Parameters\n    ----------\n    predictions_per_window : Iterable[datasets.Dataset | list[dict] | datasets.DatasetDict | dict[str, list[dict]]]\n        Predictions generated by the model for each evaluation window in the task.\n\n        The length of `predictions_per_window` must be equal to `task.num_windows`.\n\n        The predictions for each window must be formatted as described in [clean_and_validate_predictions][fev.Task.clean_and_validate_predictions].\n    model_name : str\n        Name of the model that generated the predictions.\n    training_time_s : float | None\n        Training time of the model for this task (in seconds).\n    inference_time_s : float | None\n        Total inference time to generate all predictions (in seconds).\n    trained_on_this_dataset : bool\n        Was the model trained on the dataset associated with this task? Set to False if the model is used in\n        zero-shot mode.\n    extra_info : dict | None\n        Optional dictionary with additional information that will be appended to the evaluation summary.\n\n    Returns\n    -------\n    summary : dict\n        Dictionary that summarizes the model performance on this task. Includes following keys:\n\n        - `model_name` - name of the model\n        - `test_error` - value of the `task.eval_metric` averaged over all evaluation windows (lower is better)\n        - all `Task` attributes obtained via `task.to_dict()`.\n        - values of `task.extra_metrics` averaged all evaluation windows\n        - `num_forecasts` - total number of forecasts made across all windows (accounting for multivariate targets)\n        - `dataset_fingerprint` - fingerprint of the dataset generated by the HF `datasets` library\n        - `trained_on_this_dataset` - whether the model was trained on the dataset used in the task\n        - `fev_version` - version of the `fev` package used to obtain the summary\n    \"\"\"\n    summary: dict[str, Any] = {\"model_name\": model_name}\n    summary.update(self.to_dict())\n    metrics = [get_metric(m) for m in [self.eval_metric] + self.extra_metrics]\n    eval_metric = metrics[0]\n\n    metrics_per_window = {metric.name: [] for metric in metrics}\n    if isinstance(predictions_per_window, (datasets.Dataset, datasets.DatasetDict, dict)):\n        raise ValueError(\n            f\"predictions_per_window must be iterable (e.g., a list) but got {type(predictions_per_window)}\"\n        )\n    # Use strict=True to raise error if num_predictions does not match num_windows\n    num_forecasts = 0\n    for predictions, window in zip(predictions_per_window, self.iter_windows(), strict=True):\n        cleaned_predictions = self.clean_and_validate_predictions(predictions)\n        # Count total forecasts: num_items * num_target_columns (per window)\n        num_forecasts += len(cleaned_predictions) * len(next(iter(cleaned_predictions.values())))\n        metric_scores = window.compute_metrics(\n            cleaned_predictions,\n            metrics=metrics,\n            seasonality=self.seasonality,\n            quantile_levels=self.quantile_levels,\n        )\n        for metric, value in metric_scores.items():\n            metrics_per_window[metric].append(value)\n    metrics_averaged = {metric_name: float(np.mean(values)) for metric_name, values in metrics_per_window.items()}\n    summary.update(\n        {\n            \"test_error\": metrics_averaged[eval_metric.name],\n            \"training_time_s\": training_time_s,\n            \"inference_time_s\": inference_time_s,\n            \"num_forecasts\": num_forecasts,\n            \"dataset_fingerprint\": self._dataset_fingerprint,\n            \"trained_on_this_dataset\": trained_on_this_dataset,\n            \"fev_version\": FEV_VERSION,\n            **metrics_averaged,\n        }\n    )\n    if extra_info is not None:\n        summary.update(extra_info)\n    return summary\n</code></pre>"},{"location":"api/task/#fev.Task.clean_and_validate_predictions","title":"<code>clean_and_validate_predictions(predictions: datasets.DatasetDict | dict[str, list[dict]] | datasets.Dataset | list[dict]) -&gt; datasets.DatasetDict</code>","text":"<p>Convert predictions for a single window into the format needed for computing the metrics.</p> <p>The following formats are supported for both multivariate and univariate tasks:</p> <ul> <li><code>DatasetDict</code>: Must contain a single key for each target in <code>task.target_columns</code>. Each value in     the dict must be a <code>datasets.Dataset</code> with schema compatible with <code>task.predictions_schema</code>. This is the     recommended format for providing predictions.</li> <li><code>dict[str, list[dict]]</code>: A dictionary with one key for each target in <code>task.target_columns</code>. Each value in     the dict must be a list of dictionaries, each dict following the schema in <code>task.predictions_schema</code>.</li> </ul> <p>Additionally for univariate tasks, the following formats are supported:</p> <ul> <li><code>datasets.Dataset</code>: A single <code>datasets.Dataset</code> with schema compatible with <code>task.predictions_schema</code>.</li> <li><code>list[dict]</code>: A list of dictionaries, where each dict follows the schema in <code>task.predictions_schema</code>.</li> </ul> <p>Returns:</p> Name Type Description <code>predictions</code> <code>DatasetDict</code> <p>A <code>DatasetDict</code> where each key is the name of the target column and the corresponding value is a <code>datasets.Dataset</code> with the predictions.</p> Source code in <code>src/fev/task.py</code> <pre><code>def clean_and_validate_predictions(\n    self, predictions: datasets.DatasetDict | dict[str, list[dict]] | datasets.Dataset | list[dict]\n) -&gt; datasets.DatasetDict:\n    \"\"\"Convert predictions for a single window into the format needed for computing the metrics.\n\n    The following formats are supported for both multivariate and univariate tasks:\n\n    - `DatasetDict`: Must contain a single key for each target in `task.target_columns`. Each value in\n        the dict must be a `datasets.Dataset` with schema compatible with `task.predictions_schema`. This is the\n        recommended format for providing predictions.\n    - `dict[str, list[dict]]`: A dictionary with one key for each target in `task.target_columns`. Each value in\n        the dict must be a list of dictionaries, each dict following the schema in `task.predictions_schema`.\n\n    Additionally for univariate tasks, the following formats are supported:\n\n    - `datasets.Dataset`: A single `datasets.Dataset` with schema compatible with `task.predictions_schema`.\n    - `list[dict]`: A list of dictionaries, where each dict follows the schema in `task.predictions_schema`.\n\n    Returns\n    -------\n    predictions :\n        A `DatasetDict` where each key is the name of the target column and the corresponding value is a\n        `datasets.Dataset` with the predictions.\n    \"\"\"\n\n    def _to_dataset(preds: datasets.Dataset | list[dict]) -&gt; datasets.Dataset:\n        if isinstance(preds, list):\n            try:\n                preds = datasets.Dataset.from_list(list(preds))\n            except Exception:\n                raise ValueError(\n                    \"`datasets.Dataset.from_list(predictions)` failed. Please convert predictions to `datasets.Dataset` format.\"\n                )\n        if not isinstance(preds, datasets.Dataset):\n            raise ValueError(f\"predictions must be of type `datasets.Dataset` (received {type(preds)})\")\n        return preds\n\n    if isinstance(predictions, datasets.DatasetDict):\n        pass\n    elif isinstance(predictions, dict):\n        predictions = datasets.DatasetDict({col: _to_dataset(preds) for col, preds in predictions.items()})\n    elif isinstance(predictions, (list, datasets.Dataset)):\n        predictions = datasets.DatasetDict({self.target_columns[0]: _to_dataset(predictions)})\n    else:\n        raise ValueError(\n            f\"Expected predictions to be a `DatasetDict`, `Dataset`, `list` or `dict` (got {type(predictions)})\"\n        )\n    if missing_columns := set(self.target_columns) - set(predictions.keys()):\n        raise ValueError(f\"Missing predictions for columns {missing_columns} (got {sorted(predictions.keys())})\")\n    predictions = predictions.cast(self.predictions_schema).with_format(\"numpy\")\n    for target_column, predictions_for_column in predictions.items():\n        self._assert_all_columns_finite(predictions_for_column)\n    return predictions\n</code></pre>"},{"location":"api/task/#fev.Task.load_full_dataset","title":"<code>load_full_dataset(storage_options: dict | None = None, trust_remote_code: bool | None = None, num_proc: int = DEFAULT_NUM_PROC) -&gt; datasets.Dataset</code>","text":"<p>Load the full raw dataset with preprocessing applied.</p> <p>This method validates the data, loads and preprocesses the dataset according to the task configuration, including generating univariate targets if <code>generate_univariate_targets_from</code> is provided.</p> <p>Note: This method is only provided for information and debugging purposes. For model evaluation, use <code>iter_windows()</code> instead to get properly split train/test data.</p> <p>Parameters:</p> Name Type Description Default <code>storage_options</code> <code>dict</code> <p>Passed to <code>datasets.load_dataset()</code> for accessing remote datasets (e.g., S3 credentials).</p> <code>None</code> <code>trust_remote_code</code> <code>bool</code> <p>Passed to <code>datasets.load_dataset()</code> for trusting remote code from Hugging Face Hub.</p> <code>None</code> <code>num_proc</code> <code>int</code> <p>Number of processes to use for dataset preprocessing.</p> <code>DEFAULT_NUM_PROC</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>The preprocessed dataset with all time series.</p> Source code in <code>src/fev/task.py</code> <pre><code>def load_full_dataset(\n    self,\n    storage_options: dict | None = None,\n    trust_remote_code: bool | None = None,\n    num_proc: int = DEFAULT_NUM_PROC,\n) -&gt; datasets.Dataset:\n    \"\"\"Load the full raw dataset with preprocessing applied.\n\n    This method validates the data, loads and preprocesses the dataset according to the task configuration,\n    including generating univariate targets if `generate_univariate_targets_from` is provided.\n\n    **Note:** This method is only provided for information and debugging purposes. For model evaluation, use\n    [`iter_windows()`][fev.Task.iter_windows] instead to get properly split train/test data.\n\n    Parameters\n    ----------\n    storage_options : dict, optional\n        Passed to `datasets.load_dataset()` for accessing remote datasets (e.g., S3 credentials).\n    trust_remote_code : bool, optional\n        Passed to `datasets.load_dataset()` for trusting remote code from Hugging Face Hub.\n    num_proc : int, default DEFAULT_NUM_PROC\n        Number of processes to use for dataset preprocessing.\n\n    Returns\n    -------\n    datasets.Dataset\n        The preprocessed dataset with all time series.\n    \"\"\"\n    if self._full_dataset is None:\n        self._full_dataset = self._load_dataset(\n            storage_options=storage_options, trust_remote_code=trust_remote_code, num_proc=num_proc\n        )\n    return self._full_dataset\n</code></pre>"},{"location":"api/task/#fev.Task.to_dict","title":"<code>to_dict() -&gt; dict</code>","text":"<p>Convert task definition to a dictionary.</p> Source code in <code>src/fev/task.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert task definition to a dictionary.\"\"\"\n    return dataclasses.asdict(self)\n</code></pre>"},{"location":"api/utilities/","title":"Utility methods","text":"<p>This page contains the utility methods for converting input data and predictions.</p>"},{"location":"api/utilities/#functions","title":"Functions","text":""},{"location":"api/utilities/#fev.convert_input_data","title":"<code>convert_input_data(window: EvaluationWindow, adapter: Literal['pandas', 'datasets', 'gluonts', 'nixtla', 'darts', 'autogluon'] = 'pandas', *, as_univariate: bool = False, univariate_target_column: str = 'target', **kwargs) -&gt; Any</code>","text":"<p>Convert the output of <code>task.get_input_data()</code> to a format compatible with popular forecasting frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>EvaluationWindow</code> <p>Evaluation window for which input data must be converted.</p> required <code>adapter</code> <code>('pandas', 'datasets', 'gluonts', 'nixtla', 'darts', 'autogluon')</code> <p>Format to which the dataset must be converted.</p> <code>\"pandas\"</code> <code>as_univariate</code> <code>bool</code> <p>If <code>True</code>, the separate instances will be created from each target column before passing the data to the adapter. Covariate columns will not be affected, only targets will be modified.</p> <p>Setting <code>as_univariate=True</code> makes it easy to evaluate a univariate model on a multivariate task.</p> <p>Use <code>fev.combine_univariate_predictions_to_multivariate</code> to combine univariate predictions back to the multivariate format.</p> <code>False</code> <code>univariate_target_column</code> <code>str</code> <p>Target column name used when <code>as_univariate=True</code>. Only used by the <code>\"datasets\"</code> adapter.</p> <code>'target'</code> <code>**kwargs</code> <p>Keyword arguments passed to <code>EvaluationWindow.get_input_data()</code>.</p> <code>{}</code> Source code in <code>src/fev/adapters.py</code> <pre><code>def convert_input_data(\n    window: EvaluationWindow,\n    adapter: Literal[\"pandas\", \"datasets\", \"gluonts\", \"nixtla\", \"darts\", \"autogluon\"] = \"pandas\",\n    *,\n    as_univariate: bool = False,\n    univariate_target_column: str = \"target\",\n    **kwargs,\n) -&gt; Any:\n    \"\"\"Convert the output of `task.get_input_data()` to a format compatible with popular forecasting frameworks.\n\n    Parameters\n    ----------\n    window\n        Evaluation window for which input data must be converted.\n    adapter : {\"pandas\", \"datasets\", \"gluonts\", \"nixtla\", \"darts\", \"autogluon\"}\n        Format to which the dataset must be converted.\n    as_univariate\n        If `True`, the separate instances will be created from each target column before passing the data to the adapter.\n        Covariate columns will not be affected, only targets will be modified.\n\n        Setting `as_univariate=True` makes it easy to evaluate a univariate model on a multivariate task.\n\n        Use [`fev.combine_univariate_predictions_to_multivariate`][fev.combine_univariate_predictions_to_multivariate] to combine univariate predictions back to the\n        multivariate format.\n    univariate_target_column\n        Target column name used when `as_univariate=True`. Only used by the `\"datasets\"` adapter.\n    **kwargs\n        Keyword arguments passed to [`EvaluationWindow.get_input_data()`][fev.EvaluationWindow.get_input_data].\n    \"\"\"\n    past, future = window.get_input_data(**kwargs)\n\n    if as_univariate:\n        # Raise error if column called `univariate_target_column` already exists and it's not the *only* target column\n        if univariate_target_column in past.column_names and window.target_columns != [univariate_target_column]:\n            raise ValueError(\n                f\"Column '{univariate_target_column}' already exists. Choose a different univariate_target_column.\"\n            )\n        target_columns = [univariate_target_column]\n        if len(window.target_columns) &gt; 1:\n            # For multiple targets, we split each item into multiple instances (one per target column)\n            past = utils.generate_univariate_targets_from_multivariate(\n                past,\n                id_column=window.id_column,\n                new_target_column=univariate_target_column,\n                generate_univariate_targets_from=window.target_columns,\n            )\n            # We cannot apply generate_univariate_targets_from_multivariate to future since it does not contain target cols,\n            # so we just repeat each entry and insert the IDs from past, repeating entries as [0, 0, ..., 1, 1, ..., N -1, N - 1, ...]\n            original_column_order = future.column_names\n            future = future.select([i for i in range(len(future)) for _ in range(len(window.target_columns))])\n            future = future.remove_columns(window.id_column).add_column(\n                name=window.id_column, column=past[window.id_column]\n            )\n            future = future.select_columns(original_column_order)\n        else:\n            # For single target, we just rename the existing target to univariate_target_column\n            if univariate_target_column not in past.column_names:\n                past = past.rename_column(window.target_columns[0], univariate_target_column)\n    else:\n        target_columns = window.target_columns\n\n    if adapter not in DATASET_ADAPTERS:\n        raise KeyError(f\"`adapter` must be one of {list(DATASET_ADAPTERS)}\")\n    adapter_cls = DATASET_ADAPTERS[adapter]\n\n    return adapter_cls().convert_input_data(\n        past=past,\n        future=future,\n        target_columns=target_columns,\n        id_column=window.id_column,\n        timestamp_column=window.timestamp_column,\n        static_columns=window.static_columns,\n    )\n</code></pre>"},{"location":"api/utilities/#fev.combine_univariate_predictions_to_multivariate","title":"<code>combine_univariate_predictions_to_multivariate(predictions: datasets.Dataset | list[dict] | datasets.DatasetDict | dict[str, list[dict]], target_columns: list[str]) -&gt; datasets.DatasetDict</code>","text":"<p>Combine univariate predictions back into multivariate format.</p> <p>Assumes predictions are ordered by cycling through target columns. For example: if <code>target_columns = [\"X\", \"Y\"]</code>, predictions should be ordered as <code>[item1_X, item1_Y, item2_X, item2_Y, ...]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Dataset | list[dict] | DatasetDict | dict[str, list[dict]]</code> <p>Univariate predictions for a single evaluation window.</p> <p>For the list of accepted types, see <code>Task.clean_and_validate_predictions</code>.</p> required <code>target_columns</code> <code>list[str]</code> <p>List of target columns in the original <code>Task</code> / <code>EvaluationWindow</code>.</p> required <p>Returns:</p> Type Description <code>DatasetDict</code> <p>Predictions for the evaluation window converted to multivariate format.</p> Source code in <code>src/fev/utils.py</code> <pre><code>def combine_univariate_predictions_to_multivariate(\n    predictions: datasets.Dataset | list[dict] | datasets.DatasetDict | dict[str, list[dict]],\n    target_columns: list[str],\n) -&gt; datasets.DatasetDict:\n    \"\"\"Combine univariate predictions back into multivariate format.\n\n    Assumes predictions are ordered by cycling through target columns. For example: if `target_columns = [\"X\", \"Y\"]`,\n    predictions should be ordered as `[item1_X, item1_Y, item2_X, item2_Y, ...]`.\n\n    Parameters\n    ----------\n    predictions\n        Univariate predictions for a single evaluation window.\n\n        For the list of accepted types, see [`Task.clean_and_validate_predictions`][fev.Task.clean_and_validate_predictions].\n    target_columns\n        List of target columns in the original `Task` / `EvaluationWindow`.\n\n    Returns\n    -------\n    datasets.DatasetDict\n        Predictions for the evaluation window converted to multivariate format.\n    \"\"\"\n    if isinstance(predictions, (dict, datasets.DatasetDict)):\n        assert len(predictions) == 1, \"Univariate predictions must contain a single key/value\"\n        predictions = next(iter(predictions.values()))\n    if isinstance(predictions, list):\n        try:\n            predictions = datasets.Dataset.from_list(predictions)\n        except Exception:\n            raise ValueError(\n                \"`datasets.Dataset.from_list(predictions)` failed. Please convert predictions to `datasets.Dataset` format.\"\n            )\n    assert isinstance(predictions, datasets.Dataset), \"predictions must be a datasets.Dataset object\"\n    assert len(predictions) % len(target_columns) == 0, (\n        \"Number of predictions must be divisible by the number of target columns\"\n    )\n    prediction_dict = {}\n    for i, col in enumerate(target_columns):\n        prediction_dict[col] = predictions.select(range(i, len(predictions), len(target_columns)))\n    return datasets.DatasetDict(prediction_dict)\n</code></pre>"},{"location":"tutorials/01-quickstart/","title":"Quickstart","text":"<p>This notebook contains a minimal example of using <code>fev</code> to evaluate time series forecasting models.</p> In\u00a0[1]: Copied! <pre>import fev\n</pre> import fev In\u00a0[2]: Copied! <pre># Create a task from a dataset stored on Hugging Face Hub\ntask = fev.Task(\n    dataset_path=\"autogluon/chronos_datasets\",\n    dataset_config=\"ercot\",\n    horizon=24,\n    num_windows=2,\n)\n</pre> # Create a task from a dataset stored on Hugging Face Hub task = fev.Task(     dataset_path=\"autogluon/chronos_datasets\",     dataset_config=\"ercot\",     horizon=24,     num_windows=2, ) In\u00a0[3]: Copied! <pre># A task consists of multiple rolling evaluation windows\nfor window in task.iter_windows():\n    print(window)\n</pre> # A task consists of multiple rolling evaluation windows for window in task.iter_windows():     print(window) <pre>EvaluationWindow(cutoff=-48, horizon=24, min_context_length=1, max_context_length=None, id_column='id', timestamp_column='timestamp', target_columns=['target'], known_dynamic_columns=[], past_dynamic_columns=[], static_columns=[])\nEvaluationWindow(cutoff=-24, horizon=24, min_context_length=1, max_context_length=None, id_column='id', timestamp_column='timestamp', target_columns=['target'], known_dynamic_columns=[], past_dynamic_columns=[], static_columns=[])\n</pre> In\u00a0[4]: Copied! <pre># Load data available as input to the forecasting model\npast_data, future_data = task.get_window(0).get_input_data()\n</pre> # Load data available as input to the forecasting model past_data, future_data = task.get_window(0).get_input_data() In\u00a0[5]: Copied! <pre># past data before the forecast horizon.\npast_data\n</pre> # past data before the forecast horizon. past_data Out[5]: <pre>Dataset({\n    features: ['id', 'timestamp', 'target'],\n    num_rows: 8\n})</pre> In\u00a0[6]: Copied! <pre>past_data[0]\n</pre> past_data[0] Out[6]: <pre>{'id': np.str_('COAST'),\n 'timestamp': array(['2004-01-01T01:00:00.000000000', '2004-01-01T02:00:00.000000000',\n        '2004-01-01T03:00:00.000000000', ...,\n        '2021-08-29T22:00:00.000000000', '2021-08-29T23:00:00.000000000',\n        '2021-08-30T00:00:00.000000000'], dtype='datetime64[ns]'),\n 'target': array([ 7225.09,  6994.25,  6717.42, ..., 17114.34, 16091.05, 15081.16],\n       dtype=float32)}</pre> In\u00a0[7]: Copied! <pre># future data that is known at prediction time (item ID, future timestamps, static and known covariates)\nfuture_data\n</pre> # future data that is known at prediction time (item ID, future timestamps, static and known covariates) future_data Out[7]: <pre>Dataset({\n    features: ['id', 'timestamp'],\n    num_rows: 8\n})</pre> In\u00a0[8]: Copied! <pre>future_data[0]\n</pre> future_data[0] Out[8]: <pre>{'id': np.str_('COAST'),\n 'timestamp': array(['2021-08-30T01:00:00.000000000', '2021-08-30T02:00:00.000000000',\n        '2021-08-30T03:00:00.000000000', '2021-08-30T04:00:00.000000000',\n        '2021-08-30T05:00:00.000000000', '2021-08-30T06:00:00.000000000',\n        '2021-08-30T07:00:00.000000000', '2021-08-30T08:00:00.000000000',\n        '2021-08-30T09:00:00.000000000', '2021-08-30T10:00:00.000000000',\n        '2021-08-30T11:00:00.000000000', '2021-08-30T12:00:00.000000000',\n        '2021-08-30T13:00:00.000000000', '2021-08-30T14:00:00.000000000',\n        '2021-08-30T15:00:00.000000000', '2021-08-30T16:00:00.000000000',\n        '2021-08-30T17:00:00.000000000', '2021-08-30T18:00:00.000000000',\n        '2021-08-30T19:00:00.000000000', '2021-08-30T20:00:00.000000000',\n        '2021-08-30T21:00:00.000000000', '2021-08-30T22:00:00.000000000',\n        '2021-08-30T23:00:00.000000000', '2021-08-31T00:00:00.000000000'],\n       dtype='datetime64[ns]')}</pre> In\u00a0[9]: Copied! <pre>import numpy as np\n\n\ndef naive_forecast(y: list, horizon: int) -&gt; dict[str, list]:\n    # Make predictions for a single time series\n    return {\"predictions\": [y[np.isfinite(y)][-1] for _ in range(horizon)]}\n\npredictions_per_window = []\nfor window in task.iter_windows():\n    past_data, future_data = window.get_input_data()\n    predictions = [\n        naive_forecast(ts[task.target], task.horizon) for ts in past_data\n    ]\n    predictions_per_window.append(predictions)\n</pre> import numpy as np   def naive_forecast(y: list, horizon: int) -&gt; dict[str, list]:     # Make predictions for a single time series     return {\"predictions\": [y[np.isfinite(y)][-1] for _ in range(horizon)]}  predictions_per_window = [] for window in task.iter_windows():     past_data, future_data = window.get_input_data()     predictions = [         naive_forecast(ts[task.target], task.horizon) for ts in past_data     ]     predictions_per_window.append(predictions) In\u00a0[10]: Copied! <pre>eval_summary = task.evaluation_summary(predictions_per_window, model_name=\"naive\")\neval_summary\n</pre> eval_summary = task.evaluation_summary(predictions_per_window, model_name=\"naive\") eval_summary Out[10]: <pre>{'model_name': 'naive',\n 'dataset_path': 'autogluon/chronos_datasets',\n 'dataset_config': 'ercot',\n 'horizon': 24,\n 'num_windows': 2,\n 'initial_cutoff': -48,\n 'window_step_size': 24,\n 'min_context_length': 1,\n 'max_context_length': None,\n 'seasonality': 1,\n 'eval_metric': 'MASE',\n 'extra_metrics': [],\n 'quantile_levels': [],\n 'id_column': 'id',\n 'timestamp_column': 'timestamp',\n 'target': 'target',\n 'generate_univariate_targets_from': None,\n 'known_dynamic_columns': [],\n 'past_dynamic_columns': [],\n 'static_columns': [],\n 'task_name': 'ercot',\n 'test_error': 7.301416542738646,\n 'training_time_s': None,\n 'inference_time_s': None,\n 'dataset_fingerprint': '95b91121d95f89c8',\n 'trained_on_this_dataset': False,\n 'fev_version': '0.6.0',\n 'MASE': 7.301416542738646}</pre> <p>Evaluation summaries produced by different models on different tasks can be aggregated into a single table.</p> In\u00a0[11]: Copied! <pre>import pandas as pd\n\nsummaries = pd.read_csv(\"https://raw.githubusercontent.com/autogluon/fev/refs/heads/main/benchmarks/example/results/results.csv\")\nsummaries.head()\n</pre> import pandas as pd  summaries = pd.read_csv(\"https://raw.githubusercontent.com/autogluon/fev/refs/heads/main/benchmarks/example/results/results.csv\") summaries.head() Out[11]: model_name dataset_path dataset_config horizon num_windows initial_cutoff window_step_size min_context_length max_context_length seasonality ... past_dynamic_columns static_columns task_name test_error training_time_s inference_time_s dataset_fingerprint trained_on_this_dataset fev_version MASE 0 seasonal_naive autogluon/chronos_datasets monash_m1_quarterly 8 1 -8 8 1 NaN 4 ... [] [] monash_m1_quarterly 2.077537 0.0 1.687698 5dd7170c16393209 False 0.6.0 2.077537 1 ets autogluon/chronos_datasets monash_m1_quarterly 8 1 -8 8 1 NaN 4 ... [] [] monash_m1_quarterly 1.660810 0.0 4.366176 5dd7170c16393209 False 0.6.0 1.660810 2 theta autogluon/chronos_datasets monash_m1_quarterly 8 1 -8 8 1 NaN 4 ... [] [] monash_m1_quarterly 1.705247 0.0 0.125761 5dd7170c16393209 False 0.6.0 1.705247 3 seasonal_naive autogluon/chronos_datasets monash_electricity_weekly 8 2 -16 8 1 NaN 1 ... [] [] monash_electricity_weekly 2.535526 0.0 1.175560 b7cd1c9df3391815 False 0.6.0 2.535526 4 ets autogluon/chronos_datasets monash_electricity_weekly 8 2 -16 8 1 NaN 1 ... [] [] monash_electricity_weekly 2.552429 0.0 3.755289 b7cd1c9df3391815 False 0.6.0 2.552429 <p>5 rows \u00d7 28 columns</p> In\u00a0[12]: Copied! <pre># Evaluation summaries can be provided as dataframes, dicts, JSON or CSV files\nfev.leaderboard(summaries, baseline_model=\"seasonal_naive\")\n</pre> # Evaluation summaries can be provided as dataframes, dicts, JSON or CSV files fev.leaderboard(summaries, baseline_model=\"seasonal_naive\") Out[12]: skill_score win_rate median_training_time_s median_inference_time_s training_corpus_overlap num_failures model_name ets 0.133483 0.833333 0.0 3.755289 0.0 0 theta 0.105932 0.333333 0.0 0.125761 0.0 0 seasonal_naive 0.000000 0.333333 0.0 1.444558 0.0 0 <p>The <code>leaderboard</code> method not only summarizes the results into a single table, but also ensures that all task definitions match across different models. This ensures that the scores are comparable and the comparison is fair.</p>"},{"location":"tutorials/02-dataset-format/","title":"Dataset Format","text":"<p>This notebook answers the following questions:</p> <ol> <li>What dataset format does <code>fev</code> expect?</li> <li>How is this format different from other popular time series data formats?</li> <li>How to convert my dataset into a format expected by <code>fev</code>?</li> </ol> <p>For information on how to convert a <code>datasets.Dataset</code> into other popular time series data formats see notebook 04-models.ipynb.</p> In\u00a0[1]: Copied! <pre>import warnings\nimport datasets\n\nwarnings.simplefilter(\"ignore\")\ndatasets.disable_progress_bars()\n</pre> import warnings import datasets  warnings.simplefilter(\"ignore\") datasets.disable_progress_bars() In\u00a0[2]: Copied! <pre>ds = datasets.load_dataset(\"autogluon/chronos_datasets\", \"monash_kdd_cup_2018\", split=\"train\")\nds.set_format(\"numpy\")\nds\n</pre> ds = datasets.load_dataset(\"autogluon/chronos_datasets\", \"monash_kdd_cup_2018\", split=\"train\") ds.set_format(\"numpy\") ds Out[2]: <pre>Dataset({\n    features: ['id', 'timestamp', 'target', 'city', 'station', 'measurement'],\n    num_rows: 270\n})</pre> <p>Each entry corresponds to a single time series</p> In\u00a0[3]: Copied! <pre>ds[0]\n</pre> ds[0] Out[3]: <pre>{'id': np.str_('T000000'),\n 'timestamp': array(['2017-01-01T14:00:00.000', '2017-01-01T15:00:00.000',\n        '2017-01-01T16:00:00.000', ..., '2018-03-31T13:00:00.000',\n        '2018-03-31T14:00:00.000', '2018-03-31T15:00:00.000'],\n       dtype='datetime64[ms]'),\n 'target': array([453., 417., 395., ..., 132., 158., 118.], dtype=float32),\n 'city': np.str_('Beijing'),\n 'station': np.str_('aotizhongxin_aq'),\n 'measurement': np.str_('PM2.5')}</pre> <p>The <code>datasets</code> library conveniently stores metadata about the different features of the dataset.</p> In\u00a0[4]: Copied! <pre>ds.features\n</pre> ds.features Out[4]: <pre>{'id': Value(dtype='string', id=None),\n 'timestamp': Sequence(feature=Value(dtype='timestamp[ms]', id=None), length=-1, id=None),\n 'target': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None),\n 'city': Value(dtype='string', id=None),\n 'station': Value(dtype='string', id=None),\n 'measurement': Value(dtype='string', id=None)}</pre> In\u00a0[5]: Copied! <pre>import pandas as pd\nimport fev.utils\n</pre> import pandas as pd import fev.utils In\u00a0[6]: Copied! <pre>df = pd.read_csv(\"https://autogluon.s3.us-west-2.amazonaws.com/datasets/timeseries/grocery_sales/merged.csv\")\ndf.head()\n</pre> df = pd.read_csv(\"https://autogluon.s3.us-west-2.amazonaws.com/datasets/timeseries/grocery_sales/merged.csv\") df.head() Out[6]: item_id timestamp scaled_price promotion_email promotion_homepage unit_sales product_code product_category product_subcategory location_code 0 1062_101 2018-01-01 0.879130 0.0 0.0 636.0 1062 Beverages Fruit Juice Mango 101 1 1062_101 2018-01-08 0.994517 0.0 0.0 123.0 1062 Beverages Fruit Juice Mango 101 2 1062_101 2018-01-15 1.005513 0.0 0.0 391.0 1062 Beverages Fruit Juice Mango 101 3 1062_101 2018-01-22 1.000000 0.0 0.0 339.0 1062 Beverages Fruit Juice Mango 101 4 1062_101 2018-01-29 0.883309 0.0 0.0 661.0 1062 Beverages Fruit Juice Mango 101 In\u00a0[7]: Copied! <pre>ds = fev.utils.convert_long_df_to_hf_dataset(df, id_column=\"item_id\", static_columns=[\"product_code\", \"product_category\", \"product_subcategory\", \"location_code\"])\nds.features\n</pre> ds = fev.utils.convert_long_df_to_hf_dataset(df, id_column=\"item_id\", static_columns=[\"product_code\", \"product_category\", \"product_subcategory\", \"location_code\"]) ds.features Out[7]: <pre>{'item_id': Value(dtype='string', id=None),\n 'product_code': Value(dtype='int64', id=None),\n 'product_category': Value(dtype='string', id=None),\n 'product_subcategory': Value(dtype='string', id=None),\n 'location_code': Value(dtype='int64', id=None),\n 'timestamp': Sequence(feature=Value(dtype='timestamp[us]', id=None), length=-1, id=None),\n 'scaled_price': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None),\n 'promotion_email': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None),\n 'promotion_homepage': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None),\n 'unit_sales': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None)}</pre> In\u00a0[8]: Copied! <pre>ds.with_format(\"numpy\")[0]\n</pre> ds.with_format(\"numpy\")[0] Out[8]: <pre>{'item_id': np.str_('1062_101'),\n 'product_code': np.int64(1062),\n 'product_category': np.str_('Beverages'),\n 'product_subcategory': np.str_('Fruit Juice Mango'),\n 'location_code': np.int64(101),\n 'timestamp': array(['2018-01-01T00:00:00.000000', '2018-01-08T00:00:00.000000',\n        '2018-01-15T00:00:00.000000', '2018-01-22T00:00:00.000000',\n        '2018-01-29T00:00:00.000000', '2018-02-05T00:00:00.000000',\n        '2018-02-12T00:00:00.000000', '2018-02-19T00:00:00.000000',\n        '2018-02-26T00:00:00.000000', '2018-03-05T00:00:00.000000',\n        '2018-03-12T00:00:00.000000', '2018-03-19T00:00:00.000000',\n        '2018-03-26T00:00:00.000000', '2018-04-02T00:00:00.000000',\n        '2018-04-09T00:00:00.000000', '2018-04-16T00:00:00.000000',\n        '2018-04-23T00:00:00.000000', '2018-04-30T00:00:00.000000',\n        '2018-05-07T00:00:00.000000', '2018-05-14T00:00:00.000000',\n        '2018-05-21T00:00:00.000000', '2018-05-28T00:00:00.000000',\n        '2018-06-04T00:00:00.000000', '2018-06-11T00:00:00.000000',\n        '2018-06-18T00:00:00.000000', '2018-06-25T00:00:00.000000',\n        '2018-07-02T00:00:00.000000', '2018-07-09T00:00:00.000000',\n        '2018-07-16T00:00:00.000000', '2018-07-23T00:00:00.000000',\n        '2018-07-30T00:00:00.000000'], dtype='datetime64[us]'),\n 'scaled_price': array([0.8791298 , 0.99451727, 1.005513  , 1.        , 0.88330877,\n        0.8728938 , 0.8780195 , 0.8884807 , 0.9889777 , 1.0055426 ,\n        0.98920846, 1.0054836 , 1.        , 1.        , 1.011026  ,\n        0.9945471 , 0.99454623, 1.        , 0.99451727, 1.        ,\n        1.        , 0.9945471 , 1.011026  , 1.0054251 , 1.0054537 ,\n        1.        , 1.005513  , 1.        , 1.        , 1.0123464 ,\n        1.006248  ], dtype=float32),\n 'promotion_email': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       dtype=float32),\n 'promotion_homepage': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       dtype=float32),\n 'unit_sales': array([636., 123., 391., 339., 661., 513., 555., 485., 339., 230., 202.,\n        420., 418., 581., 472., 230., 176., 242., 270., 285., 258., 285.,\n        377., 339., 310., 231., 393., 447., 486., 284., 392.],\n       dtype=float32)}</pre> <p>To verify that the dataset was converted correctly, use the <code>fev.utils.validate_time_series_dataset</code> method.</p> In\u00a0[9]: Copied! <pre>fev.utils.validate_time_series_dataset(ds, id_column=\"item_id\", timestamp_column=\"timestamp\")\n</pre> fev.utils.validate_time_series_dataset(ds, id_column=\"item_id\", timestamp_column=\"timestamp\") <p>You can save the dataset to disk as a parquet file</p> In\u00a0[10]: Copied! <pre># ds.to_parquet(DATASET_PATH)\n</pre> # ds.to_parquet(DATASET_PATH) <p>Or directly push it to HF Hub</p> In\u00a0[11]: Copied! <pre># ds.push_to_hub(repo_id=YOUR_REPO_ID, config_name=CONFIG_NAME)\n</pre> # ds.push_to_hub(repo_id=YOUR_REPO_ID, config_name=CONFIG_NAME) <p>You can then use the path to your dataset when creating a <code>fev.Task</code>.</p>"},{"location":"tutorials/02-dataset-format/#what-dataset-format-does-fev-expect","title":"What dataset format does <code>fev</code> expect?\u00b6","text":"<p>We store time series datasets using the Hugging Face <code>datasets</code> library.</p> <p>We assume that all time series datasets obey the following schema:</p> <ul> <li>each dataset entry (=row) represents a single (univariate/multivariate) time series</li> <li>each entry contains<ul> <li>1/ a field of type <code>Sequence(timestamp)</code> that contains the timestamps of observations</li> <li>2/ at least one field of type <code>Sequence(float)</code> that can be used as the target time series</li> <li>3/ a field of type <code>string</code> that contains the unique ID of each time series</li> </ul> </li> <li>all fields of type <code>Sequence</code> have the same length</li> </ul> <p>A few notes about the above schema:</p> <ul> <li>The ID, timestamp and target fields can have arbitrary names. These names can be specified when creating an <code>fev.Task</code> object.</li> <li>In addition to the required fields above, the dataset can contain arbitrary other fields such as<ul> <li>extra dynamic columns of type <code>Sequence</code></li> <li>static features of type <code>Value</code> or <code>Image</code></li> </ul> </li> <li>The dataset itself contains no information about the forecasting task. For example, the dataset does not say which dynamic columns should be used as the target column or exogenous features, or which columns are known only in the past. Such design makes it easy to re-use the same dataset across multiple different tasks without data duplication.</li> </ul> <p>Here is an example of such dataset taken from https://huggingface.co/datasets/autogluon/chronos_datasets.</p>"},{"location":"tutorials/02-dataset-format/#what-are-the-advantages-of-the-fev-format-compared-to-other-common-formats","title":"What are the advantages of the \"fev format\" compared to other common formats?\u00b6","text":"<p>We find the above dataset format (\"fev format\") more convenient and practical compared to other popular formats for storing time series data.</p> <p>Long-format data frame is quite common for storing data, is human readable and widely used by practitioners.</p> item_id timestamp scaled_price promotion_email promotion_homepage unit_sales product_code product_category product_subcategory location_code 1062_101 2018-01-01 0.87913 0 0 636 1062 Beverages Fruit Juice Mango 101 1062_101 2018-01-08 0.994517 0 0 123 1062 Beverages Fruit Juice Mango 101 1062_101 2018-01-15 1.00551 0 0 391 1062 Beverages Fruit Juice Mango 101 1062_101 2018-01-22 1 0 0 339 1062 Beverages Fruit Juice Mango 101 ... ... ... ... ... ... ... ... ... ... <p>The long-format data frame has two main limitations compared to the \"fev format\".</p> <ul> <li>Static features either need to be unnecessarily duplicated for each row, or need to be stored in a separate file.<ul> <li>This becomes especially problematic if static features contain information such as images or text documents.</li> </ul> </li> <li>Dealing with large datasets is challenging.<ul> <li>Obtaining individual time series requires an expensive <code>groupby</code> operation.</li> <li>When sharding, we need custom logic to ensure that rows corresponding to the same <code>item_id</code> are kept in the same shard.</li> <li>We either constantly need to ensure that the rows are ordered chronologically, or need to sort the rows each time the data is used.</li> </ul> </li> </ul> <p>In contrast, the \"fev format\" can easily distinguish between static &amp; dynamic features using the <code>datasets.Features</code> metadata. Since one time series corresponds to a single row, it has no problems with sharding.</p> <p>GluonTS format is another popular choice for storing time series data (e.g., used in LOTSA).</p> <p>Each entry is encoded as a dictionary with a pre-defined schema shared across all datasets</p> <pre>{\n    \"start\": \"2024-01-01\", \n    \"freq\": \"1D\", \n    \"target\": [0.5, 1.2, ...], \n    \"feat_dynamic_real\": [[...]], \n    \"past_feat_dynamic_real\": [[...]], \n    \"feat_static_cat\": [...], \n    \"feat_static_real\": [...], \n    ...,\n}\n</pre> <p>This format is efficient and can be immediately consumed by some ML models. However, it also has some drawbacks compared to the \"fev format\".</p> <ul> <li>It hard-codes the forecasting task definition into the dataset (i.e., which columns are used as target, which columns are known in the future vs. only in the past). This often leads to data duplication.<ul> <li>For example, consider a dataset that contains energy demand &amp; weather time series for some region. If you want to evaluate a model in 3 settings (weather forecast is available for the future; weather is known only in the past; weather is ignored, only historic demand is available), you will need to create 3 copies of the dataset.</li> </ul> </li> <li>It only supports numeric data, so it's not future-proof.<ul> <li>Incorporating multimodal data such images or text into time series forecasting tasks is becoming popular. The GluonTS format cannot natively handle that.</li> </ul> </li> <li>It relies on pandas <code>freq</code> aliases staying consistent over time - which is something that we cannot take for granted.</li> </ul> <p>The \"fev format\" does not hard-code the task properties, natively deals with multimodal data and is not tied to the pandas versions.</p>"},{"location":"tutorials/02-dataset-format/#how-to-convert-my-dataset-into-a-format-expected-by-fev","title":"How to convert my dataset into a format expected by <code>fev</code>?\u00b6","text":"<p>If your dataset is stored in a long-format data frame, you can convert into an fev-compatible <code>datasets.Dataset</code> object using a helper function</p>"},{"location":"tutorials/03-tasks-and-benchmarks/","title":"Tasks & Benchmarks","text":"<p>This notebook covers the following topics:</p> <ol> <li>Defining a time series forecasting <code>Task</code> consisting of multiple <code>EvaluationWindow</code>s</li> <li>Multivariate and univariate forecasting</li> <li>Evaluation on a <code>Benchmark</code> consisting of multiple tasks</li> <li>Aggregating benchmark results</li> </ol> In\u00a0[1]: Copied! <pre>import warnings\nfrom pathlib import Path\n\nimport datasets\nimport numpy as np\nfrom tqdm.auto import tqdm\n\nimport fev\n\nwarnings.simplefilter(\"ignore\")\ndatasets.disable_progress_bars()\n</pre> import warnings from pathlib import Path  import datasets import numpy as np from tqdm.auto import tqdm  import fev  warnings.simplefilter(\"ignore\") datasets.disable_progress_bars() In\u00a0[2]: Copied! <pre>task = fev.Task(\n    dataset_path=\"autogluon/chronos_datasets\",\n    dataset_config=\"monash_cif_2016\",\n    horizon=12,\n)\n</pre> task = fev.Task(     dataset_path=\"autogluon/chronos_datasets\",     dataset_config=\"monash_cif_2016\",     horizon=12, ) <p>Dataset stored on S3</p> In\u00a0[3]: Copied! <pre># Dataset consisting of a single parquet / arrow file\ntask = fev.Task(\n    dataset_path=\"s3://autogluon/datasets/timeseries/m1_monthly/data.parquet\",\n    horizon=12,\n)\n# Dataset consisting of multiple parquet / arrow files\ntask = fev.Task(\n    dataset_path=\"s3://autogluon/datasets/timeseries/m1_monthly/*.parquet\",\n    horizon=12,\n)\n</pre> # Dataset consisting of a single parquet / arrow file task = fev.Task(     dataset_path=\"s3://autogluon/datasets/timeseries/m1_monthly/data.parquet\",     horizon=12, ) # Dataset consisting of multiple parquet / arrow files task = fev.Task(     dataset_path=\"s3://autogluon/datasets/timeseries/m1_monthly/*.parquet\",     horizon=12, ) <p>Dataset stored locally</p> In\u00a0[4]: Copied! <pre># Download dataset from HF Hub and save it locally\nds = datasets.load_dataset(\"autogluon/chronos_datasets\", name=\"m4_hourly\", split=\"train\")\nlocal_path = \"/tmp/m4_hourly/data.parquet\"\nds.to_parquet(local_path)\n\ntask = fev.Task(\n    dataset_path=local_path,\n    horizon=48,\n)\n</pre> # Download dataset from HF Hub and save it locally ds = datasets.load_dataset(\"autogluon/chronos_datasets\", name=\"m4_hourly\", split=\"train\") local_path = \"/tmp/m4_hourly/data.parquet\" ds.to_parquet(local_path)  task = fev.Task(     dataset_path=local_path,     horizon=48, ) In\u00a0[5]: Copied! <pre>import pandas as pd\n# Create a toy dataset with a single time series\nts = {\n    \"id\": \"A\",\n    \"timestamp\": pd.date_range(\"2025-01-01\", freq=\"D\", periods=10),\n    \"target\": list(range(10)),\n}\nds = datasets.Dataset.from_list([ts])\ndataset_path = \"/tmp/toy_dataset.parquet\"\nds.to_parquet(dataset_path);\n</pre> import pandas as pd # Create a toy dataset with a single time series ts = {     \"id\": \"A\",     \"timestamp\": pd.date_range(\"2025-01-01\", freq=\"D\", periods=10),     \"target\": list(range(10)), } ds = datasets.Dataset.from_list([ts]) dataset_path = \"/tmp/toy_dataset.parquet\" ds.to_parquet(dataset_path); <p>We now construct a <code>Task</code> with 2 evaluation windows based on this toy dataset.</p> In\u00a0[6]: Copied! <pre>task = fev.Task(\n    dataset_path=dataset_path,\n    horizon=3,\n    num_windows=2,\n)\n\n# Show the original dataset before any splits (for reference only)\nfull_dataset = task.load_full_dataset()\nprint(full_dataset)\nprint(full_dataset[0])\n</pre> task = fev.Task(     dataset_path=dataset_path,     horizon=3,     num_windows=2, )  # Show the original dataset before any splits (for reference only) full_dataset = task.load_full_dataset() print(full_dataset) print(full_dataset[0]) <pre>Dataset({\n    features: ['id', 'timestamp', 'target'],\n    num_rows: 1\n})\n{'id': np.str_('A'), 'timestamp': array(['2025-01-01T00:00:00.000000000', '2025-01-02T00:00:00.000000000',\n       '2025-01-03T00:00:00.000000000', '2025-01-04T00:00:00.000000000',\n       '2025-01-05T00:00:00.000000000', '2025-01-06T00:00:00.000000000',\n       '2025-01-07T00:00:00.000000000', '2025-01-08T00:00:00.000000000',\n       '2025-01-09T00:00:00.000000000', '2025-01-10T00:00:00.000000000'],\n      dtype='datetime64[ns]'), 'target': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}\n</pre> <p>Now let's examine how the data is split across the 2 evaluation windows:</p> In\u00a0[7]: Copied! <pre># Show how data is split across the 2 evaluation windows\nfor window_index, window in enumerate(task.iter_windows()):\n    past, future = window.get_input_data()\n    ground_truth = window.get_ground_truth()\n    print(f\"Window {window_index} (cutoff={window.cutoff}):\")\n    print(f\"  Past data:    {past[0]['target']}\")\n    print(f\"  Ground truth: {ground_truth[0]['target']}\")\n</pre> # Show how data is split across the 2 evaluation windows for window_index, window in enumerate(task.iter_windows()):     past, future = window.get_input_data()     ground_truth = window.get_ground_truth()     print(f\"Window {window_index} (cutoff={window.cutoff}):\")     print(f\"  Past data:    {past[0]['target']}\")     print(f\"  Ground truth: {ground_truth[0]['target']}\") <pre>Window 0 (cutoff=-6):\n  Past data:    [0 1 2 3]\n  Ground truth: [4 5 6]\nWindow 1 (cutoff=-3):\n  Past data:    [0 1 2 3 4 5 6]\n  Ground truth: [7 8 9]\n</pre> In\u00a0[8]: Copied! <pre># Example 1: Start evaluation earlier with initial_cutoff\ntask = fev.Task(\n    dataset_path=dataset_path,\n    horizon=3,\n    num_windows=2,\n    initial_cutoff=-8,\n)\n\nfor window_index, window in enumerate(task.iter_windows()):\n    past, future = window.get_input_data()\n    ground_truth = window.get_ground_truth()\n    print(f\"Window {window_index} (cutoff={window.cutoff}):\")\n    print(f\"  Past data:    {past[0]['target']}\")\n    print(f\"  Ground truth: {ground_truth[0]['target']}\")\n</pre> # Example 1: Start evaluation earlier with initial_cutoff task = fev.Task(     dataset_path=dataset_path,     horizon=3,     num_windows=2,     initial_cutoff=-8, )  for window_index, window in enumerate(task.iter_windows()):     past, future = window.get_input_data()     ground_truth = window.get_ground_truth()     print(f\"Window {window_index} (cutoff={window.cutoff}):\")     print(f\"  Past data:    {past[0]['target']}\")     print(f\"  Ground truth: {ground_truth[0]['target']}\") <pre>Window 0 (cutoff=-8):\n  Past data:    [0 1]\n  Ground truth: [2 3 4]\nWindow 1 (cutoff=-5):\n  Past data:    [0 1 2 3 4]\n  Ground truth: [5 6 7]\n</pre> In\u00a0[9]: Copied! <pre># Example 2: Use smaller window_step_size\ntask = fev.Task(\n    dataset_path=dataset_path,\n    horizon=3,\n    num_windows=2,\n    window_step_size=1,\n)\n\nfor window_index, window in enumerate(task.iter_windows()):\n    past, future = window.get_input_data()\n    ground_truth = window.get_ground_truth()\n    print(f\"Window {window_index} (cutoff={window.cutoff}):\")\n    print(f\"  Past data:    {past[0]['target']}\")\n    print(f\"  Ground truth: {ground_truth[0]['target']}\")\n</pre> # Example 2: Use smaller window_step_size task = fev.Task(     dataset_path=dataset_path,     horizon=3,     num_windows=2,     window_step_size=1, )  for window_index, window in enumerate(task.iter_windows()):     past, future = window.get_input_data()     ground_truth = window.get_ground_truth()     print(f\"Window {window_index} (cutoff={window.cutoff}):\")     print(f\"  Past data:    {past[0]['target']}\")     print(f\"  Ground truth: {ground_truth[0]['target']}\") <pre>Window 0 (cutoff=-4):\n  Past data:    [0 1 2 3 4 5]\n  Ground truth: [6 7 8]\nWindow 1 (cutoff=-3):\n  Past data:    [0 1 2 3 4 5 6]\n  Ground truth: [7 8 9]\n</pre> <p>You can also set <code>initial_cutoff</code> and <code>window_step_size</code> for pandas-compatible time strings.</p> In\u00a0[10]: Copied! <pre># Example 3: Use pandas timestamp-like strings\ntask = fev.Task(\n    dataset_path=dataset_path,\n    horizon=3,\n    num_windows=2,\n    initial_cutoff=\"2025-01-05\",\n    window_step_size=\"2D\",\n)\n\nfor window_index, window in enumerate(task.iter_windows()):\n    past, future = window.get_input_data()\n    ground_truth = window.get_ground_truth()\n    print(f\"Window {window_index} (cutoff={window.cutoff}):\")\n    print(f\"  Past data:    {past[0]['target']}\")\n    print(f\"  Ground truth: {ground_truth[0]['target']}\")\n</pre> # Example 3: Use pandas timestamp-like strings task = fev.Task(     dataset_path=dataset_path,     horizon=3,     num_windows=2,     initial_cutoff=\"2025-01-05\",     window_step_size=\"2D\", )  for window_index, window in enumerate(task.iter_windows()):     past, future = window.get_input_data()     ground_truth = window.get_ground_truth()     print(f\"Window {window_index} (cutoff={window.cutoff}):\")     print(f\"  Past data:    {past[0]['target']}\")     print(f\"  Ground truth: {ground_truth[0]['target']}\") <pre>Window 0 (cutoff=2025-01-05T00:00:00):\n  Past data:    [0 1 2 3 4]\n  Ground truth: [5 6 7]\nWindow 1 (cutoff=2025-01-07T00:00:00):\n  Past data:    [0 1 2 3 4 5 6]\n  Ground truth: [7 8 9]\n</pre> In\u00a0[11]: Copied! <pre>task = fev.Task(\n    dataset_path=\"autogluon/chronos_datasets\",\n    dataset_config=\"m4_hourly\",\n    horizon=24,\n    num_windows=2,\n)\n</pre> task = fev.Task(     dataset_path=\"autogluon/chronos_datasets\",     dataset_config=\"m4_hourly\",     horizon=24,     num_windows=2, ) <p>To evaluate a forecasting model on this task we need to make predictions for each <code>EvaluationWindow</code>.</p> In\u00a0[12]: Copied! <pre>task.predictions_schema\n</pre> task.predictions_schema Out[12]: <pre>{'predictions': Sequence(feature=Value(dtype='float64', id=None), length=24, id=None)}</pre> <p>Here is an example of a function that makes predictions for a single <code>EvaluationWindow</code> and formats them as a <code>datasets.Dataset</code>.</p> In\u00a0[13]: Copied! <pre>def naive_forecast(window: fev.EvaluationWindow) -&gt; datasets.Dataset:\n    assert len(window.target_columns) == 1, \"only univariate forecasting supported\"\n    predictions: list[dict[str, np.ndarray]] = []\n    past_data, future_data = window.get_input_data()\n    for ts in past_data:\n        y = ts[window.target_columns[0]]\n        predictions.append(\n            {\"predictions\": np.array([y[-1] for _ in range(window.horizon)])}\n        )\n    return datasets.Dataset.from_list(predictions)\n\nwindow = task.get_window(0)\npredictions = naive_forecast(window)\npredictions\n</pre> def naive_forecast(window: fev.EvaluationWindow) -&gt; datasets.Dataset:     assert len(window.target_columns) == 1, \"only univariate forecasting supported\"     predictions: list[dict[str, np.ndarray]] = []     past_data, future_data = window.get_input_data()     for ts in past_data:         y = ts[window.target_columns[0]]         predictions.append(             {\"predictions\": np.array([y[-1] for _ in range(window.horizon)])}         )     return datasets.Dataset.from_list(predictions)  window = task.get_window(0) predictions = naive_forecast(window) predictions Out[13]: <pre>Dataset({\n    features: ['predictions'],\n    num_rows: 414\n})</pre> <p>Each entry in <code>predictions</code> is a dictionary where the key <code>\"predictions\"</code> corresponds to an array with <code>24</code> values.</p> In\u00a0[14]: Copied! <pre>print(predictions[0])\n</pre> print(predictions[0]) <pre>{'predictions': [684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0, 684.0]}\n</pre> <p>Once we have predictions for each evaluation window, we can compute the metrics and generate an evaluation summary</p> In\u00a0[15]: Copied! <pre>predictions_per_window = [naive_forecast(window) for window in task.iter_windows()]\ntask.evaluation_summary(predictions_per_window, model_name=\"naive\")\n</pre> predictions_per_window = [naive_forecast(window) for window in task.iter_windows()] task.evaluation_summary(predictions_per_window, model_name=\"naive\") Out[15]: <pre>{'model_name': 'naive',\n 'dataset_path': 'autogluon/chronos_datasets',\n 'dataset_config': 'm4_hourly',\n 'horizon': 24,\n 'num_windows': 2,\n 'initial_cutoff': -48,\n 'window_step_size': 24,\n 'min_context_length': 1,\n 'max_context_length': None,\n 'seasonality': 1,\n 'eval_metric': 'MASE',\n 'extra_metrics': [],\n 'quantile_levels': [],\n 'id_column': 'id',\n 'timestamp_column': 'timestamp',\n 'target': 'target',\n 'generate_univariate_targets_from': None,\n 'known_dynamic_columns': [],\n 'past_dynamic_columns': [],\n 'static_columns': [],\n 'task_name': 'm4_hourly',\n 'test_error': 3.8851860313085385,\n 'training_time_s': None,\n 'inference_time_s': None,\n 'dataset_fingerprint': '19e36bb78b718d8d',\n 'trained_on_this_dataset': False,\n 'fev_version': '0.6.0',\n 'MASE': 3.8851860313085385}</pre> <p>For probabilistic forecasting tasks (i.e., if <code>quantile_levels</code> contains at least one value), predictions must additionally contain a prediction for each quantile level.</p> In\u00a0[16]: Copied! <pre>task = fev.Task(\n    dataset_path=\"autogluon/chronos_datasets\",\n    dataset_config=\"m4_hourly\",\n    horizon=24,\n    quantile_levels=[0.1, 0.5, 0.9],\n    eval_metric=\"WQL\",\n)\n</pre> task = fev.Task(     dataset_path=\"autogluon/chronos_datasets\",     dataset_config=\"m4_hourly\",     horizon=24,     quantile_levels=[0.1, 0.5, 0.9],     eval_metric=\"WQL\", ) In\u00a0[17]: Copied! <pre>task.predictions_schema\n</pre> task.predictions_schema Out[17]: <pre>{'predictions': Sequence(feature=Value(dtype='float64', id=None), length=24, id=None),\n '0.1': Sequence(feature=Value(dtype='float64', id=None), length=24, id=None),\n '0.5': Sequence(feature=Value(dtype='float64', id=None), length=24, id=None),\n '0.9': Sequence(feature=Value(dtype='float64', id=None), length=24, id=None)}</pre> In\u00a0[18]: Copied! <pre>task = fev.Task(\n    dataset_path=\"autogluon/fev_datasets\",\n    dataset_config=\"favorita_transactions_1D\",\n    horizon=7,\n    target=\"transactions\",\n)\npast_data, future_data = task.get_window(0).get_input_data()\nprint(past_data)\nprint(future_data)\n</pre> task = fev.Task(     dataset_path=\"autogluon/fev_datasets\",     dataset_config=\"favorita_transactions_1D\",     horizon=7,     target=\"transactions\", ) past_data, future_data = task.get_window(0).get_input_data() print(past_data) print(future_data) <pre>Dataset({\n    features: ['id', 'timestamp', 'transactions'],\n    num_rows: 51\n})\nDataset({\n    features: ['id', 'timestamp'],\n    num_rows: 51\n})\n</pre> <p>We can view all the columns available in the dataset with <code>Task.load_full_dataset()</code></p> In\u00a0[19]: Copied! <pre>full_ds = task.load_full_dataset()\nprint(full_ds)\nfull_ds.features\n</pre> full_ds = task.load_full_dataset() print(full_ds) full_ds.features <pre>Dataset({\n    features: ['id', 'timestamp', 'transactions', 'oil_price', 'holiday', 'store_nbr', 'city', 'state', 'type', 'cluster'],\n    num_rows: 51\n})\n</pre> Out[19]: <pre>{'id': Value(dtype='string', id=None),\n 'timestamp': Sequence(feature=Value(dtype='timestamp[ms]', id=None), length=-1, id=None),\n 'transactions': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None),\n 'oil_price': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None),\n 'holiday': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n 'store_nbr': Value(dtype='float32', id=None),\n 'city': Value(dtype='string', id=None),\n 'state': Value(dtype='string', id=None),\n 'type': Value(dtype='string', id=None),\n 'cluster': Value(dtype='float32', id=None)}</pre> <p>We can configure the task to use the additional columns as covariates. There are 3 types of covariates:</p> <p>Static covariates (<code>static_columns</code>) are the time-independent attributes of the time series, e.g.</p> <ul> <li>Location (country, state, city)</li> <li>Product properties (brand, color, size)</li> <li>IDs and constant metadata</li> </ul> <p>Known dynamic covariates (<code>known_dynamic_columns</code>) are time-varying features available for both past and future periods, e.g.</p> <ul> <li>Holidays, calendar features</li> <li>Planned promotions</li> </ul> <p>Past dynamic covariates (<code>past_dynamic_columns</code>) are time-varying features only available until the forecast start, e.g.</p> <ul> <li>Weather data, economic indicators</li> <li>Related product sales</li> </ul> <p>Dynamic covariates must have feature type <code>Sequence</code> and length must match the target length for each row</p> <p>Static covariates must have feature type <code>Value</code> (not <code>Sequence</code>).</p> In\u00a0[20]: Copied! <pre>task = fev.Task(\n    dataset_path=\"autogluon/fev_datasets\",\n    dataset_config=\"favorita_transactions_1D\",\n    horizon=7,\n    target=\"transactions\",\n    known_dynamic_columns=[\"holiday\"],  # time-dependent, known in the future and in the past\n    past_dynamic_columns=[\"oil_price\"],  # time-dependent, known only in the past\n    static_columns=[\"city\", \"state\"],  # time-independent\n)\npast_data, future_data = task.get_window(0).get_input_data()\nprint(past_data)\nprint(future_data)\n</pre> task = fev.Task(     dataset_path=\"autogluon/fev_datasets\",     dataset_config=\"favorita_transactions_1D\",     horizon=7,     target=\"transactions\",     known_dynamic_columns=[\"holiday\"],  # time-dependent, known in the future and in the past     past_dynamic_columns=[\"oil_price\"],  # time-dependent, known only in the past     static_columns=[\"city\", \"state\"],  # time-independent ) past_data, future_data = task.get_window(0).get_input_data() print(past_data) print(future_data) <pre>Dataset({\n    features: ['id', 'timestamp', 'transactions', 'holiday', 'oil_price', 'city', 'state'],\n    num_rows: 51\n})\nDataset({\n    features: ['id', 'timestamp', 'holiday', 'city', 'state'],\n    num_rows: 51\n})\n</pre> In\u00a0[21]: Copied! <pre>task = fev.Task(\n    dataset_path=\"autogluon/fev_datasets\",\n    dataset_config=\"ETT_1H\",\n    horizon=3,\n    target=[\"OT\", \"LUFL\", \"LULL\"],\n)\n</pre> task = fev.Task(     dataset_path=\"autogluon/fev_datasets\",     dataset_config=\"ETT_1H\",     horizon=3,     target=[\"OT\", \"LUFL\", \"LULL\"], ) <p>The input data created by the task in this case is identical to what would happen if we used <code>[\"OT\", \"LUFL\", \"LULL\"]</code> as <code>past_dynamic_columns</code>. That is, the target columns <code>[\"OT\", \"LUFL\", \"LULL\"]</code> are available in <code>past_data</code> but not in <code>future_data</code>.</p> In\u00a0[22]: Copied! <pre>past_data, future_data = task.get_window(0).get_input_data()\nprint(past_data)\nprint(future_data)\n</pre> past_data, future_data = task.get_window(0).get_input_data() print(past_data) print(future_data) <pre>Dataset({\n    features: ['id', 'timestamp', 'LUFL', 'LULL', 'OT'],\n    num_rows: 2\n})\nDataset({\n    features: ['id', 'timestamp'],\n    num_rows: 2\n})\n</pre> <p>The only difference in a multivariate task is that the predictions must be formatted as a <code>datasets.DatasetDict</code> where</p> <ul> <li>each key corresponds to the name of the target column</li> <li>each value is a <code>datasets.Dataset</code> containing the predictions for this column in a format compatible with <code>task.predictions_schema</code></li> </ul> In\u00a0[23]: Copied! <pre>def naive_forecast_multivariate(window: fev.EvaluationWindow) -&gt; datasets.DatasetDict:\n    \"\"\"Predicts the last observed value in each multivariate column.\"\"\"\n    past_data, future_data = window.get_input_data()\n    predictions = datasets.DatasetDict()\n    for col in window.target_columns:\n        predictions_for_column = []\n        for ts in past_data:\n            predictions_for_column.append({\"predictions\": [ts[col][-1] for _ in range(window.horizon)]})\n        predictions[col] = datasets.Dataset.from_list(predictions_for_column)\n    return predictions\n</pre> def naive_forecast_multivariate(window: fev.EvaluationWindow) -&gt; datasets.DatasetDict:     \"\"\"Predicts the last observed value in each multivariate column.\"\"\"     past_data, future_data = window.get_input_data()     predictions = datasets.DatasetDict()     for col in window.target_columns:         predictions_for_column = []         for ts in past_data:             predictions_for_column.append({\"predictions\": [ts[col][-1] for _ in range(window.horizon)]})         predictions[col] = datasets.Dataset.from_list(predictions_for_column)     return predictions In\u00a0[24]: Copied! <pre>window = task.get_window(0)\npredictions_per_window = naive_forecast_multivariate(window).cast(task.predictions_schema)\npredictions_per_window\n</pre> window = task.get_window(0) predictions_per_window = naive_forecast_multivariate(window).cast(task.predictions_schema) predictions_per_window Out[24]: <pre>DatasetDict({\n    LUFL: Dataset({\n        features: ['predictions'],\n        num_rows: 2\n    })\n    LULL: Dataset({\n        features: ['predictions'],\n        num_rows: 2\n    })\n    OT: Dataset({\n        features: ['predictions'],\n        num_rows: 2\n    })\n})</pre> <p>We can also look at the individual values in the <code>Dataset</code> objects</p> In\u00a0[25]: Copied! <pre>for col in task.target_columns:\n    print(f\"Predictions for column '{col}'\")\n    print(f\"\\t{predictions_per_window[col].to_list()}\")\n</pre> for col in task.target_columns:     print(f\"Predictions for column '{col}'\")     print(f\"\\t{predictions_per_window[col].to_list()}\") <pre>Predictions for column 'LUFL'\n\t[{'predictions': [3.5329999923706055, 3.5329999923706055, 3.5329999923706055]}, {'predictions': [-10.331000328063965, -10.331000328063965, -10.331000328063965]}]\nPredictions for column 'LULL'\n\t[{'predictions': [1.6749999523162842, 1.6749999523162842, 1.6749999523162842]}, {'predictions': [-1.2899999618530273, -1.2899999618530273, -1.2899999618530273]}]\nPredictions for column 'OT'\n\t[{'predictions': [11.043999671936035, 11.043999671936035, 11.043999671936035]}, {'predictions': [48.18349838256836, 48.18349838256836, 48.18349838256836]}]\n</pre> <p>The rest of the code can stay the same.</p> In\u00a0[26]: Copied! <pre>task.evaluation_summary([predictions_per_window], model_name=\"naive\")\n</pre> task.evaluation_summary([predictions_per_window], model_name=\"naive\") Out[26]: <pre>{'model_name': 'naive',\n 'dataset_path': 'autogluon/fev_datasets',\n 'dataset_config': 'ETT_1H',\n 'horizon': 3,\n 'num_windows': 1,\n 'initial_cutoff': -3,\n 'window_step_size': 3,\n 'min_context_length': 1,\n 'max_context_length': None,\n 'seasonality': 1,\n 'eval_metric': 'MASE',\n 'extra_metrics': [],\n 'quantile_levels': [],\n 'id_column': 'id',\n 'timestamp_column': 'timestamp',\n 'target': ['LUFL', 'LULL', 'OT'],\n 'generate_univariate_targets_from': None,\n 'known_dynamic_columns': [],\n 'past_dynamic_columns': [],\n 'static_columns': [],\n 'task_name': 'ETT_1H',\n 'test_error': 1.1921320279836811,\n 'training_time_s': None,\n 'inference_time_s': None,\n 'dataset_fingerprint': '305bfc1cf6779b47',\n 'trained_on_this_dataset': False,\n 'fev_version': '0.6.0',\n 'MASE': 1.1921320279836811}</pre> In\u00a0[27]: Copied! <pre>past_data[\"id\"]\n</pre> past_data[\"id\"] Out[27]: <pre>array(['ETTh1', 'ETTh2'], dtype='&lt;U5')</pre> <p>If we set <code>generate_univariate_targets_from=[\"OT\", \"LUFL\", \"LULL\"]</code>, <code>fev</code> will create 3 univariate time series from each time series in the original dataset.</p> In\u00a0[28]: Copied! <pre>task = fev.Task(\n    dataset_path=\"autogluon/fev_datasets\",\n    dataset_config=\"ETT_1H\",\n    horizon=3,\n    generate_univariate_targets_from=[\"OT\", \"LUFL\", \"LULL\"],\n    target=\"target\",  # new name for the target columns ['OT', 'LUFL', 'LULL'] after splitting\n)\n</pre> task = fev.Task(     dataset_path=\"autogluon/fev_datasets\",     dataset_config=\"ETT_1H\",     horizon=3,     generate_univariate_targets_from=[\"OT\", \"LUFL\", \"LULL\"],     target=\"target\",  # new name for the target columns ['OT', 'LUFL', 'LULL'] after splitting ) In\u00a0[29]: Copied! <pre>past_data, future_data = task.get_window(0).get_input_data()\nprint(past_data)\nprint(future_data)\n</pre> past_data, future_data = task.get_window(0).get_input_data() print(past_data) print(future_data) <pre>Dataset({\n    features: ['id', 'timestamp', 'target'],\n    num_rows: 6\n})\nDataset({\n    features: ['id', 'timestamp'],\n    num_rows: 6\n})\n</pre> <p>The new dataset contains 6 items (2 original ids $\\times$ 3 target columns).</p> In\u00a0[30]: Copied! <pre>past_data[\"id\"]\n</pre> past_data[\"id\"] Out[30]: <pre>array(['ETTh1_LUFL', 'ETTh1_LULL', 'ETTh1_OT', 'ETTh2_LUFL', 'ETTh2_LULL',\n       'ETTh2_OT'], dtype='&lt;U10')</pre> <p>We can confirm that the naive forecast achieves the same MASE score on this equivalent representation of the multivariate task.</p> In\u00a0[31]: Copied! <pre>def naive_forecast_univariate(window: fev.EvaluationWindow) -&gt; datasets.Dataset:\n    \"\"\"Predicts the last observed value.\"\"\"\n    past_data, future_data = window.get_input_data()\n    predictions = []\n    for ts in past_data:\n        predictions.append({\"predictions\": [ts[window.target_columns[0]][-1] for _ in range(window.horizon)]})\n    return datasets.Dataset.from_list(predictions)\n</pre> def naive_forecast_univariate(window: fev.EvaluationWindow) -&gt; datasets.Dataset:     \"\"\"Predicts the last observed value.\"\"\"     past_data, future_data = window.get_input_data()     predictions = []     for ts in past_data:         predictions.append({\"predictions\": [ts[window.target_columns[0]][-1] for _ in range(window.horizon)]})     return datasets.Dataset.from_list(predictions) In\u00a0[32]: Copied! <pre>predictions_per_window = []\nfor window in task.iter_windows():\n    predictions_per_window.append(naive_forecast_univariate(window))\ntask.evaluation_summary(predictions_per_window, model_name=\"naive\")\n</pre> predictions_per_window = [] for window in task.iter_windows():     predictions_per_window.append(naive_forecast_univariate(window)) task.evaluation_summary(predictions_per_window, model_name=\"naive\") Out[32]: <pre>{'model_name': 'naive',\n 'dataset_path': 'autogluon/fev_datasets',\n 'dataset_config': 'ETT_1H',\n 'horizon': 3,\n 'num_windows': 1,\n 'initial_cutoff': -3,\n 'window_step_size': 3,\n 'min_context_length': 1,\n 'max_context_length': None,\n 'seasonality': 1,\n 'eval_metric': 'MASE',\n 'extra_metrics': [],\n 'quantile_levels': [],\n 'id_column': 'id',\n 'timestamp_column': 'timestamp',\n 'target': 'target',\n 'generate_univariate_targets_from': ['OT', 'LUFL', 'LULL'],\n 'known_dynamic_columns': [],\n 'past_dynamic_columns': [],\n 'static_columns': [],\n 'task_name': 'ETT_1H',\n 'test_error': 1.1921320279836811,\n 'training_time_s': None,\n 'inference_time_s': None,\n 'dataset_fingerprint': '81591e84125d0e33',\n 'trained_on_this_dataset': False,\n 'fev_version': '0.6.0',\n 'MASE': 1.1921320279836811}</pre> In\u00a0[33]: Copied! <pre>tasks_configs = [\n    {\n        \"dataset_path\": \"autogluon/chronos_datasets\",\n        \"dataset_config\": \"monash_m1_quarterly\",\n        \"horizon\": 8,\n        \"seasonality\": 4,\n        \"eval_metric\": \"MASE\",\n    },\n    {\n        \"dataset_path\": \"autogluon/chronos_datasets\",\n        \"dataset_config\": \"monash_electricity_weekly\",\n        \"horizon\": 8,\n        \"num_windows\": 2,\n    },\n    {\n        \"dataset_path\": \"autogluon/chronos_datasets\",\n        \"dataset_config\": \"monash_m1_yearly\",\n        \"horizon\": 6,\n    },\n]\nbenchmark = fev.Benchmark.from_list(tasks_configs)\n</pre> tasks_configs = [     {         \"dataset_path\": \"autogluon/chronos_datasets\",         \"dataset_config\": \"monash_m1_quarterly\",         \"horizon\": 8,         \"seasonality\": 4,         \"eval_metric\": \"MASE\",     },     {         \"dataset_path\": \"autogluon/chronos_datasets\",         \"dataset_config\": \"monash_electricity_weekly\",         \"horizon\": 8,         \"num_windows\": 2,     },     {         \"dataset_path\": \"autogluon/chronos_datasets\",         \"dataset_config\": \"monash_m1_yearly\",         \"horizon\": 6,     }, ] benchmark = fev.Benchmark.from_list(tasks_configs) <p>Or from a YAML file</p> In\u00a0[\u00a0]: Copied! <pre>benchmark_path = \"https://raw.githubusercontent.com/autogluon/fev/refs/tags/v0.6.1/benchmarks/example/tasks.yaml\"\n# Show contents of the benchmark YAML file\n!curl -s {benchmark_path} | cat\n</pre> benchmark_path = \"https://raw.githubusercontent.com/autogluon/fev/refs/tags/v0.6.1/benchmarks/example/tasks.yaml\" # Show contents of the benchmark YAML file !curl -s {benchmark_path} | cat <pre>tasks:\n- dataset_path: autogluon/chronos_datasets\n  dataset_config: monash_m1_quarterly\n  horizon: 8\n  seasonality: 4\n- dataset_path: autogluon/chronos_datasets\n  dataset_config: monash_electricity_weekly\n  horizon: 8\n  num_windows: 2\n- dataset_path: autogluon/chronos_datasets\n  dataset_config: monash_m1_yearly\n  horizon: 6\n  seasonality: 1\n</pre> In\u00a0[35]: Copied! <pre>benchmark = fev.Benchmark.from_yaml(benchmark_path)\n</pre> benchmark = fev.Benchmark.from_yaml(benchmark_path) In\u00a0[36]: Copied! <pre>benchmark.tasks\n</pre> benchmark.tasks Out[36]: <pre>[Task(dataset_path='autogluon/chronos_datasets', dataset_config='monash_m1_quarterly', horizon=8, num_windows=1, initial_cutoff=-8, window_step_size=8, min_context_length=1, max_context_length=None, seasonality=4, eval_metric='MASE', extra_metrics=[], quantile_levels=[], id_column='id', timestamp_column='timestamp', target='target', generate_univariate_targets_from=None, known_dynamic_columns=[], past_dynamic_columns=[], static_columns=[], task_name='monash_m1_quarterly'),\n Task(dataset_path='autogluon/chronos_datasets', dataset_config='monash_electricity_weekly', horizon=8, num_windows=2, initial_cutoff=-16, window_step_size=8, min_context_length=1, max_context_length=None, seasonality=1, eval_metric='MASE', extra_metrics=[], quantile_levels=[], id_column='id', timestamp_column='timestamp', target='target', generate_univariate_targets_from=None, known_dynamic_columns=[], past_dynamic_columns=[], static_columns=[], task_name='monash_electricity_weekly'),\n Task(dataset_path='autogluon/chronos_datasets', dataset_config='monash_m1_yearly', horizon=6, num_windows=1, initial_cutoff=-6, window_step_size=6, min_context_length=1, max_context_length=None, seasonality=1, eval_metric='MASE', extra_metrics=[], quantile_levels=[], id_column='id', timestamp_column='timestamp', target='target', generate_univariate_targets_from=None, known_dynamic_columns=[], past_dynamic_columns=[], static_columns=[], task_name='monash_m1_yearly')]</pre> <p>Now let's evaluate some simple forecasting models on this toy benchmark.</p> In\u00a0[\u00a0]: Copied! <pre># You might need to restart the notebook after installing the dependencies\n!pip install -q statsforecast \"numpy&lt;=2.2\" \"scipy&lt;1.16\"\n</pre> # You might need to restart the notebook after installing the dependencies !pip install -q statsforecast \"numpy&lt;=2.2\" \"scipy&lt;1.16\" In\u00a0[38]: Copied! <pre>from statsforecast.models import AutoETS, SeasonalNaive, Theta\n\n\ndef predict_with_model(task: fev.Task, model_name: str = \"seasonal_naive\") -&gt; list[datasets.Dataset]:\n    assert len(task.target_columns) == 1, \"only univariate forecasting supported\"\n    if model_name == \"seasonal_naive\":\n        model = SeasonalNaive(season_length=task.seasonality)\n    elif model_name == \"theta\":\n        model = Theta(season_length=task.seasonality)\n    elif model_name == \"ets\":\n        model = AutoETS(season_length=task.seasonality)\n    else:\n        raise ValueError(f\"Unknown model_name: {model_name}\")\n\n    predictions_per_window = []\n    for window in task.iter_windows():\n        past_data, future_data = window.get_input_data()\n        predictions = [\n            {\"predictions\": model.forecast(y=ts[task.target], h=task.horizon)[\"mean\"]}\n            for ts in past_data\n        ]\n        predictions_per_window.append(datasets.Dataset.from_list(predictions))\n    return predictions_per_window\n</pre> from statsforecast.models import AutoETS, SeasonalNaive, Theta   def predict_with_model(task: fev.Task, model_name: str = \"seasonal_naive\") -&gt; list[datasets.Dataset]:     assert len(task.target_columns) == 1, \"only univariate forecasting supported\"     if model_name == \"seasonal_naive\":         model = SeasonalNaive(season_length=task.seasonality)     elif model_name == \"theta\":         model = Theta(season_length=task.seasonality)     elif model_name == \"ets\":         model = AutoETS(season_length=task.seasonality)     else:         raise ValueError(f\"Unknown model_name: {model_name}\")      predictions_per_window = []     for window in task.iter_windows():         past_data, future_data = window.get_input_data()         predictions = [             {\"predictions\": model.forecast(y=ts[task.target], h=task.horizon)[\"mean\"]}             for ts in past_data         ]         predictions_per_window.append(datasets.Dataset.from_list(predictions))     return predictions_per_window In\u00a0[39]: Copied! <pre>import time\n\nsummaries = []\nfor task in tqdm(benchmark.tasks, desc=\"Tasks completed\"):\n    for model_name in [\"seasonal_naive\", \"ets\", \"theta\"]:\n        start_time = time.time()\n        predictions_per_window = predict_with_model(task, model_name=model_name)\n        infer_time_s = time.time() - start_time\n        eval_summary = task.evaluation_summary(\n            predictions_per_window,\n            model_name=model_name,\n            inference_time_s=infer_time_s,\n            training_time_s=0.0,\n        )\n\n        summaries.append(eval_summary)\n</pre> import time  summaries = [] for task in tqdm(benchmark.tasks, desc=\"Tasks completed\"):     for model_name in [\"seasonal_naive\", \"ets\", \"theta\"]:         start_time = time.time()         predictions_per_window = predict_with_model(task, model_name=model_name)         infer_time_s = time.time() - start_time         eval_summary = task.evaluation_summary(             predictions_per_window,             model_name=model_name,             inference_time_s=infer_time_s,             training_time_s=0.0,         )          summaries.append(eval_summary) <pre>Tasks completed:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> In\u00a0[40]: Copied! <pre>fev.leaderboard(summaries, baseline_model=\"seasonal_naive\")\n</pre> fev.leaderboard(summaries, baseline_model=\"seasonal_naive\") Out[40]: skill_score win_rate median_training_time_s median_inference_time_s training_corpus_overlap num_failures model_name ets 0.133483 0.833333 0.0 3.637132 0.0 0 theta 0.105932 0.333333 0.0 0.131447 0.0 0 seasonal_naive 0.000000 0.333333 0.0 1.638496 0.0 0 <p>The <code>leaderboard</code> method aggregates the performance into a single number.</p> <p>We can investigate the performance for individual tasks using the <code>pivot_table</code> method</p> In\u00a0[41]: Copied! <pre>fev.pivot_table(summaries, task_columns=[\"dataset_config\"])\n</pre> fev.pivot_table(summaries, task_columns=[\"dataset_config\"]) Out[41]: model_name ets seasonal_naive theta dataset_config monash_electricity_weekly 2.552429 2.535526 2.557008 monash_m1_quarterly 1.660810 2.077537 1.705247 monash_m1_yearly 3.957011 4.894322 4.225722 In\u00a0[42]: Copied! <pre>fev.pairwise_comparison(summaries)\n</pre> fev.pairwise_comparison(summaries) Out[42]: skill_score win_rate model_1 model_2 ets ets 0.000000 0.500000 seasonal_naive 0.133483 0.666667 theta 0.030815 1.000000 seasonal_naive ets -0.154045 0.333333 seasonal_naive 0.000000 0.500000 theta -0.118484 0.333333 theta ets -0.031794 0.000000 seasonal_naive 0.105932 0.666667 theta 0.000000 0.500000 <p>Both <code>leaderboard()</code> and <code>pivot_table()</code> methods can handle single or multiple evaluation summaries in different formats:</p> <ul> <li><code>pandas.DataFrame</code></li> <li>list of dictionaries</li> <li>paths to JSONL (orient=\"record\") or CSV files</li> </ul> <p>Here is an example of how we can work with URLs of CSV files:</p> In\u00a0[43]: Copied! <pre>summaries = [\n    \"https://raw.githubusercontent.com/autogluon/fev/refs/heads/main/benchmarks/chronos_zeroshot/results/auto_arima.csv\",\n    \"https://raw.githubusercontent.com/autogluon/fev/refs/heads/main/benchmarks/chronos_zeroshot/results/chronos_bolt_base.csv\",\n    \"https://raw.githubusercontent.com/autogluon/fev/refs/heads/main/benchmarks/chronos_zeroshot/results/seasonal_naive.csv\",\n]\nfev.leaderboard(summaries, metric_column=\"MASE\")\n</pre> summaries = [     \"https://raw.githubusercontent.com/autogluon/fev/refs/heads/main/benchmarks/chronos_zeroshot/results/auto_arima.csv\",     \"https://raw.githubusercontent.com/autogluon/fev/refs/heads/main/benchmarks/chronos_zeroshot/results/chronos_bolt_base.csv\",     \"https://raw.githubusercontent.com/autogluon/fev/refs/heads/main/benchmarks/chronos_zeroshot/results/seasonal_naive.csv\", ] fev.leaderboard(summaries, metric_column=\"MASE\") Out[43]: skill_score win_rate median_training_time_s median_inference_time_s training_corpus_overlap num_failures model_name chronos_bolt_base 0.204508 0.722222 NaN 0.406413 0.0 0 auto_arima 0.130551 0.648148 NaN 75.883700 0.0 0 seasonal_naive 0.000000 0.129630 NaN 0.096449 0.0 0 <p>We can also compute the 95% confidence intervals for <code>skill_score</code> and <code>win_rate</code> columns via bootstrap by setting the <code>n_resamples</code> parameter.</p> In\u00a0[44]: Copied! <pre>fev.leaderboard(summaries, metric_column=\"MASE\", n_resamples=1000).round(3)\n</pre> fev.leaderboard(summaries, metric_column=\"MASE\", n_resamples=1000).round(3) Out[44]: skill_score skill_score_lower skill_score_upper win_rate win_rate_lower win_rate_upper median_training_time_s median_inference_time_s training_corpus_overlap num_failures model_name chronos_bolt_base 0.205 0.147 0.265 0.722 0.611 0.833 NaN 0.406 0.0 0 auto_arima 0.131 0.053 0.204 0.648 0.500 0.778 NaN 75.884 0.0 0 seasonal_naive 0.000 0.000 0.000 0.130 0.056 0.204 NaN 0.096 0.0 0 In\u00a0[45]: Copied! <pre>fev.pairwise_comparison(summaries, metric_column=\"MASE\", n_resamples=1000).round(3)\n</pre> fev.pairwise_comparison(summaries, metric_column=\"MASE\", n_resamples=1000).round(3) Out[45]: skill_score skill_score_lower skill_score_upper win_rate win_rate_lower win_rate_upper model_1 model_2 chronos_bolt_base chronos_bolt_base 0.000 0.000 0.000 0.500 0.500 0.500 auto_arima 0.085 0.000 0.162 0.519 0.333 0.704 seasonal_naive 0.205 0.147 0.265 0.926 0.815 1.000 auto_arima chronos_bolt_base -0.093 -0.194 -0.000 0.481 0.296 0.667 auto_arima 0.000 0.000 0.000 0.500 0.500 0.500 seasonal_naive 0.131 0.053 0.204 0.815 0.667 0.963 seasonal_naive chronos_bolt_base -0.257 -0.360 -0.173 0.074 0.000 0.185 auto_arima -0.150 -0.256 -0.056 0.185 0.037 0.333 seasonal_naive 0.000 0.000 0.000 0.500 0.500 0.500 <p>Like before, we can view the scores for individual tasks with the <code>pivot_table</code> method.</p> In\u00a0[46]: Copied! <pre>fev.pivot_table(summaries, task_columns=[\"dataset_config\"], metric_column=\"WQL\").round(3)\n</pre> fev.pivot_table(summaries, task_columns=[\"dataset_config\"], metric_column=\"WQL\").round(3) Out[46]: model_name auto_arima chronos_bolt_base seasonal_naive dataset_config ETTh 0.089 0.071 0.122 ETTm 0.105 0.052 0.141 dominick 0.485 0.345 0.453 ercot 0.041 0.021 0.037 exchange_rate 0.011 0.012 0.013 m4_quarterly 0.079 0.077 0.119 m4_yearly 0.125 0.121 0.161 m5 0.617 0.562 1.024 monash_australian_electricity 0.067 0.036 0.084 monash_car_parts 1.333 0.995 1.600 monash_cif_2016 0.033 0.016 0.015 monash_covid_deaths 0.029 0.047 0.133 monash_fred_md 0.035 0.042 0.122 monash_hospital 0.059 0.057 0.073 monash_m1_monthly 0.154 0.139 0.191 monash_m1_quarterly 0.088 0.101 0.150 monash_m1_yearly 0.133 0.151 0.209 monash_m3_monthly 0.098 0.093 0.149 monash_m3_quarterly 0.077 0.076 0.101 monash_m3_yearly 0.156 0.129 0.167 monash_nn5_weekly 0.084 0.084 0.123 monash_tourism_monthly 0.091 0.090 0.104 monash_tourism_quarterly 0.100 0.065 0.119 monash_tourism_yearly 0.129 0.166 0.209 monash_traffic 0.354 0.231 0.362 monash_weather 0.215 0.134 0.217 nn5 0.248 0.150 0.425"},{"location":"tutorials/03-tasks-and-benchmarks/#main-classes","title":"Main classes\u00b6","text":"<p>The <code>fev</code> package provides 3 core classes for evaluating time series forecasting models:</p> <ol> <li><p><code>Task</code> - Defines a single forecasting task with dataset path, forecast horizon, and evaluation settings. Each <code>Task</code> contains one or more evaluation windows.</p> </li> <li><p><code>EvaluationWindow</code> - Represents a single train/test split of the data at a specific cutoff point. Model performance is averaged across all windows within a <code>Task</code>.</p> </li> <li><p><code>Benchmark</code> - A collection of multiple tasks (e.g., different datasets). Individual task results are aggregated to compute overall benchmark scores.</p> </li> </ol> <p>In short, the hierarchy is <code>Benchmark</code> -&gt; <code>Task</code> -&gt; <code>EvaluationWindow</code>.</p> <p>This tutorial demonstrates the functionality of these classes.</p>"},{"location":"tutorials/03-tasks-and-benchmarks/#data-sources","title":"Data sources\u00b6","text":"<p>Dataset stored on Hugging Face Hub: https://huggingface.co/datasets/autogluon/chronos_datasets</p>"},{"location":"tutorials/03-tasks-and-benchmarks/#evaluation-windows","title":"Evaluation windows\u00b6","text":"<p>A single <code>Task</code> consists of one or more <code>EvaluationWindow</code>s.</p> <p>Each <code>EvaluationWindow</code> represents a single train/test split of the time series data at a specific cutoff point.</p> <p>We'll create a task with a toy dataset to demonstrate how evaluation windows work.</p>"},{"location":"tutorials/03-tasks-and-benchmarks/#customizing-evaluation-window-parameters","title":"Customizing evaluation window parameters\u00b6","text":"<p>You can control how evaluation windows are positioned using <code>initial_cutoff</code> and <code>window_step_size</code> parameters.</p>"},{"location":"tutorials/03-tasks-and-benchmarks/#univariate-forecasting","title":"Univariate forecasting\u00b6","text":"<p>The simplest kind of forecasting task is univariate forecasting where the goal is to predict a single <code>target</code> for each time series in the dataset.</p>"},{"location":"tutorials/03-tasks-and-benchmarks/#predictions-format","title":"Predictions format\u00b6","text":"<p>Predictions must follow a certain format that is specified by <code>task.predictions_schema</code>.</p> <p>For point forecasting tasks (i.e., if <code>quantile_levels=None</code>), predictions must contain a single array of length <code>horizon</code> for each time series.</p>"},{"location":"tutorials/03-tasks-and-benchmarks/#probabilistic-forecasting","title":"Probabilistic forecasting\u00b6","text":""},{"location":"tutorials/03-tasks-and-benchmarks/#covariates","title":"Covariates\u00b6","text":"<p>By default, only the <code>id_column</code>, <code>timestamp_column</code> and <code>target</code> columns are loaded from the dataset.</p>"},{"location":"tutorials/03-tasks-and-benchmarks/#multivariate-forecasting","title":"Multivariate forecasting\u00b6","text":"<p>In all previous examples we considered univariate forecasting tasks, where the goal was to predict a single <code>target</code> into the future.</p> <p><code>fev</code> also supports multivariate tasks, where the goal is to simultaneously predict multiple target columns.</p>"},{"location":"tutorials/03-tasks-and-benchmarks/#real-multivariate-tasks","title":"\"Real\" multivariate tasks\u00b6","text":"<p>We can define multivariate forecasting tasks by setting the <code>target</code> attribute to a <code>list</code> of column names.</p>"},{"location":"tutorials/03-tasks-and-benchmarks/#converting-multivariate-tasks-into-univariate-tasks","title":"Converting multivariate tasks into univariate tasks\u00b6","text":"<p>Alternatively, we can convert a multivariate task into a univariate one by creating multiple univariate time series from each multivariate time series.</p> <p>The original <code>ETTh</code> dataset contains two multivariate time series with the following ids:</p>"},{"location":"tutorials/03-tasks-and-benchmarks/#evaluation-on-a-benchmark-consisting-of-multiple-tasks","title":"Evaluation on a Benchmark consisting of multiple tasks\u00b6","text":"<p>A <code>fev.Benchmark</code> object is essentially a collection of <code>Task</code>s.</p> <p>We can create a benchmark from a list of dictionaries. Each dictionary is interpreted as a <code>fev.TaskGenerator</code>.</p>"},{"location":"tutorials/04-adapters/","title":"Adapters","text":"<p>This notebook covers the following topics:</p> <ol> <li>Converting <code>datasets.Dataset</code> into other popular time series data formats.</li> </ol> In\u00a0[1]: Copied! <pre>import fev\n\n# Define a task with a mix of static &amp; dynamic features\ntask = fev.Task(\n    dataset_path=\"autogluon/chronos_datasets\",\n    dataset_config=\"monash_rideshare\",\n    horizon=30,\n    target=\"price_mean\",\n    past_dynamic_columns=[\"distance_mean\", \"surge_mean\"],\n    known_dynamic_columns=[\"api_calls\", \"temp\", \"rain\", \"humidity\", \"clouds\", \"wind\"],\n    static_columns=[\"source_location\", \"provider_name\", \"provider_service\"],\n)\n</pre> import fev  # Define a task with a mix of static &amp; dynamic features task = fev.Task(     dataset_path=\"autogluon/chronos_datasets\",     dataset_config=\"monash_rideshare\",     horizon=30,     target=\"price_mean\",     past_dynamic_columns=[\"distance_mean\", \"surge_mean\"],     known_dynamic_columns=[\"api_calls\", \"temp\", \"rain\", \"humidity\", \"clouds\", \"wind\"],     static_columns=[\"source_location\", \"provider_name\", \"provider_service\"], ) <p>By default, <code>window.get_input_data()</code> returns two <code>datasets.Dataset</code> objects:</p> <ul> <li><code>past_data</code> contains all past data including target, timestamps, and covariates</li> <li><code>future_data</code> contains future values of timestamps and known covariates</li> </ul> In\u00a0[2]: Copied! <pre>window = task.get_window(0)\npast_data, future_data = window.get_input_data()\nprint(past_data)\nprint(future_data)\n</pre> window = task.get_window(0) past_data, future_data = window.get_input_data() print(past_data) print(future_data) <pre>Dataset({\n    features: ['id', 'timestamp', 'price_mean', 'api_calls', 'clouds', 'humidity', 'rain', 'temp', 'wind', 'distance_mean', 'surge_mean', 'provider_name', 'provider_service', 'source_location'],\n    num_rows: 156\n})\nDataset({\n    features: ['id', 'timestamp', 'api_calls', 'clouds', 'humidity', 'rain', 'temp', 'wind', 'provider_name', 'provider_service', 'source_location'],\n    num_rows: 156\n})\n</pre> <p>You can use the <code>fev.convert_input_data()</code> method to convert the past &amp; future data into formats expected by other frameworks.</p> In\u00a0[3]: Copied! <pre>from IPython.display import display\n\ntrain_df, future_df, static_df = fev.convert_input_data(window, adapter=\"pandas\")\nprint(\"train_df\")\ndisplay(train_df.head())\nprint(\"future_df\")\ndisplay(future_df.head())\nprint(\"static_df\")\ndisplay(static_df.head())\n</pre> from IPython.display import display  train_df, future_df, static_df = fev.convert_input_data(window, adapter=\"pandas\") print(\"train_df\") display(train_df.head()) print(\"future_df\") display(future_df.head()) print(\"static_df\") display(static_df.head()) <pre>train_df\n</pre> id timestamp price_mean api_calls clouds humidity rain temp wind distance_mean surge_mean 0 T000000 2018-11-26 06:00:00 16.555555 9.0 0.990667 0.913333 0.0 40.627335 1.350667 1.726667 1.055556 1 T000000 2018-11-26 07:00:00 17.299999 10.0 0.970000 0.920000 0.0 41.137501 1.735000 1.690000 1.100000 2 T000000 2018-11-26 08:00:00 13.500000 1.0 0.980000 0.923333 0.0 40.919998 1.330000 1.380000 1.000000 3 T000000 2018-11-26 09:00:00 17.954546 11.0 1.000000 0.927500 0.0 40.937500 1.365000 1.920909 1.113636 4 T000000 2018-11-26 10:00:00 18.625000 12.0 0.995000 0.940000 0.0 40.695000 1.895000 2.122500 1.083333 <pre>future_df\n</pre> id timestamp api_calls clouds humidity rain temp wind 0 T000000 2018-12-17 13:00:00 10.0 0.97 0.90 0.0 35.169998 7.22 1 T000000 2018-12-17 14:00:00 7.0 0.92 0.90 0.0 36.299999 6.87 2 T000000 2018-12-17 15:00:00 13.0 0.88 0.87 0.0 37.250000 7.58 3 T000000 2018-12-17 16:00:00 12.0 1.00 0.84 0.0 39.000000 6.28 4 T000000 2018-12-17 17:00:00 9.0 0.95 0.81 0.0 40.009998 6.46 <pre>static_df\n</pre> id provider_name provider_service source_location 0 T000000 Lyft Lux Back Bay 1 T000001 Lyft Lux Black Back Bay 2 T000002 Lyft Lux Black XL Back Bay 3 T000003 Lyft Lyft Back Bay 4 T000004 Lyft Lyft XL Back Bay In\u00a0[4]: Copied! <pre>train_dataset, prediction_dataset = fev.convert_input_data(window, adapter=\"gluonts\")\nprint(\"train_dataset\")\nprint(train_dataset)\nprint(\"prediction_dataset\")\nprint(prediction_dataset)\n</pre> train_dataset, prediction_dataset = fev.convert_input_data(window, adapter=\"gluonts\") print(\"train_dataset\") print(train_dataset) print(\"prediction_dataset\") print(prediction_dataset) <pre>train_dataset\nPandasDataset&lt;size=156, freq=h, num_feat_dynamic_real=6, num_past_feat_dynamic_real=2, num_feat_static_real=0, num_feat_static_cat=3, static_cardinalities=[ 2. 13. 12.]&gt;\nprediction_dataset\nPandasDataset&lt;size=156, freq=h, num_feat_dynamic_real=6, num_past_feat_dynamic_real=2, num_feat_static_real=0, num_feat_static_cat=3, static_cardinalities=[ 2. 13. 12.]&gt;\n</pre> In\u00a0[5]: Copied! <pre>train_df, known_covariates = fev.convert_input_data(window, adapter=\"autogluon\")\nprint(\"train_df\")\ndisplay(train_df)\nprint(\"train_df.static_features\")\ndisplay(train_df.static_features)\nprint(\"known_covariates\")\ndisplay(known_covariates)\n</pre> train_df, known_covariates = fev.convert_input_data(window, adapter=\"autogluon\") print(\"train_df\") display(train_df) print(\"train_df.static_features\") display(train_df.static_features) print(\"known_covariates\") display(known_covariates) <pre>train_df\n</pre> target api_calls clouds humidity rain temp wind distance_mean surge_mean item_id timestamp T000000 2018-11-26 06:00:00 16.555555 9.0 0.990667 0.913333 0.000 40.627335 1.350667 1.726667 1.055556 2018-11-26 07:00:00 17.299999 10.0 0.970000 0.920000 0.000 41.137501 1.735000 1.690000 1.100000 2018-11-26 08:00:00 13.500000 1.0 0.980000 0.923333 0.000 40.919998 1.330000 1.380000 1.000000 2018-11-26 09:00:00 17.954546 11.0 1.000000 0.927500 0.000 40.937500 1.365000 1.920909 1.113636 2018-11-26 10:00:00 18.625000 12.0 0.995000 0.940000 0.000 40.695000 1.895000 2.122500 1.083333 ... ... ... ... ... ... ... ... ... ... ... T000155 2018-12-17 08:00:00 9.454545 11.0 1.000000 0.920000 0.000 37.279999 10.670000 2.230909 1.000000 2018-12-17 09:00:00 9.700000 15.0 1.000000 0.930000 0.000 36.189999 9.760000 2.447333 1.000000 2018-12-17 10:00:00 9.300000 10.0 1.000000 0.930000 0.003 34.750000 9.950000 2.203000 1.000000 2018-12-17 11:00:00 9.400000 15.0 1.000000 0.930000 0.009 34.180000 9.240000 2.139333 1.000000 2018-12-17 12:00:00 9.593750 16.0 0.990000 0.930000 0.000 34.209999 8.380000 1.958750 1.000000 <p>79716 rows \u00d7 9 columns</p> <pre>train_df.static_features\n</pre> provider_name provider_service source_location item_id T000000 Lyft Lux Back Bay T000001 Lyft Lux Black Back Bay T000002 Lyft Lux Black XL Back Bay T000003 Lyft Lyft Back Bay T000004 Lyft Lyft XL Back Bay ... ... ... ... T000151 Uber Taxi West End T000152 Uber UberPool West End T000153 Uber UberX West End T000154 Uber UberXL West End T000155 Uber WAV West End <p>156 rows \u00d7 3 columns</p> <pre>known_covariates\n</pre> api_calls clouds humidity rain temp wind item_id timestamp T000000 2018-12-17 13:00:00 10.0 0.97 0.90 0.0 35.169998 7.22 2018-12-17 14:00:00 7.0 0.92 0.90 0.0 36.299999 6.87 2018-12-17 15:00:00 13.0 0.88 0.87 0.0 37.250000 7.58 2018-12-17 16:00:00 12.0 1.00 0.84 0.0 39.000000 6.28 2018-12-17 17:00:00 9.0 0.95 0.81 0.0 40.009998 6.46 ... ... ... ... ... ... ... ... T000155 2018-12-18 14:00:00 17.0 0.48 0.47 0.0 26.190001 13.89 2018-12-18 15:00:00 15.0 0.34 0.46 0.0 27.219999 15.03 2018-12-18 16:00:00 15.0 0.31 0.47 0.0 28.700001 14.60 2018-12-18 17:00:00 9.0 0.15 0.46 0.0 30.049999 13.55 2018-12-18 18:00:00 12.0 0.00 0.46 0.0 30.790001 13.09 <p>4680 rows \u00d7 6 columns</p> In\u00a0[6]: Copied! <pre>train_df, future_df, static_df = fev.convert_input_data(window, adapter=\"nixtla\")\nprint(\"train_df\")\ndisplay(train_df.head())\nprint(\"future_df\")\ndisplay(future_df.head())\nprint(\"static_df\")\ndisplay(static_df.head())\n</pre> train_df, future_df, static_df = fev.convert_input_data(window, adapter=\"nixtla\") print(\"train_df\") display(train_df.head()) print(\"future_df\") display(future_df.head()) print(\"static_df\") display(static_df.head()) <pre>train_df\n</pre> unique_id ds y api_calls clouds humidity rain temp wind distance_mean surge_mean 0 T000000 2018-11-26 06:00:00 16.555555 9.0 0.990667 0.913333 0.0 40.627335 1.350667 1.726667 1.055556 1 T000000 2018-11-26 07:00:00 17.299999 10.0 0.970000 0.920000 0.0 41.137501 1.735000 1.690000 1.100000 2 T000000 2018-11-26 08:00:00 13.500000 1.0 0.980000 0.923333 0.0 40.919998 1.330000 1.380000 1.000000 3 T000000 2018-11-26 09:00:00 17.954546 11.0 1.000000 0.927500 0.0 40.937500 1.365000 1.920909 1.113636 4 T000000 2018-11-26 10:00:00 18.625000 12.0 0.995000 0.940000 0.0 40.695000 1.895000 2.122500 1.083333 <pre>future_df\n</pre> unique_id ds api_calls clouds humidity rain temp wind 0 T000000 2018-12-17 13:00:00 10.0 0.97 0.90 0.0 35.169998 7.22 1 T000000 2018-12-17 14:00:00 7.0 0.92 0.90 0.0 36.299999 6.87 2 T000000 2018-12-17 15:00:00 13.0 0.88 0.87 0.0 37.250000 7.58 3 T000000 2018-12-17 16:00:00 12.0 1.00 0.84 0.0 39.000000 6.28 4 T000000 2018-12-17 17:00:00 9.0 0.95 0.81 0.0 40.009998 6.46 <pre>static_df\n</pre> unique_id provider_name provider_service source_location 0 T000000 Lyft Lux Back Bay 1 T000001 Lyft Lux Black Back Bay 2 T000002 Lyft Lux Black XL Back Bay 3 T000003 Lyft Lyft Back Bay 4 T000004 Lyft Lyft XL Back Bay"},{"location":"tutorials/04-adapters/#dataset-adapters","title":"Dataset adapters\u00b6","text":"<p>Unfortunately, different time series forecasting libraries use very different data formats.</p> <p>Luckily, <code>fev</code> comes with various adapters that make it easy to convert the data associated with each <code>Task</code> into an appropriate format for the different libraries.</p>"},{"location":"tutorials/04-adapters/#pandas","title":"Pandas\u00b6","text":""},{"location":"tutorials/04-adapters/#gluonts","title":"GluonTS\u00b6","text":"<p>Data is stored in a <code>PandasDataset</code>.</p> <p>The <code>train_dataset</code> contains only the historic data; the <code>prediction_dataset</code> additionally contains future values of the dynamic features.</p>"},{"location":"tutorials/04-adapters/#autogluon","title":"AutoGluon\u00b6","text":"<p>Converts historic &amp; future values to TimeSeriesDataFrame objects.</p>"},{"location":"tutorials/04-adapters/#nixtla","title":"Nixtla\u00b6","text":"<p>Similar to <code>pandas</code>, but ID, timestamp and target columns are renamed to <code>unique_id</code>, <code>ds</code> and <code>y</code> respectively.</p>"},{"location":"tutorials/05-add-your-model/","title":"Models","text":"<p>This notebook covers the following topics:</p> <ol> <li>Adding a wrapper for your model to fev/examples.</li> <li>Submitting the results for your model to the fev-leaderboard.</li> </ol> In\u00a0[1]: Copied! <pre>import fev\n\ntask = fev.Task(\n    dataset_path=\"autogluon/chronos_datasets\",\n    dataset_config=\"monash_rideshare\",\n    target=\"price_mean\",\n    horizon=30,\n)\ntask.predictions_schema\n</pre> import fev  task = fev.Task(     dataset_path=\"autogluon/chronos_datasets\",     dataset_config=\"monash_rideshare\",     target=\"price_mean\",     horizon=30, ) task.predictions_schema <pre>/home/shchuro/envs/fev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> Out[1]: <pre>{'predictions': Sequence(feature=Value(dtype='float64', id=None), length=30, id=None)}</pre> <p>For a probabilistic forecasting task (if <code>task.quantile_levels</code> are provided), each entry of <code>predictions</code> must additionally contain the quantile forecasts. For example</p> In\u00a0[9]: Copied! <pre>task = fev.Task(\n    dataset_path=\"autogluon/chronos_datasets\",\n    dataset_config=\"monash_rideshare\",\n    target=\"price_mean\",\n    horizon=30,\n    quantile_levels=[0.1, 0.5, 0.9],\n)\ntask.predictions_schema\n</pre> task = fev.Task(     dataset_path=\"autogluon/chronos_datasets\",     dataset_config=\"monash_rideshare\",     target=\"price_mean\",     horizon=30,     quantile_levels=[0.1, 0.5, 0.9], ) task.predictions_schema Out[9]: <pre>{'predictions': Sequence(feature=Value(dtype='float64', id=None), length=30, id=None),\n '0.1': Sequence(feature=Value(dtype='float64', id=None), length=30, id=None),\n '0.5': Sequence(feature=Value(dtype='float64', id=None), length=30, id=None),\n '0.9': Sequence(feature=Value(dtype='float64', id=None), length=30, id=None)}</pre> <p>The <code>predictions</code> cannot contain any missing values represented by <code>NaN</code>, otherwise an exception will be raised.</p> <p>Other than what's described above, there are no hard restrictions on how the <code>predict_with_model</code> method needs to be implemented. For example, it's completely up to you whether the method uses any datasets columns except the target or how the data is preprocessed.</p> <p>Still, here is some general advice:</p> <ul> <li>If your model is capable of generating probabilistic forecasts, make sure that you correct the \"optimal\" forecast for the <code>task.eval_metric</code>. For example, metrics like <code>\"MSE\"</code> or <code>\"RMSSE\"</code>, the mean forecast is preferred, while metrics like <code>\"MASE\"</code> are optimized by the median forecast.</li> <li>Use <code>fev.convert_input_data()</code> to take advantage of the adapters and reduce the boilerplate preprocessing code.</li> <li>Make sure that your wrapper can deal with missing values (or at least imputes them before passing the data to your model).</li> <li>Make sure that your wrapper takes advantage of the extra features of the task. For example, the following attributes might be useful:</li> </ul> In\u00a0[10]: Copied! <pre>print(f\"{task.static_columns=}\")\nprint(f\"{task.dynamic_columns=}\")\nprint(f\"{task.known_dynamic_columns=}\")\nprint(f\"{task.past_dynamic_columns=}\")\n# Attributes available after `task.load_full_dataset` is called\ntask.load_full_dataset()\nprint(f\"{task.freq=}\")\n</pre> print(f\"{task.static_columns=}\") print(f\"{task.dynamic_columns=}\") print(f\"{task.known_dynamic_columns=}\") print(f\"{task.past_dynamic_columns=}\") # Attributes available after `task.load_full_dataset` is called task.load_full_dataset() print(f\"{task.freq=}\") <pre>task.static_columns=[]\ntask.dynamic_columns=[]\ntask.known_dynamic_columns=[]\ntask.past_dynamic_columns=[]\ntask.freq='h'\n</pre> <pre># Example code from fev/examples/my_amazing_model/evaluate_model.py\n\ndef predict_with_model(task: fev.Task, **kwargs) -&gt; tuple[list[datasets.DatasetDict], float, dict]:\n    \"\"\"Wrapper for my_amazing_model\"\"\"\n    ...\n    return predictions_per_window, inference_time, extra_info\n\nif __name__ == \"__main__\":\n    model_name = \"my_amazing_model\"\n    benchmark = fev.Benchmark.from_yaml(\n        \"https://raw.githubusercontent.com/autogluon/fev/refs/heads/main/benchmarks/fev_bench/tasks.yaml\"\n    )\n\n    summaries = []\n    for task in benchmark.tasks:\n        predictions_per_window, inference_time, extra_info = predict_with_model(task)\n        evaluation_summary = task.evaluation_summary(\n            predictions_per_window,\n            model_name=model_name,\n            inference_time_s=inference_time,\n            extra_info=extra_info,\n            trained_on_this_dataset=True,  # True if model has seen this dataset during training, False otherwise. Please try to be honest!\n        )\n        summaries.append(evaluation_summary)\n\n    summary_df = pd.DataFrame(summaries)\n    print(summary_df)\n    summary_df.to_csv(f\"{model_name}.csv\", index=False)\n</pre>"},{"location":"tutorials/05-add-your-model/#adding-a-wrapper-for-your-model-to-fevexamples","title":"Adding a wrapper for your model to fev/examples.\u00b6","text":"<p>To add a wrapper for your library to <code>fev/examples</code>, you need to create a folder under <code>fev/examples/{YOUR_MODEL_NAME}</code> that contains:</p> <ul> <li>A Python file <code>evaluate_model.py</code> that contains a method <code>predict_with_model</code> with signature<pre>def predict_with_model(task: fev.Task, **kwargs) -&gt; tuple[list[datasets.DatasetDict], float, dict]:\n    \"\"\"Returns model predictions, inference time and potentially extra information about the model.\"\"\"\n    ...\n</pre> </li> <li><code>requirements.txt</code> file containing the required dependencies for your model.</li> </ul> <p>Defining the method <code>predict_with_model</code> is the most complex part of this process. We recommend looking at the implementations of some existing models to see how this can be done.</p> <p>The only hard requirement for this method is that it should return a tuple consisting of 3 elements:</p> <ol> <li><code>predictions</code> (<code>list[datasets.DatasetDict]</code>) object containing the model predictions for each evaluation window.</li> <li><code>inference_time</code> (<code>float</code>) inference time of the model for the entire task (in seconds).</li> <li><code>extra_info</code> (<code>dict | None</code>) optional information about the model such as model configuration.</li> </ol> <p>Predictions should follow the schema provided by <code>task.predictions_schema</code>.</p> <p>Each entry of <code>predictions</code> must contain a list of length <code>task.horizon</code></p>"},{"location":"tutorials/05-add-your-model/#submitting-the-results-for-your-model-to-the-fev-leaderboard","title":"Submitting the results for your model to the fev-leaderboard\u00b6","text":"<p>After you've implemented the wrapper for your model in <code>fev/examples</code>, complete the following steps:</p> <ol> <li>Fork <code>autogluon/fev</code> and clone your fork to your machine.</li> <li>Implement your model's wrapper in <code>fev/examples</code>.</li> <li>Run the model on all tasks from the benchmark and save the results to <code>fev/benchmarks/fev_bench/results/{model_name}.csv</code>.</li> <li>Open a pull request to <code>autogluon/fev</code> containing the following files:<ul> <li><code>fev/examples/{model_name}/evaluate_model.py</code></li> <li><code>fev/examples/{model_name}/requirements.txt</code></li> <li><code>fev/benchmarks/fev_bench/results/{model_name}.csv</code></li> </ul> </li> <li>We will independently reproduce the results using the code you provided and add the results to the leaderboard.</li> </ol>"}]}